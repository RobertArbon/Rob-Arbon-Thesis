\let\textcircled=\pgftextcircled
\chapter{Conclusions and outlook}
\label{chap:conclusions}


% The contributions of this thesis have been further demonstrate the scope of MMs and to introduce a number of ideas from the worlds of machine learning and mixture models to improve the MM analysis pipeline. 

Chapter \ref{chap:water} demonstrated the use of MMs to explain deviations to the Stoke-Einstein (S-E) equation of water diffusion in a sucrose matrix. The natural MSM basis set to answer this question was the Cartesian coordinates of the water molecule. This did not require variatonal optimisation of the kind described in chapter \ref{chap:msm} and meant that the modelling could be done at scale - 8000 different MSMs were estimated and used to partition, classify and analyse the MD data. The results showed that \SI{11.8}{\percent} of the time the water was trapped in cavities defined by the sucrose matrix undergoing reversible hopping dynamics with an average barrier height of \num{6.42}$k_{B}T$. 

To develop this work further a number of improvements and extensions can be made. First, the proportion of time spent in local equilibria could be refined by more accurately partitioning each MD trajectory into reversible/non-reversible sections. Instead of a single sucrose position decorrelation time (which determined the \SI{1}{\nano\second} time-slices used in chapter \ref{chap:water}), a local estimate can be used by averaging the autocorrelation function over small range of values $t-\Delta t < t < t+\Delta t$:
\begin{equation}
\mathrm{R}_{X,  X}(t^{\prime})=\mathbb{E}_{t \in [t\pm\Delta t]}\left[X_{t} \bar{X}_{t+t^{\prime}}\right]
\end{equation}
and choosing the longest time such that $\mathrm{R}_{X,  X}(t^{\prime}) > 0.8$ (or some other threshold). By repeating this process along the whole trajectory an `adaptive' partition of trajectories into stationary would be achieved. Second, using this more accurate partition, the irreversible timescales could be estimated using Koopman models \cite{wuVariationalKoopmanModels2017}. With both sets of timescales estimated a more mathematically precise comaparison of the S-E and cavity dynamics model can be made. Third, using more complex basis sets involving water-sucrose bonds lengths and relative orientations could be  variationally  optimised using the methods of chapter \ref{chap:msm}, potentially shedding more light on the microscopic mechanism of transport between the cavities. 

Chapter \ref{chap:msm} demonstrated how to use Bayesian optimisation of model hyperparameters, a technique common in machine learning, for an MSM of alanine dipeptide. Specifically, Gaussian processes (GP) regression was used to model the response surface of an MSM with respect to its hyperparameters and Bayesian optimisation used to optimise the number of microstates, $n$ and the continuous feature, $\chi$. Over the range of hyperparameters tested the number of microstates was shown to be irrelevant in determining the VAMP-2 score. Both the $(\phi, \psi)$ dihedral angles \emph{and} the heavy atom positions captured the slow relaxation processes equally well. Bayesian optimisation was then used to optimise the hyperparameters and it was shown that $50$ seed observations was needed to initialise the Bayesian optimisation algorithm. 

The main limitation of this study is that the response surface was unrealistically simple and did not provide adequate variation to test the Bayesian optimisation algorithm (proven in other machine learning model studies \cite{bergstraAlgorithmsHyperParameterOptimizationa,bergstrajamesbergstraRandomSearchHyperParameter2012}).  In addition, the irrelevance of the number of microstates, $n$, was not in line with other MM studies \cite{mcgibbonStatisticalModelSelection2014a,wuVariationalApproachLearning2020c,mcgibbonVariationalCrossvalidationSlow2015}. This counter-intuitive result needs to be verified. To do this, the search space of $n$ should be shifted from $[10, 1000]$ to $[2, 100]$, MD data with a higher temporal resolution data used, and the search space expanded to include different clustering algorithms.  In addition, this should be replicated on different benchmark systems, e.g., the Villin headpiece and $\beta$-lactamase used in reference \cite{bowmanQuantitativeComparisonAlternative2013}. Using this extended benchmark system, the demonstration of Bayesian optimisation and the test of the number of required seed observations could be improved by: i) increasing the number of BO steps from \num{10} to \num{100}, or until convergence in the response surface, and ii) repeating the optimisation a larger number of times to gain a statistical insight into its efficacy, and a more generalizable estimate of the number of seed observations. The workflow of this chapter could be applied to the 12 fast folding proteins \cite{lindorff-larsenHowFastFoldingProteins2011a}, which would extend and refine the work of reference \cite{husicOptimizedParameterSelection2016} which used the GMRQ to elucidate a set of principles for MSM construction. Working with a set of well-converged protein simulations would increase the generalizability of the conclusions about hyperparameter relevance, and provide a more robust test of Bayesian optimisation than just alanine dipeptide.  To extend this work even futher and bring statistical modelling of bimolecular systems ever closer to machine learning best practice, the use of response surface models which take into account conditional hyperparameter search spaces such as TPEs or RFs could by tested.

Chapter \ref{chap:hmm} drew on ideas from the mixture model community to demonstrate the use of the integrated classification likelihood (ICL) and other information criteria in determining  the appropriate number of hidden states in a HMM.  The benefits of these type of model selection metrics are that they are easy to calculate and do not rely on being able to resolve clear gaps in the eigenvalue spectrum. The ICL was able to correctly determine the number of metastable states in an HMM of the four well Prinz potential for \num{3} out of \num{5} lag times tested, while selecting only one extra state the remaining \num{2} lag times. The remaining criteria overestimated the number  hidden states by a considerable margin. The main limitation of this work is the limited amount of  benchmarking. A number of further steps are needed to gain a better estimate of the generalizability of the ICL. First, bootstrap the simulation data and calculate the selected number of hidden states on each bootstrap sample. This would give probabilities of the each criteria selecting the correct number of states, rather than the current single point estimates. Second, and still using the Prinz potential simulation data, calculate performance of each criteria with differing amounts of data. This would allow the performance of each criteria under the more realistic condition of limited data to be assessed. Third, more benchmark systems are needed, both `toy' models with well defined numbers of metastable states e.g. the 2D triple well from reference \cite{noeProjectedHiddenMarkov2013a}, the model 2D potential in reference \cite{bacalladoBayesianComparisonMarkov2009a} used to benchmark Bayes factors for MSMs, and M{\"u}ller potential in the AIC and BIC benchmarking in reference \cite{mcgibbonStatisticalModelSelection2014a}. In addition, standard molecular benchmarks such alanine dipeptide, the Villin headpiece and $\beta$-lactamase (used in reference \cite{bowmanQuantitativeComparisonAlternative2013}) and the Fip35WW (used in reference \cite{mcgibbonStatisticalModelSelection2014a}).  

Chapter \ref{chap:aadh} used the response surface and BO methods from chapter \ref{chap:msm} and the model selection techniques of chapter \ref{chap:hmm} to create a HMM of the active site of AADH. A simulation data set of AADH was created and the response surface using an expanded set of hyperparameters was estimated and optimised. A number of sensitivity models were suggested from inspection of the optimised response surface. HMMs were estimated using the number of hidden states suggested by the ICL. The most relevant parameters for determining the VAMP-2 score were the TICA lag time and the least relevant the number of microstates.  The optimal continuous feature was the backbone and residue dihedral angles. Coarse graining the optimal MSM using a Bayesian HMM and choosing the number of hidden states with the ICL revealed a complex network of states. The slowest relaxation process of approximately \SI{1}{\micro\second} involved transport between two unreactive states through a flux bottleneck state. This bottle neck state had the shortest average donor-acceptor distance and was thus deemed the most `reactive'. The second sensitivity test (and the only sensitivity test to converge a HMM) did not confirm these results and showed a qualitatively different story albeit with similar relaxation timescales. Due to the effective lack of data, neither of the HMMs could be validated. This work failed to find evidence for the hypothesis from reference \cite{glowackiTakingOckhamRazor2012b} that there are two distinct, reactive conformational states. 

The main drawback of this work is the AADH data set which must be corrected and expanded before any inferences can be drawn. First, the mistake in the disulphide bond must corrected and the simulation cell re-equilibrated. Second, the sampling trajectories must be initialized from independent starting structures by minimizing and re-equilibrating structures taken from a seeding trajectory. Third, the source of the large conformational changes both in the loop structure adjacent to the active site and the tail residues must be investigated. This will ensure that the sampling is taking place in  conformations relevant for the rate determining reaction. The amount of simulation data should be determined by monitoring the convergence of implied timescale  for optimized MSMs (using the methods of chapter \ref{chap:msm}), or through an adaptive sampling method \cite{doerrOntheFlyLearningSampling2014}. 

The main theme of this thesis is that ideas from machine learning and statistics can be powerful tools for improving analysis of molecular simulations. Bayesian optimisation and response surface methods are well known for optimising general machine learning models but they can also be applied to Markov models and potentially to the wide range of models for molecular simulation \cite{noeMachineLearningMolecular2020}. Recasting HMMs for MSM coarse-graining as a classification problem showed that i) the ICL could be used fruitfully for selecting the number of metastable states and ii) there is a connection between classification and metastability through the classification entropy. The second theme of this thesis is that the scope of MMs, traditionally focused on large biomolecules, can be easily extended to smaller yet still suitably complex systems.  

The outlook for further work builds on these themes.  First, MMs can and should be used for areas outside of large biomolecule simulations. The mechanism of water diffusion suggested by this work is not proven generally and needs investigation in both the other saccharide systems experimentally studied here, as well as through other amorphous materials such pharmaceutical delivery systems \cite{hancockCharacteristicsSignificanceAmorphous1997}. Second, the issue of conformational dynamics of enzymes is of critical importance to enzymology in general, as discussed in chapter \ref{chap:aadh}.  However, the users of MMs for understanding enzymes are not necessarily experts in machine learning. This has motivated the publication of user friendly packages for creating \emph{arbitrary} MMs \cite{schererPyEMMASoftwarePackage2015a,beauchampMSMBuilder2ModelingConformational2011}.  The next stage is to use the response surface and BO methods to create an automatic  pipeline for creating \emph{optimized} MMs. Steps towards this have been taken with the creation of Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016}.  In addition, the author of this thesis is currently engaged in a code re-factoring to accommodate the work of chapter \ref{chap:msm} into Osprey. In future this may allow automatic optimisation of hyperparameters as well as understanding counter-intuitive features via their response surface. For example, solvent degrees of freedom are important and often ignored \cite{guBuildingMarkovState2013}. The distance metric for incorporating solvent degrees of freedom in reference \cite{guBuildingMarkovState2013} does not have the salience of, say, a dihedral angle. However estimating and optimising the response surface with respect to its hyperparameters (the number of number of solvent molecules incorporated and the width of its distance kernel, $\sigma$) can give an intuitive understanding of its effect on explaining the kinetic variance. Beyond the realm of MMs, understanding machine learning models through their response surface could help in breaking down technical barrier between non-expert users and the ever increasing set of methods for understanding molecular simulations \cite{noeMachineLearningMolecular2020}.

