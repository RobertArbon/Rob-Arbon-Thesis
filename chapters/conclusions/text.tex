\let\textcircled=\pgftextcircled
\chapter{Conclusions and outlook}
\label{chap:conclusions}


% The contributions of this thesis have been further demonstrate the scope of MMs and to introduce a number of ideas from the worlds of machine learning and mixture models to improve the MM analysis pipeline. 
\section{Summary of main findings and improvements}
Chapter \ref{chap:water} demonstrated the use of Markov models to explain deviations to the Stoke-Einstein (S-E) equation of water diffusion in a sucrose matrix. Compared to the workflow presented in later chapters, a simplified modelling approach was taken.  After inspection of the simulation data, the natural MSM basis set used to model the water dynamics was microstates clustered in the space of the Cartesian coordinates of the water molecule.  Heuristics from the from the literature guided the other modelling choices and no variational optimisation of basis sets, of the kind described in chapter \ref{chap:msm}, was performed. This meant that the modelling could be done at scale - 8000 different MSMs were estimated and used to partition, classify and analyse the MD data. The results showed that \SI{11.8}{\percent} of the time the water was trapped in cavities defined by the sucrose matrix undergoing reversible hopping dynamics with an average barrier height of \num{6.42}$k_{B}T$. 

To develop this work further a number of improvements can be made. First, the proportion of time spent in local equilibria could be refined by more accurately partitioning each MD trajectory into reversible/non-reversible sections. Instead of a single sucrose position decorrelation time (which determined the \SI{1}{\nano\second} time-slices used in chapter \ref{chap:water}), a local estimate can be used by averaging the autocorrelation function over small range of values $t-\Delta t < t < t+\Delta t$:
\begin{equation}
\mathrm{R}_{X,  X}(t^{\prime})=\mathbb{E}_{t \in [t\pm\Delta t]}\left[X_{t} \bar{X}_{t+t^{\prime}}\right]
\end{equation}
and choosing the longest time such that $\mathrm{R}_{X,  X}(t^{\prime}) > 0.8$ (or some other threshold). By repeating this process along the whole trajectory an `adaptive' partition of trajectories into stationary would be achieved. Second, using this more accurate partition, the irreversible timescales could be estimated using Koopman models \cite{wuVariationalKoopmanModels2017}. With both sets of timescales estimated a more mathematically precise comparison of the S-E and cavity dynamics model can be made. Third, using more complex basis sets involving water-sucrose interactions could be variationally  optimised using the methods of chapter \ref{chap:msm}, potentially shedding more detailed light on the microscopic mechanism of transport between the cavities. 

Chapter \ref{chap:msm} demonstrated how to use Bayesian optimisation of model hyperparameters, a technique common in machine learning, for an MSM of alanine dipeptide. Specifically, Gaussian processes regression was used to model the response surface of an MSM with respect to its hyperparameters and Bayesian optimisation used to optimise the number of microstates, $n$ and the continuous feature, $\chi$. Gaussian processes regression modelled the response surface well. Calculation of the hyperparameter relevance, quantified what was already apparent from visualisation of the response surface, namely: that the VAMP-2 score was insensitive to the number of microstates, and, for different protein features, the shape of the response surface was similar. An interpretation of the hyperparameter relevance for categorical variables which extended the usual definition \cite{bergstrajamesbergstraRandomSearchHyperParameter2012}) was presented. Both the $(\phi, \psi)$ dihedral angles \emph{and} the heavy atom positions captured the slow relaxation processes equally well. Bayesian optimisation was then used to optimise the hyperparameters and it was shown that $50$ seed observations was needed to initialise the Bayesian optimisation algorithm. However, this did not improve the VAMP-2 score, but did provide a useful convergence check on the hyperparameters already selected. 

The main limitation of this study is that the response surface was too simple and did provide adequate complexity to test the Bayesian optimisation algorithm (proven in other machine learning model studies \cite{bergstraAlgorithmsHyperParameterOptimizationa,bergstrajamesbergstraRandomSearchHyperParameter2012}).  In addition, the irrelevance of the number of microstates, $n$, was not in line with other MM studies \cite{mcgibbonStatisticalModelSelection2014a,wuVariationalApproachLearning2020c,mcgibbonVariationalCrossvalidationSlow2015}. This counter-intuitive result needs to be verified. To do this, the search space of $n$ should be shifted from $[10, 1000]$ to $[2, 100]$; MD data with a higher temporal resolution data used; and the search space expanded to include different clustering algorithms.  In addition, this should be replicated on different benchmark systems, e.g., the Villin headpiece and $\beta$-lactamase used in reference \cite{bowmanQuantitativeComparisonAlternative2013} or the twelve fast folding proteins \cite{lindorff-larsenHowFastFoldingProteins2011a} used in other studies \cite{husicOptimizedParameterSelection2016, schererPyEMMASoftwarePackage2015a, husicWardClusteringImproves2017a}. Using these extended benchmark systems, the demonstration of Bayesian optimisation (BO) and the test of the number of required seed observations could be improved by: i) increasing the number of Bayesian optimisation steps from \num{10} to \num{100}, or until convergence in the response surface, and ii) repeating the optimisation a larger number of times to gain a statistical insight into its efficacy, and a more generalizable estimate of the number of seed observations. Working with a set of well-converged simulations of larger proteins, would increase the generalizability of the conclusions about hyperparameter relevance, and provide a more robust test of Bayesian optimisation than just alanine dipeptide.  To extend this work even futher and bring statistical modelling of biomolecular systems ever closer to machine learning best practice, the use of response surface models which take into account conditional hyperparameter search spaces such as tree Parzen estimators or Random forests could by tested.

Chapter \ref{chap:hmm} drew on ideas from the mixture model community to demonstrate the use of the integrated classification likelihood (ICL) and other information criteria in determining  the appropriate number of hidden states in a HMM.  The benefits of these type of model selection metrics are that they are easy to calculate and do not rely on being able to resolve clear gaps in the eigenvalue spectrum. The ICL was able to correctly determine the number of metastable states in an HMM of the four well Prinz potential for \num{3} out of \num{5} lag times tested, while selecting only one extra state for the remaining \num{2} lag times. The remaining criteria overestimated the number  hidden states by a considerable margin. The main limitation of this work is the limited amount of  benchmarking. A number of further steps are needed to gain a better estimate of the generalizability of the ICL. First, bootstrap the simulation data and calculate the selected number of hidden states on each bootstrap sample. This would give probabilities of the each criteria selecting the correct number of states, rather than the current single point estimates. Second, and still using the Prinz potential simulation data, calculate performance of each criteria with differing amounts of data. This would allow the performance of each criteria under the more realistic condition of limited data to be assessed. Third, more benchmark systems are needed, both `toy' models with well defined numbers of metastable states e.g. the 2D triple well from reference \cite{noeProjectedHiddenMarkov2013a}, the model 2D potential in reference \cite{bacalladoBayesianComparisonMarkov2009a} used to benchmark Bayes factors for MSMs, and M{\"u}ller potential in the AIC and BIC benchmarking in reference \cite{mcgibbonStatisticalModelSelection2014a}. Other, more realistic,  benchmark systems should also be used, such alanine dipeptide, the Villin headpiece and $\beta$-lactamase (used in reference \cite{bowmanQuantitativeComparisonAlternative2013}) and the Fip35WW (used in reference \cite{mcgibbonStatisticalModelSelection2014a}). Lastly, the results of the ICL could be compared the full Bayes factor method from  reference \cite{bacalladoBayesianComparisonMarkov2009a}. 

Chapter \ref{chap:aadh} used the response surface and Bayesian optimisation methods from chapter \ref{chap:msm} and the model selection techniques of chapter \ref{chap:hmm} to create a HMM of the active site of AADH. A simulation data set of AADH was created and the response surface using an expanded set of hyperparameters was estimated and optimised. A number of sensitivity models were suggested from inspection of the optimised response surface. HMMs were estimated using the number of hidden states suggested by the ICL. The most relevant parameters for determining the VAMP-2 score were the TICA lag time and the least relevant the number of microstates.  The optimal continuous feature was the backbone and residue dihedral angles. Coarse graining the optimal MSM using a Bayesian HMM and choosing the number of hidden states with the ICL revealed a complex network of states. The slowest relaxation process of approximately \SI{1}{\micro\second} involved transport between two unreactive states through a flux bottleneck state. This bottle neck state had the shortest average donor-acceptor distance and was thus deemed the most `reactive'. The second sensitivity test (and the only sensitivity test to converge a HMM) did not confirm these results and showed a qualitatively different story albeit with similar relaxation timescales. Due to the effective lack of data and the Bayesian estimation algorithm, neither of the HMMs could be validated. This work failed to find evidence for the hypothesis from reference \cite{glowackiTakingOckhamRazor2012b} that there are two distinct, reactive conformational states. 

The main drawback of this work is the AADH data set which must be corrected and expanded before any inferences can be drawn. First,  the missing disulphide bridges must modelled correctly and the simulation cell re-equilibrated. Second, the sampling trajectories must be initialized from independent starting structures by minimizing and re-equilibrating structures taken from a seeding trajectory. Third, the source of the large conformational changes both in the loop structure adjacent to the active site and the tail residues must be investigated. This will ensure that the sampling is taking place in  conformations relevant for the rate determining reaction. The amount of simulation data should be determined by monitoring the convergence of implied timescale  for optimized MSMs (using the methods of chapter \ref{chap:msm}), or through an adaptive sampling method \cite{doerrOntheFlyLearningSampling2014}. 

\section{Lessons learned}

The objectives of this thesis were to investigate whether model selection and optimisation methods,  common in the machine learning and statistic community,  could be used to create robust MSMs in a more transparent and efficient way.  Two systems were studied utilising two different workflows. The first was water diffusion in secondary organic aerosol particles which were modelled using using a simplified Markov model construction workflow. MSM and HMM parameters (number of basis states, hidden states, essential degrees of freedom) were selected using heuristics from existing literature and visualisation of the simulation data. The second was the conformational dynamics of AADH, an important enzyme in the debate surrounding tunneling and the role of dynamics in enzyme catalysis.  The workflow used here was more complex and consisted of modelling and optimising the response surface of an Markov state model, creating sensitivity tests, and using statistical model selection to select a final coarse-grained description with a hidden Markov model.  A number of lessons and themes can be drawn from comparing these two approaches. 

\emph{Lesson one}. The more complex Markov modelling workflow, involving modelling and optimising response surfaces, is not necessary to produce scientific insight. The results of the simplified Markov model workflow produced interpretable and novel insights into the mechanism of water diffusion in organic aerosol particles. However, this does not rule out the possibility that a more complex optimisation procedure would bring benefits. To test this, a range of different features describing the water-sucrose system could be optimised using the methods in chapters \ref{chap:msm} and  \ref{chap:hmm}.  

\emph{Lesson two}. The choice of feature is important but the VAMP-2 scores of MSMs are largely insensitive to other hyperparameters.  Visual inspection of the response surface of alanine dipeptide, and the calculation of the hyperparameter relevance for AADH, showed that the VAMP-2 score was insensitive to the values of the number of microstates and TICA parameters. The choice of feature was important, but in both alanine dipeptide and AADH, more than one feature was identified as being  optimal.  

\emph{Lesson three}.  Bayesian optimisation is not necessary for optimising low relevance hyperparameters.  Bayesian optimisation was applied to the response surface of both alanine dipeptide and AADH and failed to improve the VAMP-2 scores in both cases. However, when seeded with an appropriate amount of data, the Bayesian optimisation algorithm provided a convergence check on the optimum value of the VAMP-2 score.  

\emph{Lesson four}. Modelling and visualising the response surface of a model allows the creation of principled sensitivity tests.  In order to test the conclusions of a statistical analysis, results should be compared after changing key modelling choices. Sensitivity tests 2 and 3 for AADH were devised after inspection of response surface showed how different features and TICA lag times affected the VAMP-2 scores.  

\emph{Lesson five}. MSMs with similar VAMP-2 scores produce different results.  Three models with different hyperparameters were produced based on their similar VAMP-2 scores for AADH\footnote{Sensitivity test 1 changed the Markov lag-time which is not a hyperparameter.}.  However, the results from two of them did not produce similar descriptions of conformational dynamics.  However, this could be the result of inadequate simulation data.  

\emph{Lesson six}.  The integrated complete data likelihood criterion is a promising model selection criterion for selecting the number of metastable macrostates.  It selected the appropriate number of metastable macrostates in the model system used, but the test was not thorough and it should be tested further.  However, it is simple to calculate, and its interpretation is in line with the goals of a hidden Markov model analysis, namely that coarse-graining is a trade-off between crisp partitioning of microstates into macrostates and modelling the dynamics accurately. However, it should be noted that this only applies to data in which the dynamics are metastable. When it is important to model transition regions, the HMM approach to coarse-graining is not appropriate. 


\section{Outlook and further work}
The outlook for further work builds on these lessons and the specific findings of chapters \ref{chap:water} and \ref{chap:aadh} in three areas. 

First, Markov models can and should be used for areas outside of large biomolecule simulations. The mechanism of water diffusion suggested by this work is not proven generally and needs investigation in both the other saccharide systems experimentally studied here, as well as through other amorphous materials such as those found in pharmaceutical delivery systems \cite{hancockCharacteristicsSignificanceAmorphous1997}. 

Second, the classification likelihood used in the ICL selection metric, could be used to extend the work of reference \cite{bacalladoBayesianComparisonMarkov2009a} to create a \emph{classification} Bayes factor. This would provide an alternative to the current Bayes factor for judging the quality of a coarse-graining method and number of metastable states. The information gained from this type of Bayes factor would be to more accurately (than the approximate ICL) determine how well a given coarse-graining crisply partitions microstates into macrostates, while both preserving Markovian dynamics and taking into account over-fitting.  Like the full Bayes factor method of reference \cite{bacalladoBayesianComparisonMarkov2009a}, however, this would require much more computational effort than the ICL. However, Bayesian variational inference methods \cite{foxTutorialVariationalBayesian2012a}, which replace the expensive sampling of traditional Bayesian estimation with approximations of the Bayesian posterior distribution, could be used as an approximate method for calculating both types of Bayes factor. 

Third, the metrics used to score Markov state models need further investigation.  Lesson five suggests that the VAMP-2 score may hide important differences between MSMs built with different hyperparameters. If this is accurate and holds generally, this would be an example of the \emph{Rashomon} effect: where models which perform similarly according to some metric (e.g., predictive performance, VAMP-2 score), differ in interpretation \cite{breiman2001}. If MSMs do exhibit the Rashomon effect then this would have important consequences for the interpretation of simulation data.  For example, if two models with similar VAMP-2 scores describe different dynamical processes then this needs to be taken into account when reporting results. This would also call for further tests (experimental or computational) to determine which picture is correct.  A test for the Rashomon effect would be to estimate models using different protein features and compare the results, using well converged simulation data, e.g., the benchmark twelve fast-folding proteins \cite{lindorff-larsenHowFastFoldingProteins2011a}. 

Fourth, response surface methods and Bayesian optimisation can be used to facilitate a robust Markov modelling workflow. The users of Markov models for understanding biomolecular systems are not necessarily experts in statistical model development. This has motivated the publication of user friendly packages for creating arbitrary Markov models \cite{schererPyEMMASoftwarePackage2015a,beauchampMSMBuilder2ModelingConformational2011}. These packages facilitate a workflow where only a handful of different sets of hyperparameters are used, such as the simplified workflow from chapter \ref{chap:water}. The next stage of Markov model development could be to create not a handful of MSMs, but the entire MSM response surface for a given set of simulation data. This would allow the creation of sensitivity tests for testing the robustness of statistical inferences.  While the lessons learned from this thesis suggest that only the protein feature is important for determining the VAMP-2 score, the response surface methodology could incorporate other metrics, which may be more sensitive to other hyperparameters. If the response surface is sensitive to the hyperparameters, Bayesian optimisation could be used to optimise this surface.  Steps towards this have been taken with the creation of Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016}.  In addition, the author of this thesis is currently engaged in a code re-factoring of this package to accommodate the work of chapter \ref{chap:msm}. In future this may allow automatic optimisation of hyperparameters as well as understanding counter-intuitive features via their response surface. For example, solvent degrees of freedom are important but often ignored \cite{guBuildingMarkovState2013}. The distance metric for incorporating solvent degrees of freedom in reference \cite{guBuildingMarkovState2013} does not have the salience of, say, a dihedral angle. However estimating and optimising the response surface with respect to its hyperparameters (the number of number of solvent molecules incorporated and the width of its distance kernel, $\sigma$) can give an intuitive understanding of its effect on explaining the kinetic variance. Beyond the realm of Markov models, understanding machine learning models through their response surface could help in breaking down technical barrier between non-expert users and the ever increasing set of methods for understanding molecular simulations \cite{noeMachineLearningMolecular2020}.




