%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Introduction}
\label{chap:intro}
This thesis describes the use of Markov models (MMs) to describe the slow conformational  dynamics of two systems at different ends of the complexity scale: water diffusing through an sucrose matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH). In particular this thesis describes contributions to the methods of optimising and selecting the hyperparameters of MMs. 

\section{Markov models and their applications}
MMs provide a framework for describing the slow processes of metastable states of systems from molecular dynamics (MD) simulations \cite{prinzMarkovModelsMolecular2011}. The central idea \cite{zwanzigClassicalDynamicsContinuous1983a} is that for complex systems, over a sufficiently long periods of time, transitions rates between regions of the systems total configurational space are independent on their path history. If the configurational space is partitioned into $n$ discrete states then the dynamics of the system can be described by an $n\times n$ transition matrix $\mathbf{T}$ \cite{prinzMarkovModelsMolecular2011}. Each element of $\mathbf{T}$ describes the conditional probability of the system jumping between states over a time period, $\tau$, the Markov lag time \cite{prinzMarkovModelsMolecular2011}. It is usually the case that the chemically interesting processes are not between the $n$ individual discrete states. Rather they are between sets of discrete states for which the probability of leaving the set is low compared to the probability of remaining within the set \cite{schutteDirectApproachConformational1999}, these ``almost invariant'' sets are known as the metastable conformations or metastable states of the system. Determining these metastable states, their kinetics, and structural characteristics from MD simulations are the main goals of a MM analysis and the remain a topic of continued interest for both method development and applications \cite{husicMarkovStateModels2018,noeMarkovModelsMolecular2019b, wangConstructingMarkovState2018c}.  

Applications of MMs are concentrated on biomolecular systems and form an intrinsic part of the bimolecular simulation tool-box \cite{hugginsBiomolecularSimulationsDynamics2019}. Applications include modelling both protein folding pathways \cite{singhalUsingPathSampling2004,swopeDescribingProteinFolding2004} as well as intrinsically discorded proteins \cite{schorAnalyticalMethodsStructural2016a}. 
MMs have been applied to enzyme systems and used to elucidate, for example, ligand docking pathways \cite{ahalawatMappingSubstrateRecognition2018a} and regioselectivity mechanisms in cytochrome p450 \cite{dodaniDiscoveryRegioselectivitySwitch2016a}, the conformational heterogeneity in the important cancer target SETD8 \cite{chenDynamicConformationalLandscape2019a}, loop dynamics in triosephosphate isomerase \cite{LoopMotionTriosephosphate}, and allosteric effects in cyclophilin A \cite{wapeesittipanAllostericEffectsCyclophilin2019}. Other applications include self-assembly \cite{senguptaAutomatedMarkovState2019} and dimer formation \cite{leahyCoarseMasterEquations2016} of amyloid peptides, identifying important conformations in drug targets to improve drug docking free energy calculations \cite{amaroEnsembleDockingDrug2018}, and rational drug design \cite{gervasioBiomolecularSimulationsStructureBased2019}. There have been comparatively little use of MM on smaller systems (whose kinetics tend to be derived from quantum mechanical and thermodynamic data \cite{glowackiMESMEROpenSourceMaster2012, pillingMasterEquationModels2003}, rather than statistically estimated from MD data), however, one recent example used MMs to determine hydrogen bond rearrangement in liquid water \cite{schulzCollectiveHydrogenbondRearrangement2018}. 

MMs are a varied collection of techniques covering a range of different simulation conditions and models of dynamics and for recent reviews of the full range of techniques and applications see references \cite{husicMarkovStateModels2018, noeMarkovModelsMolecular2019b}. The early theory of MMs concentrated on describing conformational dynamics of systems in thermal equilibrium by constructing $n$ discrete geometrically similar \emph{microstates} and modelling the dynamics as a $n$ state Markov chain \cite{singhalUsingPathSampling2004, swopeDescribingProteinFolding2004, prinzMarkovModelsMolecular2011} also known as a \emph{Markov state model} (MSM). In order to recover the metastable conformations from an MSM a number of \emph{coarse graining} schemes have been introduced. One of the first was Perron-cluster cluster analysis, PCCA \cite{deuflhardIdentificationAlmostInvariant2000a} and robust PCCA \cite{deuflhardRobustPerronCluster2005b}, which used the eigenvectors of $\mathbf{T}$ to determine the metastable states. Later, hidden Markov models (HMMs), an already well known statistical model \cite{rabinerTutorialHiddenMarkov1989}, were introduced  which  identified the metastable states of the system with the hidden states of a HMM \cite{noeMarkovModelsMolecular2019b}. 

The key to creating accurate MSMs are finding the ``essential degrees of freedom'' of the system \cite{zwanzigClassicalDynamicsContinuous1983a, schutteDirectApproachConformational1999}. Clustering over these variables, rather than the atomic coordinates, helps mitigate the problem of discretizing high-dimensional systems. To that end, time-lagged independent component analysis, TICA \cite{perez-hernandezIdentificationSlowMolecular2013a, schwantesImprovementsMarkovState2013}, was introduced which identifies linear combinations of the atomic positions as collective variables over which to discretize the dynamics. 
The next step in the development of MMs was the realisation that estimating MSMs and TICA could be cast as variational optimisation problem (the variational approach to conformational dynamics, VAC \cite{nuskeVariationalApproachMolecular2014}). While VAC showed that TICA and MSMs were the optimal description of the slow dynamics for a \emph{given} continuous and discrete basis set (respectively), the authors of reference \cite{mcgibbonVariationalCrossvalidationSlow2015} showed that the same variational principle could optimize the basis sets themselves. The key innovation of this work was to combine cross-validation \cite{arlotSurveyCrossvalidationProcedures2009} and the variational principal to score a given basis set using the generalized matrix Rayleigh coefficient, GMRQ. The theory was then broadened with Koopman models to encompass simulations of systems out of thermal equilibrium \cite{wuVariationalKoopmanModels2017}. With the variational approach to Markov processes (VAMP) the theory of MSMs and Koopman models was unified into one conceptual framework. This increased the scope of MMs and presented a range of model scoring metrics (VAMP scores), of which the GMRQ was a special case.

\textbf{Chapter \ref{chap:theory}} sets out the theory of MMs relevant to this thesis which focuses on MSM estimation, TICA for preprocessing, variational optimisation of basis sets using VAMP scores, and coarse graining MSMs using HMMs. 

\section{A simple MSM}
The MSM literature has concentrated on large biomolecules because of the requirement of a free energy surface which is ``sufficiently complex'' that memoryless transitions assumption holds \cite{zwanzigClassicalDynamicsContinuous1983a}. As already mentioned, the success of the MM process is the creation of a good set of microstates to represent the slow dynamics of the system. However, for biomolecules the process is complicated by the large number of potentially relevant degrees of freedom which cannot be determined a-priori \cite{shallowayMacrostatesClassicalStochastic1996}. 

\textbf{Chapter \ref{chap:water}} demonstrates a system which is both sufficiently complex for the Markovian assumption to hold while being simple enough that the essential degrees of freedom naturally present themselves: a single water molecule diffusing through a sucrose matrix, as may be found in, for example, organic aerosol particles \cite{songTransientCavityDynamics2020a}. In this system, the complex free energy surface is created by the sucrose matrix and essential degrees of freedom are the coordinates of the center-of-mass motion of the water molecule. These systems are important to understand from a basic science perspective because the observed diffusion rates are larger than those predicted from the Stokes-Einstein diffusion \cite{songTransientCavityDynamics2020a}. In addition, these type of systems have applications from understanding the properties of pharmaceutical delivery systems to airborne pollution. 

\section{Complex MSMs}
With large biomolecules, determining the essential degrees of freedom becomes more difficult. This was apparent outside the context of MSMs as the authors of  reference \cite{karpen1993statistical} note in 1992: 
``It is becoming essential to develop methods which reduce the complexity of data resulting from these long simulations while retaining the relevant information." The method they proposed to investigate the mechanism of peptide turn formation was to use a statistical clustering algorithm (where the data determine the cluster definitions) to cluster the pentapeptide configurations in the space of the backbone ($\phi, \psi$) dihedral angles. Rather than data based clustering, the authors of reference \cite{mckelveyCHARMMAnalysisConformations1991} use predetermined regions of the ($\phi, \psi$) plane to describe the conformational landscape of the metastasis-inhibiting laminin peptide.  

The importance of selecting appropriate features for MSM analysis also quickly became important. In combination with the domain knowledge of chemists and biochemists, dimensionality reduction \cite{friedman2001elements} techniques became popular.  Principal component analysis (PCA) \cite{leverPrincipalComponentAnalysis2017} identifies linear combinations, or `principal components', of variables which explain the variance in a given set of data. The first principal component (PC1) is a vector which points in the direction of the greatest variance, PC2 is orthogonal to PC1 and lies in the direction of the second-most variance etc. Projecting the data onto the first $m$ principal components (where $m$ is typically much less than the number of original dimensions) which explain the majority of the variance, while ignoring the rest, is a way of reducing the dimension of the system. Using MD simulations of a $\beta$-heptapeptide, the authors of reference \cite{degrootEssentialDynamicsReversible2001} used the first three principal components as features and clustered the trajectories to understand the folding process in terms of its hydrogen bonding network. 

The problem with PCA as a dimensionality reduction technique for Markov state models is that the first $m$ PCs retain the greatest \emph{configurational} variance \cite{friedman2001elements}. However, the goal of MMs is capture the slowest dynamics, not the dynamics over the greatest physical distances. However, using TICA to reduce dimension by keeping the first $m$ TICA components retains the greatest \emph{kinetic} variance \cite{noeKineticDistanceKinetic2015}. The total kinetic variance describes the ability  the TICA components (or any basis set) to capture the slow dynamics of the system. Using TICA as a preprocessing step in MSM construction has been systematically investigated and shown to be more accurate than both PCA and no preprocessing at all \cite{husicOptimizedParameterSelection2016} in capturing the slow dynamics. However, it brings with it two new modelling choices (aside from the number of microstates, $n$): the TICA lag-time (also called $\tau$ but not necessarily the same as the Markov lag-time $\tau$) and the number of independent components to retain, $m$.  

\section{Evaluating MSM performance}\label{sec:intro_msm_perf}
The MM analysis pipeline described so far, consists of first transforming MD trajectories into features (the essential degrees of freedom, $\chi$), then reducing the dimension with TICA,  discretizing the TICA components into $n$ microstates, and finally estimating the MSM. The modelling choices or \emph{hyperparameters}, ($\chi, \tau, m, n$), create the MSM basis set, which in turn determine the accuracy of the resulting MSM, and so a method of evaluating the performance of these hyperparameters  is needed. While the ground truth of the kinetic processes is not available, the initial way forward came through cross-validation and the GMRQ \cite{mcgibbonVariationalCrossvalidationSlow2015}. 

The innovation in reference \cite{mcgibbonVariationalCrossvalidationSlow2015} was to create a model score, the GMRQ (the Rayleigh trace from quantum mechanics), which could be used to judge the quality of the model choices while accounting for the tendency of models to fit to noisy signals in the data. This was achieved through cross-validation \cite{arlotSurveyCrossvalidationProcedures2009}: a model is estimated using a portion of the data and scored on the remaining data. Maximizing the cross-validated GMRQ by varying the hyperparameters increases the accuracy of the eigenvectors \cite{mcgibbonVariationalCrossvalidationSlow2015}. The GMRQ is a special case of the first VAMP score, VAMP-1, while maximizing the total kinetic variance is the same as maximizing the VAMP-2 score. These VAMP metrics completed the analysis pipeline \cite{schererVariationalSelectionFeatures2019} which now can be summarised as: i) transform MD trajectories into features, $\chi$, ii) select reasonable choices of  hyperparameters (features, TICA parameters, number of discrete states) and calculate the cross-validated VAMP-2 score, iii) change the hyperparameters and repeat analysis, iv) stop when the VAMP-2 score stops increasing. 

\section{Hyperparameter optimisation}\label{sec:intro_hyper_opt}
Choosing the hyperparameters which maximize the VAMP-2 score is a `black-box' optimisation problem \cite{jonesEfficientGlobalOptimization1998a}, so called because no gradient information on the response of the VAMP to the hyperparameters is available. This is a common problem in the machine learning community where models have many parameters and may take days to train \cite{feurer2019hyperparameter}. In this case it is not feasible to exhaustively search through combinations of hyperparameters. A popular method for optimising large sets of hyperparameters is Bayesian optimisation (also known as sequential model based optimisation, SMBO) \cite{hutterSequentialModelbasedOptimization2011,NIPS2012_4522,bergstraAlgorithmsHyperParameterOptimizationa,bergstraMakingScienceModel2013}. The idea behind Bayesian optimisation is that there is an objective function which is costly to optimise \cite{brochuTutorialBayesianOptimization2010,shahriariTakingHumanOut2016} (in this case the VAMP-2 score). So instead of optimising this directly, the BO procedure builds an statistical model of  objective function known as a \emph{surrogate function} or \emph{response surface}, using randomly sampled  values of the objective function. Having built an initial response surface, searching for the next hyperparameter to evaluate is guided by an \emph{acquisition function}. These can be selected or adjusted to trade off high-uncertainty regions (the `explore' regime) of the response surface with the high-value, low-uncertainty  regions (the `exploit' regime) \cite{shahriariTakingHumanOut2016}. A suggestion is evaluated, the response surface updated and the process repeats. Bayesian optimisation for hyperparameter optimisation is popular, as the number of packages designed for this purpose will attest (this list is non-exhaustive): Hyperopt \cite{bergstraHyperoptPythonLibrary2013}; sequential model-based algorithm configuration \cite{hutterSequentialModelbasedOptimization2011}, SMAC; BayesOpt \cite{martinez-cantinBayesOptBayesianOptimization2014}; Spearmint \cite{DBLP:conf/uai/GelbartSA14,snoekAbstractBayesianOptimization2013,snoekInputWarpingBayesian2014a,NIPS2013_5086,NIPS2012_4522}, GPyOpt \cite{gpyopt2016}, DragonFly \cite{JMLR:v21:18-223}; Auptimiser \cite{liuAuptimizerExtensibleOpenSource2019}; and Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016}. A popular choice of response surface model is a Gaussian process (GP) \cite{rasmussenGaussianProcessesMachine2006}, a highly flexible type of model which fits naturally within the Bayesian optimisation paradigm \cite{shahriariTakingHumanOut2016}. Indeed, six of the eight packages listed here all implement some kind of Gaussian process  as their response surface model.

\textbf{Chapter \ref{chap:msm}} demonstrates the use Bayesian optimisation to optimise the MSM hyperparameters using cross-validated VAMP-2 scores of the model system alanine dipeptide. In addition, the parameters of GPs are explored as a way to describe the relevance of hyperparameters in determining the VAMP-2 score. This chapter lays the ground-work for performing a similar analysis on AADH in chapter \ref{chap:aadh}, in particular: how to fit and interpret GPs and how to use GPs with Bayesian optimisation to optimise hyperparameters. 

\section{The number of metastable states}
Having arrived at an optimal set of MSM hyperparameters by maximising the kinetic variance (VAMP-2), the next task in the MM pipeline is to coarse grain the MSM into a interpretable model. This is often done by fitting a hidden Markov model (HMM) to the same microstates used to estimate the MSM \cite{noeMarkovModelsMolecular2019b}, although many other techniques exist (e.g., observer operator models \cite{wuProjectedMetastableMarkov2015}, see chapter \ref{chap:hmm} for a full discussion). A HMM is a model of a Markovian process between $g$ \emph{hidden} states i.e., states which are not directly observed in the data. While in a hidden state the system emits randomly, according to a probability distribution, to one of a set of \emph{observed} states, which are seen in the data. The transition rates and emission probabilities are estimated from the data.  The only hyperparameter stipulated by the user (at least for the discrete HMMs considered here) is the number of hidden states, $g$. To model the metastable dynamics, the number of hidden states of the HMM is set equal to the number of metastable states of the system, $r$ \cite{noeProjectedHiddenMarkov2013a}. The number of metastable states is determined by looking for gaps in the eigenvalue spectrum of the MSM \cite{prinzMarkovModelsMolecular2011,noeProjectedHiddenMarkov2013a}. However, due to poor discretisation or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}. This observation, along with the plethora of other coarse-graining schemes motivated the introduction of Bayes factors \cite{kassBayesFactors1995} for both judging the quality of the coarse graining scheme and the number of metastable states \cite{bacalladoBayesianComparisonMarkov2009a}. The Bayes factor is the ratio of the integrated likelihood of the data given the model parameters \cite{kassBayesFactors1995}, for two competing models. Bayes factors measure the relative evidence of a model (its parameters and hyperparameters) given a trajectory of \emph{observed states} (the microstates). For example, if the Bayes factor between a HMM with $g=2$ and  $3$ hidden states is $100$ then the $g=2$ model is more strongly favoured. 

The integrated likelihood only takes into account how the model explains the observed states \cite{biernackiAssessingMixtureModel2000a,mclachlanFiniteMixtureModels2000}. It does not factor in directly the hidden state structure of the model. Rather than view the HMM as a coarse graining technique, another approach is to view an HMM and other hidden mixture models as a clustering algorithm \cite{mclachlanFiniteMixtureModels2000} where observed states are classified as belonging to hidden states. An important analogue of the integrated likelihood in mixture model community is the integrated classification likelihood \cite{mclachlanFiniteMixtureModels2000}. This measures the evidence for  both the observed states \emph{and the hidden states} given the model parameters and hyperparameters. 

\textbf{Chapter \ref{chap:hmm}} explores the utility of approximations to the Bayes factor and similar criteria for determining the optimal value of $g$: the Bayesian information criterion, BIC \cite{schwarzEstimatingDimensionModel1978a}, the integrated complete data likelihood criterion, ICL \cite{biernackiAssessingMixtureModel2000a}, the Akaikie information crteria, AIC \cite{akaikeInformationTheoryExtension1998}, and cross-validated log-likelihood, CVLL \cite{celeuxSelectingHiddenMarkov2008}. This extends previous work \cite{mcgibbonStatisticalModelSelection2014a} which looked at the AIC and BIC for selecting parameters of MSMs. Like chapter \ref{chap:msm} this lays the ground-work for application to the case of AADH in chapter \ref{chap:aadh}. The example used is a four well potential, the Prinz potential \cite{prinzMarkovModelsMolecular2011}, with a well defined number of metastable states against which the BIC, ICL, AIC and CVLL can be judged. The benefits of the these information criteria (except CVLL) are that they do not required extra calculation after a HMM has been estimated and so lend themselves to situations in which a large number of models need to be tested, a situation that will be encountered in this thesis. 

\section{Aromatic amine dehydrogenase}
Aromatic amine dehydrogenase (AADH) is an enzyme involved in the on-going debate over the role of dynamics in tunneling and catalysis \cite{glowackiTakingOckhamRazor2012b,glowackiProteinDynamicsEnzyme2012a,mcgeaghProteinDynamicsEnzyme2011}. As the name suggests, AADH oxidizes aromatic amines such as tryptamine with a proton transfer from a Schiff base intermediate to an acceptor aspartate residue as its rate limiting step \cite{masgrauAtomicDescriptionEnzyme2006}.  Substituting deuterium in the substrate causes the rate to drop by up to a factor of $55$ (the kinetic isotope effect, KIE), well below the decrease expected from the difference in zero-point energy between C---H and C---D, implicating significant quantum mechanical tunneling in the mechanism \cite{masgrauAtomicDescriptionEnzyme2006, klinmanbeyond2009}. It has been suggested that the temperature dependence of the KIEs for different enzymes, including AADH, cannot be explained by transition state theory (TST) \cite{agrawalVibrationallyEnhancedHydrogen2004,kohenEnzymeCatalysisClassical1998}. However, the authors of reference \cite{glowackiProteinDynamicsEnzyme2012a} showed that by using TST to model the reaction as two rapidly interconverting metastable states which can both react to form the product, the temperature dependence of the KIE could be explained. To test this hypothesis, the conformational landscape of AADH has to be determined. 

\textbf{Chapter \ref{chap:aadh}} brings together the hyperparameter optimisation work of chapter  \ref{chap:msm} and the hidden state selection work of chapter \ref{chap:hmm} and applies it to the case of AADH. An expanded set of hyperparameters is optimised and understood in terms of the response surface and in addition to the optimized parameters, a set of sensitivity models are proposed. The model selection criteria of chapter \ref{chap:hmm} are used to select the appropriate number of hidden states in the coarse grained HMMs and the results of the sensitivity models compared and discussed. 

\textbf{Chapter \ref{chap:conclusions}}, discusses the conclusions of this thesis and sets out concrete steps for further work in this area. 


% using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 


% Gaussian Processes (GPs) have been


% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 


% with a concomitant increase in the number of software packages 


%  - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% % Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}




% \textbf{From art to science:} 
% MSMs are master equations (ME review [6])
% Model is n x n matrix, spans the conformational space, conditional probabilities as elements. 
% State populations are give thermodynamics, off-diagonal elements give kinetics. 
% If we assume thermodynamic equilibrium then eigendecomposition is useful [description of eigenvectors]
% the n states should capture the dyanmics. 

% Memoryless transition networks come from Zwanzig [1] then Van Kampen [2] key MSM papers [7-9]. 

% creating meaningful states is difficult [33,34] Karpen did dihedrals [10] de Groot [20] did PCA and k-medioids.

% can stitch together different trajectories: McCammon [11] now Folding@home [35], Luty [12] suggested stitching together different trajs for ligand binding. 

% Hardware devs: FoldingAtHome, BlueGened [39-41], GPUGRID [43,44]

% ITS plots [9] CKtest [46]

% Different features different dynamics [45]

% Errors are (1) state decomposition and (2) finite sampling [47]

% Global descriptors worse than internal degrees of freedom [49]

% Sarich [73, 92]: discretization error decreases and partitioning become finer and the lag time increases. 

% Djurdjevac [93]: upper bounds for error between MSM and trajectories decreases with lag time. 

% TICA\c variance explained [115]:
% This MSM score was termed the GMRQ, which stands for
% generalized matrix Rayleigh quotient, the form of the approx-imator (also referred to as the Rayleigh trace).124
% The GMRQ on the validation set will be poor if the model was overfit on the
% training set but better if the model identifies the underlying
% dynamics common to both sets. In 2016, Noé and Clementi115 demonstrated that kinetic variance in a data set can be explained area.
% by summing the squared tICA eigenvalues. Since the variational principle derived in Noé and Nüske95 holds for any strictly nonincreasing weights applied to the scored eigenvalues,96 the kinetic variance can also be used to score models, or to deter- mine how many tICs are needed to explain a given amount of kinetic variance in the data.

% \textbf{Simple MSM}
% Analysis of three water molecules \cite{schulzCollectiveHydrogenbondRearrangement2018} to understand the collective hydrogen bond rearrangement, uses both Euler angles and spherical coordinates for dofs. 








% \textbf{Coarse graining and HMMs}


% \textbf{Hyperparameter search}
% Feature selection: \cite{schererVariationalSelectionFeatures2019}


% \cite{bergstraHyperoptPythonLibrary2013} Hyperopt - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}

% Optunity: \cite{claesenEasyHyperparameterSearch2014}

% \cite{pmlr-v32-hutter14} use random forest and ANOVA to assess parameter importance. 
% \cite{gramacyVariableSelectionSensitivity2013} using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 

% \cite{falknerBOHBRobustEfficient2018a} goes beyond BO and random bandit. 

% \cite{di2018genetic} genetic algorithm for hyperparameter search. 

% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 




















