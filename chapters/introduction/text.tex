%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Introduction}
\label{chap:intro}
This thesis describes the use of Markov models to describe the slow conformational and configurational dynamics of two systems and different ends of the complexity scale: water diffusing through an organic matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH). In particular this thesis describes contributions to methods of optimising and selecting the parameters Markov models. MMs provide a framework for describing the important dynamical processes of metastable states of molecules from molecular dynamics (MD) simulations. 

\section{Markov state models}
The central idea [zwanzig] is that over a sufficiently long period of time transitions rates between regions of phase space are independent on their path history. If phase space is partitioned into $n$ discrete states then the dynamics of the system can be described by an $n\times n$ transition matrix $\mathbf{T}$. Each element of $\mathbf{T}$ describes the conditional probability of the system jumping between states. It is usually the case that the interesting kinetics is not between the $n$ individual discrete states but instead between sets of states separated by a common kinetic barriers. These metastable states and their kinetics and structural characteristics are the main goal of a MM analysis. Markov modelling is the process of transforming the configurational snapshots from molecular dynamics (MD) simulations to an estimate of $\mathbf{T}$ and the implied metastable states of the system.  It is a  topic of continued interest \cite{husicMarkovStateModels2018}\cite{noeMarkovModelsMolecular2019b}\cite{wangConstructingMarkovState2018c} for both method development and applications.  

Applications of MMs are concentrated on biomolecular systems and form an intrinsic part of the bimolecular simulation tool-box\cite{hugginsBiomolecularSimulationsDynamics2019}. Applications include modelling both protein folding pathways [] as well as intrinsically discorded proteins \cite{schorAnalyticalMethodsStructural2016a}. In enzymes MMs have been used to elucidate, for example, ligand docking pathways \cite{ahalawatMappingSubstrateRecognition2018a} and regioselectivity mechanisms in cytochrome p450  \cite{dodaniDiscoveryRegioselectivitySwitch2016a}, the conformational heterogeneity in the important cancer target SETD8 \cite{chenDynamicConformationalLandscape2019a}, loop dynamics in  triosephosphate isomerase \cite{LoopMotionTriosephosphate}, allosteric effects in \cite{wapeesittipanAllostericEffectsCyclophilin2019}. Other applications include self-assembly of  \cite{senguptaAutomatedMarkovState2019} and dimer formation \cite{leahyCoarseMasterEquations2016} of amyloid peptides; identifying important conformations in drug targets to improve drug docking free energy calculations  \cite{amaroEnsembleDockingDrug2018} and rational drug design \cite{gervasioBiomolecularSimulationsStructureBased2019}. There has been comparatively little use of MSMs on smaller systems (whose kinetics tend to be derived from quantum mechanical and thermodynamic data \cite{glowackiMESMEROpenSourceMaster2012}, rather than statistically estimated from MD data), however, one recent example used MSMs to determine hydrogen bond rearrangement in liquid water \cite{schulzCollectiveHydrogenbondRearrangement2018}. 

MMs are a varied collection of techniques covering a range of different simulation conditions and dynamical models. Discrete Markov state models (MSMs) are used for modelling the fine grained dynamics of proteins in thermodynamic equilibrium [prinz].  Observable Operator models (OOMs) and Hidden Markov models (HMMs) were developed as methods of coarse graining an MSM from a model with potentially thousands of states to a handful of metastable states [][], while retaining quantitative accuracy. TICA [] and associated techniques [][] are useful for creating approximate collective variables as features, which both increases the accuracy and precision of an MSM. A crucial step in the development of MMs was the realisation that estimating MSMs and TICA could be cast as variational optimisation problem (the variational approach to conformational dynamics, VAC []). This in turn led to methods for scoring different discretization schemes used to represent the same dynamics, the generalized matrix Rayleigh coefficient, GMRQ []. The theory of stationary MSMs was broadened to encompass simulations of systems out of thermal equilibrium, with Koopman models []. The theory of MSMs and Koopman models was unified into one conceptual framework known as the variational approach to Markov processes (VAMP)[]. This increase the scope of MMs and presented a range of model scoring metrics (VAMP scores). Recently deep learning techniques such as feed forward neural networks (FNN) and variational autoencoders (VAEs) have used the VAMP framework to replace the modelling pipeline of MMs with a single flexible model [][]. 

Chapter \ref{chap:theory} sets out some of the theory of MMs relevant of this thesis. For a fuller review of the history of MMs, the latest techniques and their applications see \cite{husicMarkovStateModels2018} and \cite{noeMarkovModelsMolecular2019b}. 

\section{A simple MSM}
Central to the MM process is the mapping from regions of phase space to $n$ discrete states. The discretisation scheme is important for accurately describing the kinetics and thermodynamics \cite{shallowayMacrostatesClassicalStochastic1996}. For large biomolecules the process is complicated by the large number of relevant degrees of freedom \cite{shallowayMacrostatesClassicalStochastic1996} and the ``rough'' [zwanzig] potential energy surface. However, simple and yet relevant systems, do exist beyond the toy models in, for example [prinz], [bacallado] and [pmm]. 

Chapter \ref{chap:water} demonstrates one such simple system: a single water molecule diffusing through a sucrose matrix as may be found in, for example, organic aerosol particles [cavitydyn]. MSMs were created using the discretized  center-of-mass water coordinates using molecular dynamics simulations, validated against extensive experimental work. This work [] demonstrated a new type of water diffusion mechanism which proceeds via a series of local equilibrium `cavity hopping' steps.  

\section{Complex MSMs}
The discretization process becomes more complicated as the number of atoms in the system increases, and, coupled with increasing computer power, the need for statistical analysis using clustering techniques in chemically significant feature spaces became apparent. As the authors of \cite{karpen1993statistical} note in 1992: 
``It is becoming essential to develop methods which reduce the complexity of data resulting from these long simulations while retaining the relevant information."
They then go on to cluster a pentapeptide in the space of the backbone dihedral angles to investigate the mechanism of peptide turn formation. Rather than data based clustering, \cite{mckelveyCHARMMAnalysisConformations1991} use regions of the $\phi, \psi$ plane to describe the conformational landscape of the metastasis-inhibiting laminin peptide. The importance of selecting appropriate features for MSM analysis also quickly became important.  Clustering with the root mean square deviation (RMSD) of heavy atoms as a similarity metric [wang 86] but the resulting state definitions were are not robust to noise in the observations [wang].  
Principal component analysis (PCA) [] identifies linear combinations, or `principal components', of variables which explain the variance in a given set of data. The first principal component (PC1) of a PCA  is a vector which points in the direction of the greatest variance, the second PC in an orthogonal direction of the second-most variance etc. []. The PCs are eigenvectors of the covariance matrix of the data, while the eigenvalues are proportional to the amount of explained variance. Projecting the data onto the subset principal components which explain the majority of the variance, while ignoring the rest, is a way of reducing the dimension of the system to a few relevant coordinates. Using MD simulation of a $\beta$-heptapeptide the authors of \cite{degrootEssentialDynamicsReversible2001} used the first three principal components as features and clustered the trajectories to understand the folding process in terms of its hydrogen bonding network. 

Markov modelling is primarily concerned with identifying the motions of the system which are the \emph{slowest} not the \emph{largest}. The principal components are not guaranteed (or usually the case [husic]) to be in the same direction as slowest movement. TICA was introduced to the biomolecular simulation community [] [] as a method of dimensionality reduction which finds linear combinations of the data which pointed in the direction of the slowest movement. TICA is a type of independent component analysis (ICA) which has its origins in the signal processing community [ica book]. However, the variational approach to conformational dynamics showed that TICA was equivalent to an MSM analysis in a continuous, rather than discrete, basis. TICA was shown to be analogous to PCA in that the eigenvalues associated with each independent component are proportional the amount of explained \emph{kinetic} variance. [Noé and Clementi115 ]. Pre-processing MD data with TICA has been shown to improve the quality of the MSMs but has brought with it, two new modelling choices, the TICA lag-time (the time period over which to measure the `slowness' of the system, and the number of independent components (IC) to use. 

\section{Feature selection}
TICA is not only used for dimensionality reduction on atomic coordinates but of any relevant feature of the system, e.g.,  backbone dihedral angles. A typical analysis pipeline is then []: i) calculate features expected to be related to the slow movements of the system, ii) reduce the dimension with TICA, iii) discretize TICA components, iv) estimate MSM. This carries with it a number of modelling choices, such as the choice of feature, the TICA parameters, and the number of discrete states. These choices create the basis sets, which in turn determine the accuracy of the resulting MSM and so a method of assessing the quality of an MSM is needed. While the ground truth of the kinetic processes is not available, the way forward came through cross-validation and the GMRQ \cite{mcgibbonVariationalCrossvalidationSlow2015}. 

The innovation in \cite{mcgibbonVariationalCrossvalidationSlow2015} was to create a model score, the GMRQ (the Rayleigh trace from quantum mechanics), which could be used to judge the quality of the model choices while accounting for the tendency of models to fit to noisy signals in the data. This was achieved through cross-validation []: a model is estimated using a portion of the data and scored on the remaining data. In addition to the GMRQ, the VAMP scores allowed related metrics to be used, such as the VAMP-2 score, which maximizes the kinetic variance. The variational metrics completed the analysis pipeline \cite{schererVariationalSelectionFeatures2019} which can be summarised as: for a fixed lag time, select reasonable choices of  hyperparameters (features, TICA parameters, number of discrete states) and calculate the cross-validated VAMP-2 score; change the hyperparameters and repeat; stop when the VAMP-2 score stops increasing. 

\section{Hyperparameter optimisation}
Choosing the hyperparameters which maximize the VAMP score is a `black-box' optimisation problem [], so called because no gradient information on the response of the VAMP to the hyperparameters is available. This is a common problem in the machine learning community where models have many parameters and may take days to train. In this case it is not feasible to exhaustively search through combinations of parameters. A popular method for optimising large sets of hyperparameters is Bayesian optimisation (also known as sequential model based optimisation, SMBO)  \cite{hutterSequentialModelbasedOptimization2011} \cite{NIPS2012_4522}\cite{bergstraAlgorithmsHyperParameterOptimizationa} \cite{bergstraMakingScienceModel2013}. The idea behind Bayesian optimisation is  \cite{brochuTutorialBayesianOptimization2010} that there is an objective function which is costly to optimise. So instead of optimising this directly, the BO procedure builds an statistical model of  objective function known as a \emph{surrogate} using randomly sampled  values of the objective function. Searching for the next hyperparameter to evaluate is guided by an \emph{acquisition function} which trades off low-mean, high-uncertainty regions (the `explore' regime)  of the surrogate with the high-mean, low-uncertainty (the `exploit' regime) regions. Each suggestion is evaluated, the surrogate model updated and the process repeats. The key parameters of the BO procedure are the type of surrogate and the acquisition function. Bayesian optimisation for hyperparameter optimisation is popular, as the number of packages designed for this purpose will attest (this list is non-exhaustive): Hyperopt \cite{bergstraHyperoptPythonLibrary2013} uses surrogate functions called Tree Parzen Estimators (TPEs) \cite{bergstraAlgorithmsHyperParameterOptimization}; sequential model-based algorithm configuration  \cite{hutterSequentialModelbasedOptimization2011}, SMAC, uses random forests (RFs)[]; BayesOpt \cite{martinez-cantinBayesOptBayesianOptimization2014}, Spearmint \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}, GPyOpt \cite{gpyopt2016}, DragonFly \cite{JMLR:v21:18-223}, all implement Gaussian [] or T-student processes [] as surrogate functions. Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016a} and Auptimiser \cite{liuAuptimizerExtensibleOpenSource2019} both implement a range of different surrogates. 

Not only is it important to optimise hyperparameters but to understand how the hyperparameters affect the model. The authors of \cite{bergstrajamesbergstraRandomSearchHyperParameter2012} used Gaussian processes to show that some hyperparameters are more relevant for improving model performance than others. Based on this observations the authors went on to show that this meant choosing parameters at random is superior to choosing parameters based on a regular grid in hyperparameter space. The authors of  \cite{gramacyVariableSelectionSensitivity2013} and \cite{pmlr-v32-hutter14} used  random forests to assess the importance of variables for optimising compiler options and machine learning models respectively. 

Chapter \ref{chap:msm} demonstrates the use of Gaussian processes surrogate models to both determine the relevance of, and optimise, the MSM hyperparameters of the model system alanine dipeptide. Gaussian processes are non-parametric models which fit curves of a given covariance, rather than say, partial derivative in the case of linear models []. Part of the fitting process is determining how best to scale input variables and the functional form of the covariance and model selection methods for making these choices are discussed. 

\section{The number of metastable states}
Having arrived at an optimal set of MSM hyperparameters by maximising the kinetic variance (VAMP-2), the next task in the MM pipeline is to coarse grain the MSM into a interpretable model. This is typically done by fitting a hidden Markov model (HMM) to the same states used to estimate the MSM \cite{noeMarkovModelsMolecular2019b}, although many other techniques exist ([] [] [] ). A HMM models a Markov process with $g$ hidden states, while in a hidden state the system emits randomly to one of a set of observed states which comprise data. The transition rates and emission probabilities are estimated from the data.  The only hyperparameter (at least the discrete HMMs considered here) stipulated by the researcher is the number of hidden states, $g$. To model the metastable dynamics, the number of hidden states of the HMM are set equal to the number of metastable states of the system, $r$. The number of metastable states is determined by looking for gaps in the eigenvalue spectrum of the MSM. However, due to poor discretisation or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}. This observation, along with the plethora of other coarse-graining schemes motivated the introduction of Bayes factors \cite{kassBayesFactors1995} for both judging the quality of the coarse graining scheme and the number of metastable states \cite{bacalladoBayesianComparisonMarkov2009a}. The Bayes factor is the ratio of the integrated likelihood of the data given the model parameters, for two competing models. Bayes factors measure the relative evidence of a model (its parameters and hyperparameters) given the trajectory of \emph{observed states}. For example, if the Bayes factor between a HMM with $g=2$ and  $3$ hidden states is $100$ then the $g=2$ model is more strongly favoured. 

The Bayes factor only takes into account how the model explains the observed states. It does not factor in directly the hidden state structure of the model. Rather than view the HMM as a coarse graining technique, another approach is to view an HMM and other hidden mixture models as a clustering algorithm [] where observed states are classified as belonging to hidden states. An important analogue of the integrated likelihood in mixture model community is the integrated classification likelihood []. This measures the probability of both the observed states \emph{and the hidden states} given the model parameters.

Chapter \ref{chap:hmm} explores the utility of approximations to the Bayes factor and its classification likelihood analogue (the Bayesian information criterion (BIC) and the integrated complete data likelihood criterion (ICL)) for selecting the number of hidden states in HMMs. The example used is a four well potential, the Prinz potential, with a well defined number of metastable states against which the BIC, ICL and other mixture model selection criteria can be judged. The benefits of the these information criteria are that they do not required extra calculation after a HMM has been estimated and so lend themselves to situations in which a large number of models need to be tested. 

\section{AADH}
Aromatic amine dehydrogenase (AADH) is an enzyme with one of the largest kinetic isotope effects measured (primary KIE $\simeq 55$).  The KIE is the ratio of the rate constants with the reactants substituted with $^1H$ and $^{2}H$. Such a large KIE cannot be explained by the different zero point energy contributions of the reacting C---H and C---D bonds, instead there is a contribution to the rate from the hydrogen atom tunneling through the potential barrier. The large KIE and its anomalous temperature dependence has made it the subject of many computational, theoretical and experimental studies ([][][]). The KIE has been used as a probe of enzyme dynamics their role in catalysis []. However, instead of invoking dynamical couplings of enzyme motion and reaction coordinates, the authors of \cite{glowackiProteinDynamicsEnzyme2012a} took the simpler approach of using an extension of transition state theory which invokes two reactive metastable states. They showed that this theory could account for the temperature dependence of KIEs in four different enzymes. To test this hypothesis, the conformational landscape of AADH, and in particular how many potential reactive metastable states it has, has to be determined. 

Chapter \ref{chap:aadh} brings together the hyperparameter optimisation work of chapter  \ref{chap:msm} and the hidden state selection of chapter \ref{chap:hmm} and applies it to the case of AADH. Using a larger set of hyperparameters extended the work of chapter \ref{chap:msm} and the relevance of the hyperparameters guided visualisation of the surrogate function in order to aide understanding of the MSM hyperparameters and suggest sensitivity tests. The model selection criteria of chapter \ref{chap:hmm} were used to select the appropriate number of hidden states in a HMM and the results of the sensitivity test compared. The final chapter \ref{chap:conclusions}, discusses the conclusions of this thesis and sets out a concrete set of steps for further work in this area. 


% using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 


% Gaussian Processes (GPs) have been


% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 


% with a concomitant increase in the number of software packages 


%  - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% % Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}




% \textbf{From art to science:} 
% MSMs are master equations (ME review [6])
% Model is n x n matrix, spans the conformational space, conditional probabilities as elements. 
% State populations are give thermodynamics, off-diagonal elements give kinetics. 
% If we assume thermodynamic equilibrium then eigendecomposition is useful [description of eigenvectors]
% the n states should capture the dyanmics. 

% Memoryless transition networks come from Zwanzig [1] then Van Kampen [2] key MSM papers [7-9]. 

% creating meaningful states is difficult [33,34] Karpen did dihedrals [10] de Groot [20] did PCA and k-medioids.

% can stitch together different trajectories: McCammon [11] now Folding@home [35], Luty [12] suggested stitching together different trajs for ligand binding. 

% Hardware devs: FoldingAtHome, BlueGened [39-41], GPUGRID [43,44]

% ITS plots [9] CKtest [46]

% Different features different dynamics [45]

% Errors are (1) state decomposition and (2) finite sampling [47]

% Global descriptors worse than internal degrees of freedom [49]

% Sarich [73, 92]: discretization error decreases and partitioning become finer and the lag time increases. 

% Djurdjevac [93]: upper bounds for error between MSM and trajectories decreases with lag time. 

% TICA\c variance explained [115]:
% This MSM score was termed the GMRQ, which stands for
% generalized matrix Rayleigh quotient, the form of the approx-imator (also referred to as the Rayleigh trace).124
% The GMRQ on the validation set will be poor if the model was overfit on the
% training set but better if the model identifies the underlying
% dynamics common to both sets. In 2016, Noé and Clementi115 demonstrated that kinetic variance in a data set can be explained area.
% by summing the squared tICA eigenvalues. Since the variational principle derived in Noé and Nüske95 holds for any strictly nonincreasing weights applied to the scored eigenvalues,96 the kinetic variance can also be used to score models, or to deter- mine how many tICs are needed to explain a given amount of kinetic variance in the data.

% \textbf{Simple MSM}
% Analysis of three water molecules \cite{schulzCollectiveHydrogenbondRearrangement2018} to understand the collective hydrogen bond rearrangement, uses both Euler angles and spherical coordinates for dofs. 








% \textbf{Coarse graining and HMMs}


% \textbf{Hyperparameter search}
% Feature selection: \cite{schererVariationalSelectionFeatures2019}


% \cite{bergstraHyperoptPythonLibrary2013} Hyperopt - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}

% Optunity: \cite{claesenEasyHyperparameterSearch2014}

% \cite{pmlr-v32-hutter14} use random forest and ANOVA to assess parameter importance. 
% \cite{gramacyVariableSelectionSensitivity2013} using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 

% \cite{falknerBOHBRobustEfficient2018a} goes beyond BO and random bandit. 

% \cite{di2018genetic} genetic algorithm for hyperparameter search. 

% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 




















