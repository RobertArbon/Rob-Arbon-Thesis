%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Introduction}
\label{chap:intro}
% This thesis describes the use of Markov models (MMs) to describe the slow conformational  dynamics of two systems at different ends of the complexity scale: water diffusing through an sucrose matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH). In particular this thesis describes contributions to the methods of optimising and selecting the hyperparameters of MMs.


\hl{This thesis describes the use of statistical and machine learning  model selection and optimisation techniques applied to Markov models (MM) for describing the slow conformational dynamics of two biomolecular systems: water diffusing through an sucrose matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH).}

\section{The importance of conformational changes in biochemical systems}


Quantitatively  describing the conformational changes in biomolecular systems is of central importance for understanding their function, chemical and biological properties. Conformational changes are at the heart of enzyme catalysis \cite{hammesMultipleConformationalChanges2002b,hammes-schifferRelatingProteinMotion2006, klinmanHydrogenTunnelingLinks2013}. Triosephosphate isomerase (TIM) catalyses the isomerisation dihydroxyacetone phosphate (DHAP) and (R)-glyceraldehyde-3-phosphate \cite{LoopMotionTriosephosphate} and is considered a paradigmatic system for enzyme catalysis \cite{richardParadigmEnzymeCatalyzedProton2012a} having been studied by molecular simulation since at least 1987 \cite{brownMolecularDynamicsSimulations1987, josephAnatomyConformationalChange1990}.  The catalytic process requires the closure of a loop after ligand binding, through approximately \SI{7}{\angstrom}, which creates the necessary conditions to allow the isomerisation to occur,  opening again to allow product release \cite{LoopMotionTriosephosphate}. More extensive conformational changes are also present in other systems.  Dihydrofolate reductase (DHFR) catalyses the reduction of 7,8-dihydrofolate (DHF) to 5,6,7,8-tetrahydrofolate (THF) \cite{schnellStructureDynamicsCatalytic2004a} with at least five kinetically distinct intermediates detected \cite{fierkeConstructionEvaluationKinetic1987} across the catalytic cycle. The Met20 loop adopts at least three distinct conformations across the catalytic cycle \cite{sawayaLoopSubdomainMovements1997} with mutation experiments demonstrating its importance in rate determining step \cite{liFunctionalRoleMobile1992}. The relative positions of the two subdomains also changes with an implied role in transition state stabilisation and product release \cite{sawayaLoopSubdomainMovements1997}. Other biochemical examples include G protein-coupled receptors, a large family of transmembrane proteins involved in extracellular sensing and signalling and are responsible for olfaction, vision and taste \cite{rosenbaumStructureFunctionGproteincoupled2009}.  They transmit information from outside of the cell by way of ligand binding which inducing a series of conformational changes which in turn triggers the response within the cell \cite{bockenhauerConformationalDynamicsSingle2011a}. Large conformational changes are also implicated in the mechanism by which proteins associate with specific parts of DNA, thus enabling a whole host of cellular functions from gene regulation to DNA replication \cite{vandervaartCoupledBindingBending2015}.  

\section{Learning conformational dynamics from molecular simulations}

Computational approaches to studying conformational dynamics are important because their allow descriptions with high temporal and spatial resolution beyond the scope of most experimental techniques \cite{hugginsBiomolecularSimulationsDynamics2019}. A popular approach is to evolve the systems equations of motion using molecular dynamics (MD) to produce a set of trajectories through phase space. These trajectories can be used to reveal conformational transition pathways and metastable states and other properties of interest by estimating the relevant regions of the free energy landscape (the free energy with respect to a given set of coordinates)  \cite{rohrdanzDiscoveringMountainPasses2013a}.  

There are a wide range of techniques for understanding conformational dynamics from MD simulations which depend on current knowledge of the system and the questions being asked. Path based techniques such as transition path sampling (TPS) \cite{bolhuisTRANSITIONPATHSAMPLING2002, dellagoTransitionPathSampling1998, dellagoTransitionPathSampling2002a}, transition interface sampling (TIS) \cite{vanerpNovelPathSampling2003} and forward flux sampling (FFS) 
\cite{allenSamplingRareSwitching2005} all start with two specified metastable conformations, A \& B, and can be used to estimate rate constants and reaction coordinates of A $\rightleftharpoons$ B without previous knowledge of transition states. TPS works by first proposing an reaction pathway between A and B. Then a statistical ensemble of pathways is generated from this initial path using Metropolis Monte Carlo. TIS and FFS are similar but define interfaces based on an order parameter\footnote{a quantity such as the root mean square deviation, which, while not a true reaction coordinate, varies between the two states.} which separate A and B. Molecular dynamics simulations are used to estimate the flux between the interfaces and hence the rate between A \& B with FFS relaxing the assumption of equilibrium dynamics present in TIS and TPS. These techniques have been applied \cite{juraszekSamplingMultipleFolding2006, juraszekRateConstantReaction2008,velez-vegaKineticsMechanismUnfolding2010} to the model protein folding system Trp-cage \cite{neidighDesigning20residueProtein2002} to elucidate the folding pathway and have replicated some of the experimental microscopic rates in the folding pathway. String methods posit a discrete set of states along a path (or string) of fixed length and moves these states such that the string corresponds to minimum free energy path between A and B \cite{weinane.TransitionPathTheoryPathFinding2010, jnssonNudgedElasticBand1998}. String methods have been used to determine the mechanism and binding free energy of platinum based drugs to DNA \cite{elderSequenceSpecificRecognitionCancer2012}. 

Directional milestoning \cite{faradjianComputingTimeScales2004, majekMilestoningReactionCoordinate2010,kirmizialtinRevisitingComputingReaction2011a} is a technique which doesn't require knowledge of specific metastable states, only a  collective variable known to be related to the important conformational changes. Using directional milestoning one can ... First a set of states which cover the relevant conformational space, known as anchors, are selected. The milestones are sets of conformations which separate (as measured by the collective variable) the anchors from one another, and are used to restart trajectories, calculate reaction coordinates and conformational kinetics. Milestoning has been used to understand the selectivity of DNA reverse transcriptase \cite{kirmizialtinHowConformationalDynamics2012} and to understand the permeation mechanism and rate of tryptophan through cell membranes \cite{cardenasUnassistedTransportAcetyl2012}.  

When no previous information such as known metastable states or appropriate collective variables are known, more abstract statistical and machine learning methods have been increasingly shown to be important. Principal component analysis (PCA) finds the linear combinations of coordinates or features of the molecule, such as $\alpha$-carbon coordinates, which maximise the variance \cite{pearson1901liii}. PCA of the heavy atoms of the ribonuclease barnase \cite{noldeEssentialDomainMotions2002} was used to identify high variance/flexible regions of the enzyme which partially explained the role of distal residues in its activity and stability. The authors of \cite{amadeiEssentialDynamicsProteins1993a} used PCA in the  protein lysozyme to identify highly flexible ``essential'' degrees of freedom which are related to the opening and closing of the active site.  One of the draw backs of PCA is that only linear combinations of features make up the principal components.  Kernel PCA, which incorporates non-linear transformations of input features, has also been developed and used with TPS simulations to extract a reaction coordinate for the reaction of lactate  dehydrogenase \cite{antoniouIdentificationReactionCoordinate2011,quaytmanReactionCoordinateEnzymatic2007}. Multidimensional scaling (MDS) \cite{borg1997modern} is similar to PCA in that it seeks to represent high dimensional data with a smaller number of  combinations of input features. Rather than finding components to capture the variance, MDS preserves, as far as possible, distances between observations.  MDS has been used to characterise the conformational states and track simulation convergence of bovine pancreatic trypsin inhibitor \cite{troyerProteinConformationalLandscapes1995}. Similar machine learning methods, which find low dimensional representations of the dynamics while preserving various metrics, have also been used.  Isomap \cite{tenenbaumGlobalGeometricFramework2000} preserves the distances between conformations on a curved surface (manifold), the geometry of which is inferred from the observed conformations. A computationally efficient method of Isomap (SciMAP) \cite{dasLowdimensionalFreeenergyLandscapes2006} was used to determine the protein folding reaction coordinate for SH3 domain. Sketch-map \cite{ceriottiSimplifyingRepresentationComplex2011} preserves only certain subsets of distances deemed to be important and has been used to understand the unfolding dynamics and the effect of point mutations of a beta hairpin polypeptide \cite{ardevolProbingUnfoldedConfigurations2015}. 
Diffusion map \cite{fergusonNonlinearDimensionalityReduction2011} and locally scaled diffusion map, preserve diffusion distances (i.e., how easily states can diffuse to one another). They have been used to characterise folding pathways in a number of small proteins:  Trp-cage \cite{kimSystematicCharacterizationProtein2015}, a beta-hairpin \cite{zhengDelineationFoldingPathways2011} and in Microcin J25 \cite{fergusonNonlinearDimensionalityReduction2011}. 

\section{Markov models and their applications}\label{sec:mm_applications}
An alternative to the techniques described above are Markov models.  Markov models provide a framework for classifying conformations into metastable states, finding reaction pathways and estimating kinetic and thermodynamic quantities. While they are able to incorporate knowledge of important order parameters or features they do not require such knowledge \cite{husicMarkovStateModels2018, pandeEverythingYouWanted2010}.

The central idea \cite{zwanzigClassicalDynamicsContinuous1983a} \hl{is that for complex systems, over a sufficiently long periods of time, the rate at which the system transitions out of region $A$ and into region $B$ of configurational phase space is not dependent on how the system arrived at $A$. In other words, these transitions are ``memoryless''}. In mathematical notation this is\cite{noeTransitionNetworksModeling2008}: 

\begin{equation}\label{eqn:masterequation}
    \frac{\mathrm{d}\mathbf{p}(t)}{\mathrm{d}t} = \mathbf{p}(t)\mathbf{K},
\end{equation}

where the elements of $p_{i}$ represent the probability of the system being in a region of phase labelled by $i$; $K_{ij}$ is the rate of transitioning from region $i$ to $j$, and $K_{ii} = -\sum_{j\ne i} K_{ij}$. In addition to being ``memoryless'', equation \ref{eqn:masterequation}, also implies that the rate matrix $\mathbf{K}$ does not change with time, i.e., the system is ``stationary'' \cite{zwanzigClassicalDynamicsContinuous1983a}.  The solutions to this equation describe how the probability of the system being in discrete regions of phase space changes smoothly over time. The justifying assumption is that biomolecular systems have a free energy surface (the free energy with respect to some set of coordinates) which is is characterised by many local minima, arising due to the many degrees of freedom afforded by its large structure (e.g., rotations about bonds or dihedral angles). The system resides in these minima for a sufficiently long period of time, $\tau$, that transitions between become independent of one another. 

The Markov model approach to solving equation \ref{eqn:masterequation} is by considering a discrete time process ($t = \tau, 2\tau, \ldots$), where the partitioning the configurational phase space into $n$ discrete states, where the dynamics of the system can be described by an $n\times n$ transition matrix $\mathbf{T}$ \cite{prinzMarkovModelsMolecular2011}. Each element of $\mathbf{T}$ describes the conditional probability of the system jumping between states over a time period, $\tau$, the Markov lag time \cite{prinzMarkovModelsMolecular2011}, i.e., \cite{noeTransitionNetworksModeling2008}: 
\begin{equation}\label{eqn:discretemasterequation}
    \mathbf{p}((k+1)\tau) = \mathbf{p}(k\tau)\mathbf{T}
\end{equation}
\hl{The eigenvectors and eigenvalues of $\mathbf{T}$ represent the associated slow dynamic processes and their associated timescales} \cite{prinzMarkovModelsMolecular2011}. 

Applications of MMs are concentrated on biomolecular systems and form an intrinsic part of the biomolecular simulation tool-box \cite{hugginsBiomolecularSimulationsDynamics2019}. Applications include modelling both protein folding pathways \cite{singhalUsingPathSampling2004,swopeDescribingProteinFolding2004} as well as intrinsically discorded proteins \cite{schorAnalyticalMethodsStructural2016a}. 
MMs have been applied to enzyme systems and used to elucidate, for example, ligand docking pathways \cite{ahalawatMappingSubstrateRecognition2018a} and regioselectivity mechanisms in cytochrome p450 \cite{dodaniDiscoveryRegioselectivitySwitch2016a}, the conformational heterogeneity in the important cancer target SETD8 \cite{chenDynamicConformationalLandscape2019a}, loop dynamics in triosephosphate isomerase \cite{LoopMotionTriosephosphate}, and allosteric effects in cyclophilin A \cite{wapeesittipanAllostericEffectsCyclophilin2019}. Other applications include self-assembly \cite{senguptaAutomatedMarkovState2019} and dimer formation \cite{leahyCoarseMasterEquations2016} of amyloid peptides, identifying important conformations in drug targets to improve drug docking free energy calculations \cite{amaroEnsembleDockingDrug2018}, and rational drug design \cite{gervasioBiomolecularSimulationsStructureBased2019}. There \hl{has been comparatively less use of MM on smaller systems} (whose kinetics tend to be derived from quantum mechanical and thermodynamic data \cite{glowackiMESMEROpenSourceMaster2012, pillingMasterEquationModels2003}, rather than statistically estimated from MD data), however, one recent example used MMs to determine hydrogen bond rearrangement in liquid water \cite{schulzCollectiveHydrogenbondRearrangement2018}. 

% MMs are a varied collection of techniques covering a range of different simulation conditions and models of dynamics and for recent reviews of the full range of techniques and applications see references \cite{husicMarkovStateModels2018, noeMarkovModelsMolecular2019b}. 
Early MM construction consisted of describing conformational dynamics of systems in thermal equilibrium by constructing only a handful of discrete states, and modelling the dynamics as Markov chain also known as a \emph{Markov state model} (MSM). For example the authors of \cite{degrootEssentialDynamicsReversible2001} investigated the folding of a $\beta$-heptapeptide into a $\beta$-hairpin conformation in a solution of methanol.  To decide whether the folding process is a memory-less process (i.e., conforms to equation \ref{eqn:masterequation}) they estimated a four state MSM and compared the transition probabilities implied by the model (i.e, from the elements of the $4\times 4$ matrix $\mathbf{T}$) to those observed in the MD trajectory. The four states were based on their geometric similarity in the space of a principal component analysis of the peptide backbone coordinates. They found them to be in good agreement and so concluded that their reduced dimension description of the folding process was valid. 


However, the more common approach \cite{husicMarkovStateModels2018, noeMarkovModelsMolecular2019b} is a two stage process. In the first stage, frames from MD simulations are  geometrically clustered into $n$ discrete \emph{microstates} (where typically $n \lesssim 1000$) and the elements of $\mathbf{T}$ are estimated in this microstate basis. The purpose of this discretization is to allow an precise description of the eigenvectors of $\mathbf{T}$ in terms of these microstates \cite{perez-hernandezIdentificationSlowMolecular2013a}. The assumption behind the validity of this approach is that with a fine-grained definition of microstates (i.e., each microstate is structurally very similar) their geometric similarity is enough to guarantee their kinetic similarity.  Care must be taken here as geometric similarity does not always imply kinetic similarity, so that structure which appear similar according to some metric (e.g. RMSD deviation) may have very disimilar kinetic properties \cite{bowmanUsingGeneralizedEnsemble2009a, krivovHiddenComplexityFree2004, noeTransitionNetworksModeling2008, berezovskaAccountingKineticsOrder2012}. The methods for creating this fine-grained kinetic model as well as alternatives to geometric clustering will be described in this section.


The second stage makes use of the fact that very often the case that there is a separation in timescales between the slow processes of interest and other processes. To take the triosephosphate isomerase example of earlier, loop 6 opens and closes on the timescale of \SI{100}{\micro\second} \cite{LoopMotionTriosephosphate}, whereas bond vibrations and rotations which are not immediately relevant to the loop motion are on the order of \SIrange{1}{1000}{\femto\second}. This fact allows coarse-graining these microstates into a handful, $g$, of \emph{macrostates} based on their kinetic properties. The macrostates are usually defined such that microstates have a low probability of transitioning between the macrostates, compared to inter-conversion within a macrostate \cite{schutteDirectApproachConformational1999,swopeDescribingProteinFolding2004, prinzMarkovModelsMolecular2011}. Coarse-graining will be discussed in depth in sections \ref{sec:intro_coarse} and \ref{sec:num_metastable}. 

% In order to recover the metastable conformations from an MSM a number of \emph{coarse graining} schemes have been introduced. One of the first was Perron-cluster cluster analysis, PCCA \cite{deuflhardIdentificationAlmostInvariant2000a} and robust PCCA \cite{deuflhardRobustPerronCluster2005b}, which used the eigenvectors of $\mathbf{T}$ to determine the metastable states. Later, hidden Markov models (HMMs), an already well known statistical model \cite{rabinerTutorialHiddenMarkov1989}, were introduced  which  identified the metastable states of the system with the hidden states of a HMM \cite{noeMarkovModelsMolecular2019b}. 

% [Example of lumping with simple MSM]

One approach to creating accurate MSMs is to focus on finding the ``essential degrees of freedom'' of the system \cite{zwanzigClassicalDynamicsContinuous1983a, schutteDirectApproachConformational1999} i.e., a small (compared to the number of atomic coordinates) number of features or order parameters which describe in the slow processes being studied. Examples of order parameters for protein folding, for example, include the root mean square deviation from the crystal structure, the fraction of contacts found in the crystal structure, or even thermodynamic quantities like energies arising from solvent interactions \cite{chongExaminingThermodynamicOrder2018}. There are several benefits to identifying these features before clustering microstates to estimate an MSM. First, geometric similarity as measured in the space of the order parameters is more likely to correlate with kinetic similarity than atomic coordinates than atomic coordinates (this will be discussed below in reference to time-lagged independent component analysis).  Second, k-means clustering, a popular method for performing geometric clustering, has a computational complexity which scales with the number dimensions. So reducing the number of variables from \numrange{1000}{10000} (the order of the number of atomic coordinates for typical protein) to \numrange{10}{100} for typical number of molecular features, e.g., dihedral angles of protein backbone, represents a large reduction in computational complexity.\footnote{The modal number of atoms of structures in the Protein Data Bank is \numrange{2000}{3000} according the web page: https://www.rcsb.org/stats/distribution-atom-count, accessed 27th June, 2021.}


As already discussed there have been many statistical and machine learning techniques for finding collective variables for describing the slow conformational dynamics. However, many of these techniques do not directly address capturing the slow dynamics. For example, the problem with using PCA as a preprocessing step before clustering into microstates is that the principal components retain the greatest \emph{configurational} variance \cite{perez-hernandezIdentificationSlowMolecular2013a}. A similar technique, time-lagged independent component analysis, TICA \cite{perez-hernandezIdentificationSlowMolecular2013a, schwantesImprovementsMarkovState2013}, was introduced which identifies linear combinations of the atomic positions which are maximally time correlated at a given lag time. Using TICA to reduce dimension by keeping the first $m$ TICA components retains the greatest \emph{kinetic} variance \cite{noeKineticDistanceKinetic2015}. The total kinetic variance describes the ability  the TICA components (or any basis set) to capture the slow dynamics of the system. Using TICA as a preprocessing step in MSM construction has been systematically investigated and shown to be more accurate than both PCA and no preprocessing at all \cite{husicOptimizedParameterSelection2016} in capturing the slow dynamics. However, it brings with it two new modelling choices (aside from the number of microstates, $n$): the TICA lag-time (also called $\tau$ but not necessarily the same as the Markov lag-time $\tau$) and the number of independent components to retain, $m$.  

% With large biomolecules, determining the essential degrees of freedom is difficult because of the systems high ambient dimension. This was apparent outside the context of MSMs as the authors of  reference \cite{karpen1993statistical} note in 1992: 
% ``It is becoming essential to develop methods which reduce the complexity of data resulting from these long simulations while retaining the relevant information." The method they proposed to investigate the mechanism of peptide turn formation was to use a statistical clustering algorithm (where the data determine the cluster definitions) to cluster the pentapeptide configurations in the space of the backbone ($\phi, \psi$) dihedral angles. Rather than data based clustering, the authors of reference \cite{mckelveyCHARMMAnalysisConformations1991} use predetermined regions of the ($\phi, \psi$) plane to describe the conformational landscape of the metastasis-inhibiting laminin peptide.  

The variational approach to conformational dynamics, VAC \cite{nuskeVariationalApproachMolecular2014}), \hl{cast estimating MSMs and TICA as a variational optimisation problem}. While VAC showed that TICA and MSMs were the optimal description of the slow dynamics for a \emph{given} linear continuous and discrete basis set (respectively), the authors of reference \cite{mcgibbonVariationalCrossvalidationSlow2015} showed that the same variational principle could optimize the basis sets themselves. The key innovation of this work was to combine cross-validation \cite{arlotSurveyCrossvalidationProcedures2009} and the \hl{variational principle} to score a given basis set using the generalized matrix Rayleigh coefficient, GMRQ. The theory was then broadened with Koopman models to encompass simulations of systems out of thermal equilibrium \cite{wuVariationalKoopmanModels2017}. With the variational approach to Markov processes (VAMP) the theory of MSMs and Koopman models was unified into one conceptual framework. This increased the scope of MMs and presented a range of model scoring metrics (VAMP scores), of which the GMRQ was a special case. These theoretical advances have allowed the development the following iterative optimisation MM pipeline, starting with a set of MD trajectories:

\begin{enumerate}
    \item project coordinates on to important features;
    \item geometrically cluster into discrete microstates;
    \item estimate MSM and score using VAMP (GMRQ);
    \item repeat the previous steps by varying the type of feature, number of discrete states, etc., until a satisfactory VAMP score has been achieved.   
\end{enumerate}

Other approaches to building MSMs exist which don't focus on preprocessing MD trajectories by projecting onto suitable order parameters. VAMPnets \cite{mardtVAMPnetsDeepLearning2018}, still utilises the variational framework but instead of discretising MD trajectories, uses a deep neural networks to learn continuous, non-linear estimates of the eigenvectors of $\mathbf{T}$, from the atomic coordinates of MD trajectories directly, mitigating the need to iteratively select and score features.  Enspara \cite{porterEnsparaModelingMolecular2019} is a package which facilitates clustering large volumes of MD data without the need to perform dimensionality reduction first. For example, coordinate trajectories of a cefotaximase \cite{hartModellingProteinsHidden2016} were clustered into based on similar values of RMSD relative to .  By not coarse graining these microstates into macrostates, or projecting the coordinates onto the slowest collective variables, this technique emphasizes are larger range of dynamic processes (not just the slowest ones). As a result, the authors were were able to reveal important conformations, not captured in X-ray and other structural data to explain the enzyme's specificity and antibiotic resistance.  

Another similar approach to Markov state models  for understanding conformational dynamics is discrete path sampling \cite{walesEnergyLandscapesCalculating2006}. DPS solves equation \ref{eqn:masterequation} by creating  microstates based on their kinetic properties rather than their geometric properties, using the potential energy surface, rather than MD trajectories. First a database of local minima (which define the discrete microstates) and saddle points (corresponding to transition states between the microstates) are created by geometry optimisation using the potential energy of the system. Then, the elements of $\mathbf{K}$ can be estimated from the values of the potential energy in the microstates and transition states using transition state theory \cite{wales_2004}. This method is limited by the number of degrees of freedom (which increase the fluctuations in the potential energy surface) which for biomolecules can become prohibitively large\cite{noeTransitionNetworksModeling2008}. However, using implicit solvent models to limit the number of degrees of freedom the conformational dynamics of small and medium sized systems have been investigated. These include the folding dynamics of met-enkephalin \cite{evansFreeEnergyLandscape2003a} and tryptophan zipper peptide \cite{josephStructureThermodynamicsFolding2016}, characterising the free energy surface of intrinsically disordered proteins 
\cite{josephIntrinsicallyDisorderedLandscapes2018}, and the effect of point mutations on the a coiled-coil peptide \cite{roderTransformingEnergyLandscape2017}. 

\textbf{Chapter \ref{chap:theory}} sets out the theory of MMs relevant to this thesis which focuses on MSM estimation, TICA for preprocessing, variational optimisation of basis sets using VAMP scores. 

% The importance of selecting appropriate features for MSM analysis also quickly became important. In combination with the domain knowledge of chemists and biochemists, dimensionality reduction \cite{friedman2001elements} techniques became popular.  Principal component analysis (PCA) \cite{leverPrincipalComponentAnalysis2017} identifies linear combinations, or `principal components', of variables which explain the variance in a given set of data. The first principal component (PC1) is a vector which points in the direction of the greatest variance, PC2 is orthogonal to PC1 and lies in the direction of the second-most variance etc. Projecting the data onto the first $m$ principal components (where $m$ is typically much less than the number of original dimensions \cite{leverPrincipalComponentAnalysis2017}) which explain the majority of the variance, while ignoring the rest, is a way of reducing the dimension of the system. Using MD simulations of a $\beta$-heptapeptide, the authors of reference \cite{degrootEssentialDynamicsReversible2001} used the first three principal components as features and clustered the trajectories to understand the folding process in terms of its hydrogen bonding network. 
% Determining these metastable states, their kinetics, and structural characteristics from MD simulations are the main goals of a MM analysis and the remain a topic of continued interest for both method development and applications \cite{husicMarkovStateModels2018,noeMarkovModelsMolecular2019b, wangConstructingMarkovState2018c}.  


\section{Simplified Markov state model construction}
 The MSM literature has concentrated on large biomolecules because the assumption of a rugged, free energy surface (as described in section \ref{sec:mm_applications}) giving rise to ``memoryless'' conformational transitions is a valid one. As already mentioned, the success of the MM process is the creation of a good set of microstates to represent the slow dynamics of the system using the essential degrees of freedom as an initial set of variables over which to cluster. However, for biomolecules the process is complicated by the large number of potentially relevant degrees of freedom which cannot be determined a-priori \cite{shallowayMacrostatesClassicalStochastic1996}. However, for systems with a much smaller number of relevant degrees of freedom, chemical intuition and  visualisation techniques can be used to guide the choice of collective variable.

\textbf{Chapter \ref{chap:water}} describes computational and experimental work designed to understand the diffusion of a single water molecule through a sucrose matrix, designed to mimic the conditions of water diffusion in secondary organic aerosol (SOA) droplets  \cite{songTransientCavityDynamics2020a} (i.e., aerosol consisting of organic molecules dissolved in water \cite[chapter 1]{stepheningramCausesMagnitudesAtmospheric2019}). 

The motivation for studying aerosol in general is that they have wide ranging impacts on human and planetary health \cite{Ingram2017}, from smog in cities [SI3], directly affecting the radiative balance of the atmosphere by altering its chemical composition [SI4] and indirectly through their effect on cloud formation [SI5]. As to SOAs in particular, they have been increasing recognised as an important source of total atmosphereic aerosol [SI10-12], alongside the more well known primary sources such as ocean spray, smoke from natural and man-made sources et cetera [SI7?]. The water content of SOA influences its chemical reactivity \cite{varutbangkulHygroscopicitySecondaryOrganic2006} and physical properties like size and refractive index \cite{steimerElectrodynamicBalanceMeasurements2015,tangSimultaneousDeterminationRefractive1991}. Predicting water diffusion in SOA is therefore important to explaining a range of SOA phenomena. 

The Stokes-Einstein (S-E) definition of diffusion, $D$, relates the viscosity of the solvent, $\eta$, (sucrose, in this case) to the hydrodynamic radius of the solute, $a$, at a temperature $T$ is given by \cite[chapter  17]{dill2010molecular}:
\begin{equation}\label{eqn:diffusion_intro}
D=\frac{k_{\mathrm{B}} T}{C \pi \eta a},
\end{equation}
($C$ and $k_{B}$ are constants).  For SOA droplets existing in the low humidity parts of the atmosphere, water evaporates to the point that the organic constituents of the particle become the dominant mole fraction leaving water as the solute \cite{powerTransitionLiquidSolidlike2013, Price2014, Molinero2005}. In these regimes, large deviations from S-E diffusion occur\cite{powerTransitionLiquidSolidlike2013,Price2015,Chenyakin2017}. There is a continuing debate over the applicability of the S-E description of diffusion in SOA droplets (see chapter 7 of \cite{Ingram2017} for a review), with different ad-hoc modifications of the S-E being suggested\cite{Harris2009,price2016sucrose, Fernandez-Alonso2007} as well as a case being made for entirely new theories \cite{saltzmanActivatedHoppingDynamical2006}. For the system studied in this thesis, the observed diffusion rate is much larger than that predicted from the observed viscosity of the sugar component and the water radius. These deviations occur when the viscosity is so high that the aerosol droplets start to transition to a glassy state \cite{Bones2012}. The motion of sucrose matrix in this case becomes slow on the timescale of the motion diffusing water molecules, but not so slow that it can be considered stationary. 

The aim of chapter \ref{chap:water} is to both add to the debate over water diffusion in SOA by suggesting a microscopic mechanism for water diffusion in a system with large deviations from S-E behaviour, and to show that the iterative,  variational approach to building Markov states models described in the previous section is not always necessary. Instead a simplified approach  utilising chemical knowledge and intuition can be used to construct valid and informative MSMs. The MSM approach is justified because the interactions of the water molecule with the much large sucrose molecules creates a sufficiently complex free energy landscape that the ``memoryless'' assumption for configurational transitions holds.  However, the assumption that the transition rates do not change with time was not met due to the slow but persistent motion of the sucrose matrix. Another aim of this  chapter was therefore to demonstrate a simple way of accounting for non-stationary transition rates when constructing MSMs.  


\section{Evaluating Markov state model performance}\label{sec:intro_msm_perf}
The MM analysis pipeline described so far, consists of first transforming MD trajectories into features (the essential degrees of freedom, $\chi$), then reducing the dimension with TICA,  discretizing the TICA components into $n$ microstates, and finally estimating the MSM. The modelling choices or \emph{hyperparameters}, ($\chi, \tau, m, n$), create the MSM basis set, which in turn determine the accuracy of the resulting MSM, and so a method of evaluating the performance of these hyperparameters  is needed. While the ground truth of the kinetic processes is not available, the initial way forward came through cross-validation and the GMRQ \cite{mcgibbonVariationalCrossvalidationSlow2015}. 

The innovation in reference \cite{mcgibbonVariationalCrossvalidationSlow2015} was to create a model score, the GMRQ (the Rayleigh trace from quantum mechanics), which could be used to judge the quality of the model choices while accounting for the tendency of models to fit to noisy signals in the data (over-fitting). This was achieved through cross-validation \cite{arlotSurveyCrossvalidationProcedures2009}: a model is estimated using a portion of the data and scored on the remaining data. Maximizing the cross-validated GMRQ by varying the hyperparameters increases the accuracy of the eigenvectors \cite{mcgibbonVariationalCrossvalidationSlow2015}. The GMRQ is a special case of the first VAMP score, VAMP-1, while maximizing the total kinetic variance is the same as maximizing the VAMP-2 score. These VAMP metrics completed the analysis pipeline \cite{schererVariationalSelectionFeatures2019} which now can be summarised as: i) transform MD trajectories into features, $\chi$, ii) select reasonable choices of  hyperparameters (features, TICA parameters, number of discrete states) and calculate the cross-validated VAMP-2 score, iii) change the hyperparameters and repeat analysis, iv) stop when the VAMP-2 score stops increasing. 

\section{Hyperparameter optimisation}\label{sec:intro_hyper_opt}
Choosing the hyperparameters which maximize the VAMP-2 score is a `black-box' optimisation problem \cite{jonesEfficientGlobalOptimization1998a}, so called because no gradient information on the response of the VAMP to the hyperparameters is available. This is a common problem in the machine learning community where models have many parameters and may take days to train \cite{feurer2019hyperparameter}. In this case it is not feasible to exhaustively search through combinations of hyperparameters. A popular method for optimising large sets of hyperparameters is Bayesian optimisation (also known as sequential model based optimisation, SMBO) \cite{hutterSequentialModelbasedOptimization2011,NIPS2012_4522,bergstraAlgorithmsHyperParameterOptimizationa,bergstraMakingScienceModel2013}. The idea behind Bayesian optimisation is that there is an objective function which is costly to optimise \cite{brochuTutorialBayesianOptimization2010,shahriariTakingHumanOut2016} (in this case the VAMP-2 score). So instead of optimising this directly, the BO procedure builds an statistical model of  objective function known as a \emph{surrogate function} or \emph{response surface}, using randomly sampled  values of the objective function. Having built an initial response surface, searching for the next hyperparameter to evaluate is guided by an \emph{acquisition function}. These can be selected or adjusted to trade off high-uncertainty regions (the `explore' regime) of the response surface with the high-value, low-uncertainty  regions (the `exploit' regime) \cite{shahriariTakingHumanOut2016}. A suggestion is evaluated, the response surface updated and the process repeats. Bayesian optimisation for hyperparameter optimisation is popular, as the number of packages designed for this purpose will attest (this list is non-exhaustive): Hyperopt \cite{bergstraHyperoptPythonLibrary2013}; sequential model-based algorithm configuration \cite{hutterSequentialModelbasedOptimization2011}, SMAC; BayesOpt \cite{martinez-cantinBayesOptBayesianOptimization2014}; Spearmint \cite{DBLP:conf/uai/GelbartSA14,snoekAbstractBayesianOptimization2013,snoekInputWarpingBayesian2014a,NIPS2013_5086,NIPS2012_4522}, GPyOpt \cite{gpyopt2016}, DragonFly \cite{JMLR:v21:18-223}; Auptimiser \cite{liuAuptimizerExtensibleOpenSource2019}; and Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016}. A popular choice of response surface model is a Gaussian process (GP) \cite{rasmussenGaussianProcessesMachine2006}, a highly flexible type of model which fits naturally within the Bayesian optimisation paradigm \cite{shahriariTakingHumanOut2016}. Indeed, six of the eight packages listed here all implement some kind of Gaussian process  as their response surface model.

\textbf{Chapter \ref{chap:msm}} demonstrates the use Bayesian optimisation to optimise the MSM hyperparameters using cross-validated VAMP-2 scores of the model system alanine dipeptide. In addition, the parameters of GPs are explored as a way to describe the relevance of hyperparameters in determining the VAMP-2 score. This chapter lays the ground-work for performing a similar analysis on AADH in chapter \ref{chap:aadh}, in particular: how to fit and interpret GPs and how to use GPs with Bayesian optimisation to optimise hyperparameters. 

\section{Coarse-graining}\label{sec:intro_coarse}

Having arrived at $n$ microstates via an optimal set of MSM hyperparameters by maximising the kinetic variance (VAMP-2), the second  stage in the MM pipeline is to coarse grain potentially thousands of microstates into a handful of macrostates to create a more interpretable model. 

However, coarse graining an existing MSM is not the only approach to gaining insight into the conformational landscape of biomolecules.  There are other statistical clustering techniques that have been used for this purpose. The authors of \cite{troyerProteinConformationalLandscapes1995} used hierarchical clustering \cite[chapter 10 of]{friedman2001elements} to group MD frames into groups with mutual root mean square deviation (RMSD) in their alpha-carbon positions below some small threshold value. Hierarchical clustering shows how conformations cluster together as the threshold RMSD is increased.  In this way the conformational landscape at different levels of spatial resolution can be determined and the number of clusters determined by other criteria (the authors considered a RMSD separation which give rise to different minimized conformations).  In \cite{karpen1993statistical} the authors used a neural network clustering algorithm, ART-2$^{\prime}$ \cite{carpenterARTSelforganizationStable1987} to investigate the folding mechanism of a pentapeptide. Folding events were described by up to six different clusters where the clustering took place in the space of residue dihedral angles. The number of clusters was determined by considering the extent of the clusters in the dihedral space, compared to those expected from thermal fluctuations, as opposed to those from conformational change. 

The main drawback of clustering based on geometric measures of similarity are that metastable macrostates are actually defined by their kinetic properties (i.e., conformations in a metastable macrostate undergo rapid, mutual, inter-conversion over a given timescale) which are not necessarily the same \cite{schutteDirectApproachConformational1999} as configurational similarity as already discussed in section \ref{sec:mm_applications}. Kinetic clustering dates back to at least 1969 when Kuo and Wei \cite{weiLumpingAnalysisMonomolecular1969, kuoLumpingAnalysisMonomolecular} investigated the conditions under which both exact and approximate coarse graining of systems of coupled first order reactions could occur. The term \emph{exact} implying that the coarse-grained description gave rise to kinetic description consistent with the underlying microscopic kinetics.  Hummer and Szabo \cite{hummerOptimalDimensionalityReduction2015a} tackled the problem of how to define an appropriate coarse-grained rate matrix (where the entries relate kinetic rates of transitioning from one microstate to another, rather than probabilities in the transition matrix picture) for a given coarse-graining scheme. i.e., given a mapping of micro- to macrostates, what is the most appropriate way of defining the rate matrix.  They derived expressions for rate matrices which are exact for non-Markovian dynamics (i.e., for systems where transition probabilities are dependent on the history of states visited) for a given coarse graining scheme.  They also derived expressions for the case of Markovian dynamics which, while approximate, still ensured that the macrostate relaxation timescales were exact. In addition, they proposed an algorithm for optimally determining this coarse-graining by iteratively partitioning microstates, such that the timescales implied in their rate matrices are maximized at each step. The authors of \cite{kellsCorrelationFunctionsMean2020} went on to show that this scheme was equivalent to constructing coarse-grained rate matrices which preserve the mean-first passage times of the system. This work has been developed to identify not only metastable macrostates but also the comparatively short lived transition states \cite{martiniVariationalIdentificationMarkovian2017}. 

Other methods, solely based on identifying metastable macrostates have been developed. Perron Cluster Cluster Analysis (PCCA) \cite{deuflhardIdentificationAlmostInvariant2000a} and its subsequent `robust' alternative PCCA+ \cite{deuflhardRobustPerronCluster2005b} use the sign structure of a given number of slow eigenvectors of $\mathbf{T}$ to determine the boundaries between macrostates: two microstates are in the same macrostate if they have the same sign for a given eigenvector.  The hierarchical Nystr{\"o}m exstension graph method is a hierarchical clustering method based on the Nystr{\"o}m approximation for approximating $\mathbf{T}$ \cite{yaoHierarchicalNystromMethods2013a} with a submatrix of well sampled states. This submatrix retains the same sign structure of the full matrix and uses this sign structure with PCCA to cluster microstates. The Bayesian aglomerative cluster engine (BACE) \cite{bowmanImprovedCoarsegrainingMarkov2012a} uses Bayesian statistics to assess whether two microstates have the same transition rates to each putative macrostate - if they do then they are lumped into the same state. Other methods include the most probable path \cite{jainIdentifyingMetastableStates2012a} which assigns microstates to the same macrostate if they occur on the sequence of states with the highest transition probabilities; methods based on the renormalisation group \cite{orioliDimensionalReductionMarkov2016c};  minimum variance cluster analysis, MVCA \cite{husicMinimumVarianceClustering2018} which uses the hierachical clustering to merge rows of the transition matrix; and projected Markov models, PMMs \cite{noeProjectedHiddenMarkov2013a} which include observer operator models \cite{wuProjectedMetastableMarkov2015} and hidden Markov models \cite{noeProjectedHiddenMarkov2013a} (HMM) which are dynamical models in which the microstate/macrostate coarse-grained structure is directly incorporated into the model definition. Specifically, a HMM is a model of a Markovian process between $g$ \emph{hidden} macrostates i.e., states which are not directly observed in the data. While in a macrostate the system emits randomly, according to a probability distribution, to one of a set of \emph{observed} microstates, which \emph{are} seen in the data. The emission distributions thus define the coarse-graining. 

\section{Number of metastable states}\label{sec:num_metastable}
When coarse-graining MSMs (or performing any type of cluster analysis) a key parameter is the number of clusters $g$, e.g., does the data support the hypothesis of $g=2$ or $g=3$ (say) clusters \cite{milliganExaminationProceduresDetermining1985}. This is important as each macrostate pertains to conformations important to the dynamical process being studied \cite{frauenfelderEnergyLandscapesMotions1991}. If the number of macrostates modelled are too few, then important conformations will be lost, whereas with too many macrostates, the model loses its interpretability and can potentially  create macrostates which are artifacts of noise in the data, a processes called `over-fitting' \cite[chapter 7]{friedman2001elements}. The dynamics of proteins is hierarchical \cite{frauenfelderEnergyLandscapesMotions1991} with short lived states aggregating to longer lived states, and as such kinetic clustering must always be in relation to some timescale. However, even given this timescale, the coarse graining and clustering methods so far described do not automatically select the number of macrostates - although some methods provide more information than others to help guide selection. A general approach to determining the appropriate number of macrostates is to look for gaps in the eigenvalues of the transition matrix or its implied timescales \cite{prinzMarkovModelsMolecular2011, mcgibbonVariationalCrossvalidationSlow2015, deuflhardIdentificationAlmostInvariant2000a}. However, due to poor microstate construction or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}, additionally, this also does not allow for transition state macrostates \cite{martiniVariationalIdentificationMarkovian2017}. The variationally optimized coarse-graining of reference \cite{martiniVariationalIdentificationMarkovian2017} allows the user to test whether a transition macrostate has been found but leaves the decision to continue finding more macrostates up to the user. Hierarchical approaches such as BACE,  HNEG and MVAC, automatically show how the microstates group together for different values of the clustering criteria, allowing the user to judge the most useful clustering.  A more general method for MSMs using Bayesian statistics has been developed, which takes as its data the mapping between the micro- and macrostates \cite{bacalladoBayesianComparisonMarkov2009a} and so is independent of clustering method. To decide on an appropriate number of macrostates, the Bayes factor, the Bayesian weight of evidence for a particular hypothesis \cite{kassBayesFactors1995}, for different numbers of macrostates is calculated and used to select $g$. The evidence considered is proportional to the probability of observing the microstates \emph{given} the particular coarse-graining and data\cite{bacalladoBayesianComparisonMarkov2009a}. This method is versatile and naturally takes into account model over-fitting \cite{bacalladoBayesianComparisonMarkov2009a} but it is computational intensive.  

Projected Markov models such as HMMs are distinct to the other techniques described in that they are estimated by maximizing a likelihood function \cite{wuProjectedMetastableMarkov2015, noeProjectedHiddenMarkov2013a} i.e, the probability of observing model parameters given a set of data.  Maximum likelihood models have a wide range of model selection techniques available to them which are not explicitly related to Markov processes but are nevertheless applicable because the Markov property is subsumed into the likelihood function \cite[chapter 7]{friedman2001elements}\cite{milliganExaminationProceduresDetermining1985, mclachlanFiniteMixtureModels2000}.  Some popular techniques include cross-validation \cite{arlotSurveyCrossvalidationProcedures2009}, the Akaike information criterion (AIC) \cite{akaikeInformationTheoryExtension1998}, and the  Bayesian information criterion (BIC) \cite{schwarzEstimatingDimensionModel1978a}. Cross-validation of the log-likelihood (CVLL) has been used to estimate the number of macrostates in HMMs \cite{celeuxSelectingHiddenMarkov2008}. The AIC, similarly to CV, uses the likelihood to approximate the out-of-sample predictive accuracy of the model, whereas the difference in BICs for two models is approximately equal to the Bayes factor for those models (this is directly related to the Bayes factor approach of \cite{bacalladoBayesianComparisonMarkov2009a} described previous). Both the AIC and BIC benefit from requiring negligible extra calculation once a model has been estimated and have been used to select the number of microstates in MSMs of conformational dynamics \cite{mcgibbonStatisticalModelSelection2014a} as well as being ubiquitous for general model selection \cite[chapter 7]{friedman2001elements}. However, choosing the number of microstates using the BIC and AIC has not found widespread use\footnote{PUT EVIDENCE HERE}. A BIC-like criterion called the integrated complete data likelihood (ICL) \cite{biernackiAssessingMixtureModel2000a} has been derived specifically for clustering methods such as HMMs and mixture models \cite{mclachlanFiniteMixtureModels2000} (which  group observations into macrostates, albeit without Markov dynamics). The ICL differs from the BIC and Bayes factor approaches in that the evidence it considers is proportional to the probability of observing the microstates \emph{and} the coarse-graining \emph{given} the data \cite{biernackiAssessingMixtureModel2000a,mclachlanFiniteMixtureModels2000}.  The ICL has been used extensively \cite{mclachlanFiniteMixtureModels2000} for mixture models and a recent assessment \cite{brochadoDeterminingNumberComponents2020} finds it performs well across a range of types of mixtures. The CVLL, BIC, AIC and ICL, have been utilised to determine the number of macrostates in HMMs, but yet not within biomolecular dynamics context.  


\textbf{Chapter \ref{chap:hmm}} explores the utility of approximations to the Bayes factor and similar criteria for determining the optimal value of $g$: the Bayesian information criterion, BIC \cite{schwarzEstimatingDimensionModel1978a}, the integrated complete data likelihood criterion, ICL \cite{biernackiAssessingMixtureModel2000a}, the Akaikie information criteria, AIC \cite{akaikeInformationTheoryExtension1998}, and cross-validated log-likelihood, CVLL \cite{celeuxSelectingHiddenMarkov2008}. The aim of this chapter is to determine which of these statistical model selection criteria can determine the number of metastable states in a coarse-grained description of the model system of the four-well Prinz potential. The benefits of the these criteria (except CVLL) are that they require little additional calculation after estimating a model, in contrast to the full Bayes-factor method of reference \cite{bacalladoBayesianComparisonMarkov2009a}.  This chapter lays the ground-work for application to determining the optimal coarse-grained description of the dynamics of AADH in chapter \ref{chap:aadh}. 

% Chapter \ref{chap:msm} demonstrated using response surface methods and Bayesian optimisation to arrive at an optimal MSM. In order to make sense of the potentially thousands of MSM microstates, it is common practice to kinetically lump them into a smaller number of macrostates. There have been a large number of different methods proposed and coarse-graining has been studied since at least 1969 \cite{kuoLumpingAnalysisMonomolecular, weiLumpingAnalysisMonomolecular1969}. The first among the most recent applications of coarse-graining MSMs was Perron cluster cluster analysis, PCCA \cite{deuflhardIdentificationAlmostInvariant2000a}) and its successor, robust PCCA or PCCA+ \cite{deuflhardRobustPerronCluster2005b}. Other methods include the hierarchical Nystr{\"o}m exstension graph, HNEG \cite{yaoHierarchicalNystromMethods2013a}, the Bayesian agglomerative clustering engine, BACE \cite{bowmanImprovedCoarsegrainingMarkov2012a}, methods based on the renormalisation group \cite{orioliDimensionalReductionMarkov2016c, hummerOptimalDimensionalityReduction2015a}, the most probable path algorithm \cite{jainIdentifyingMetastableStates2012a}, a method based on variationally optimising the coarse grained transition matrix \cite{martiniVariationalIdentificationMarkovian2017a}, minimum variance cluster analysis, MVCA \cite{husicMinimumVarianceClustering2018}; and projected Markov models, PMMs \cite{noeProjectedHiddenMarkov2013a}. PMMs are Markov processes projected onto a coarse grained set of states and are exactly described by observable operator models, OOMs \cite{wuProjectedMetastableMarkov2015}, and approximated by hidden Markov models, HMMs \cite{noeProjectedHiddenMarkov2013a} under certain conditions (HMMs have also been proposed to describe protein dynamics, from other, non-coarse-graining perspectives \cite{mcgibbonUnderstandingProteinDynamics}). OOMs are a generalisation of HMMs \cite{jaegerDiscretetimeDiscretevaluedObservable} and despite their theoretical advantages it is HMMs that have been more widely used to coarse grain molecular dynamics simulations \cite{mondalAtomicResolutionMechanism2018a, plattnerCompleteProteinProtein2017, panConformationalHeterogeneityMichaelis2016, juarez-jimenezDynamicDesignManipulation2020, wangDynamicalBehaviorVLactamases2019,FastFoldingPathwaysThrombinBinding2018,remingtonFluorescenceQuenching2aminopurinelabeled2019,curado-carballadaHiddenConformationsAspergillus2019,furiniIontriggeredSelectivityBacterial2018,yangMappingPathwayDynamics2018,ahalawatMappingSubstrateRecognition2018,olaposiMembraneBoundTranscriptionFactor2019, xiaoNaBindingModes2019, hansonWhatMakesKinase2019}. 


% This is often done by fitting a hidden Markov model (HMM) to the same microstates used to estimate the MSM \cite{noeMarkovModelsMolecular2019b}, although many other techniques exist (e.g., observer operator models \cite{wuProjectedMetastableMarkov2015}, see chapter \ref{chap:hmm} for a full discussion). A HMM is a model of a Markovian process between $g$ \emph{hidden} states i.e., states which are not directly observed in the data. While in a hidden state the system emits randomly, according to a probability distribution, to one of a set of \emph{observed} states, which are seen in the data. The transition rates and emission probabilities are estimated from the data.  The only hyperparameter stipulated by the user (at least for the discrete HMMs considered here) is the number of hidden states, $g$. To model the metastable dynamics, the number of hidden states of the HMM is set equal to the number of metastable states of the system, $r$ \cite{noeProjectedHiddenMarkov2013a}. The number of metastable states is determined by looking for gaps in the eigenvalue spectrum of the MSM \cite{prinzMarkovModelsMolecular2011,noeProjectedHiddenMarkov2013a}. However, due to poor discretisation or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}. This observation, along with the plethora of other coarse-graining schemes motivated the introduction of Bayes factors \cite{kassBayesFactors1995} for both judging the quality of the coarse graining scheme and the number of metastable states \cite{bacalladoBayesianComparisonMarkov2009a}. The Bayes factor is the ratio of the integrated likelihood of the data given the model parameters \cite{kassBayesFactors1995}, for two competing models. Bayes factors measure the relative evidence of a model (its parameters and hyperparameters) given a trajectory of \emph{observed states} (the microstates). For example, if the Bayes factor between a HMM with $g=2$ and  $3$ hidden states is $100$ then the $g=2$ model is more strongly favoured. 

% The integrated likelihood only takes into account how the model explains the observed states \cite{biernackiAssessingMixtureModel2000a,mclachlanFiniteMixtureModels2000}. It does not factor in directly the hidden state structure of the model. Rather than view the HMM as a coarse graining technique, another approach is to view an HMM and other hidden mixture models as a clustering algorithm \cite{mclachlanFiniteMixtureModels2000} where observed states are classified as belonging to hidden states. An important analogue of the integrated likelihood in mixture model community is the integrated classification likelihood \cite{mclachlanFiniteMixtureModels2000}. This measures the evidence for  both the observed states \emph{and the hidden states} given the model parameters and hyperparameters. 

% \textbf{Chapter \ref{chap:hmm}} explores the utility of approximations to the Bayes factor and similar criteria for determining the optimal value of $g$: the Bayesian information criterion, BIC \cite{schwarzEstimatingDimensionModel1978a}, the integrated complete data likelihood criterion, ICL \cite{biernackiAssessingMixtureModel2000a}, the Akaikie information crteria, AIC \cite{akaikeInformationTheoryExtension1998}, and cross-validated log-likelihood, CVLL \cite{celeuxSelectingHiddenMarkov2008}. This extends previous work \cite{mcgibbonStatisticalModelSelection2014a} which looked at the AIC and BIC for selecting parameters of MSMs. Like chapter \ref{chap:msm} this lays the ground-work for application to the case of AADH in chapter \ref{chap:aadh}. The example used is a four well potential, the Prinz potential \cite{prinzMarkovModelsMolecular2011}, with a well defined number of metastable states against which the BIC, ICL, AIC and CVLL can be judged. The benefits of the these information criteria (except CVLL) are that they do not required extra calculation after a HMM has been estimated and so lend themselves to situations in which a large number of models need to be tested, a situation that will be encountered in this thesis. 

\section{Aromatic amine dehydrogenase}
Aromatic amine dehydrogenase (AADH) oxidizes aromatic amines such as tryptamine by catalysing the proton transfer from a covalently bound Schiff base intermediate to an acceptor aspartate oxygen atom as its rate limiting step \cite{masgrauAtomicDescriptionEnzyme2006}.  AADH is a notable enzyme because it exhibits a large primary kinetic isotope effect: substituting deuterium for the hydrogen being cleaved in tryptamine causes the rate to drop by up to a factor of $55$, one of the highest \cite{masgrauAtomicDescriptionEnzyme2006}.  A drop in the rate is expected when considering the cleavage of the heavier deuterium atoms as the C---D bond has a smaller zero-point energy than the C---H bond, thus increasing the height of potential barrier. However, if the zero-point energy were the only difference contributing to the difference in rates, a KIE of approximately \num{8} would be expected \cite{antoniouLargeKineticIsotope1997}.  The fact that the observed KIE is almost \num{7} times as large implies significant quantum mechanical tunneling \cite{masgrauAtomicDescriptionEnzyme2006, klinmanbeyond2009, basranImportanceBarrierShape2001a}, i.e., at C---H bond distances below the top of the potential energy barrier for the reaction, there is still significant overlap of the hydrogen atom wavefunction with the product state wavefunction, allowing the reaction to proceed.  In addition to the presence of tunneling indicated by the inflated KIE, the KIE of AADH is independent of temperature, despite the fact that the underlying reaction is dependent on temperature \cite{masgrauAtomicDescriptionEnzyme2006}. 

% : the hydrogen atom no longer has to overcome the entire potential energy barrier. The hydrogen atom wavefunction overlaps with product state before the top of the potential energy barrier is reached and so tunnels through the top of the barrier - effectively decreasing the barrier and increasing the rate of reaction.  In addition to its large size, the KIE of AADH is also independent of temperature, despite the fact that rate determining step (for both hydrogen and deuterium substituted substrates) \emph{is} temperature dependent.  

It is the temperature independence of the KIE of AADH and other enzymes such as DHFR, monoamine dehydrogenase and soyabeen lipoxygenase \cite{glowackiTakingOckhamRazor2012b} which has prompted some to turn from the dominant theory of reaction rates for enzymes \cite{klinmanbeyond2009}, namely transition state theory \cite{garciavilocaHowEnzymesWork2004} (TST). The TST picture of reaction rates in the presence of tunneling is as follows. The enzyme-substrate complex (for AADH this is the Schiff-base after reaction with tryptamine), is in thermal equilibrium. Thermal fluctuations along the reaction coordinate move the complex through a transition state, and on to the product state (the oxidized Shiff-base). The rate at which this happens is proportional to $\exp{(-\Delta G^{\mathrm{TS}}/RT)}$, where  $\Delta G^{\mathrm{TS}}$ is the between the reactant and transition states (the activation energy), $R$ is the gas constant and $T$ is the temperature. The zero-point energy difference in isotopes changes the value $\Delta G^{\mathrm{TS}}$. Tunnelling occurs when the thermal fluctuation along the reaction coordinate brings hydrogen atom close enough to the acceptor atom so that wavefunctions of the hydrogen atom and the product state overlap, effectively lowering the value of $\Delta G^{\mathrm{TS}}$ \cite{puMultidimensionalTunnelingRecrossing2006}.  As Klinman and Kohen point out \cite{klinmanHydrogenTunnelingLinks2013}, this model can explain temperature dependent rates and KIEs because difference in zero-point energies is incorporated into differences in $\Delta G^{\mathrm{TS}}$. However, the rate determining step is both temperature \emph{dependent} while the KIE is temperature \emph{independent}. 

To account for this temperature independence a number of other non-TST theories have been invoked. `Marcus-like' theories (which take their name from their similarity to the Marcus theory of electron transfer \cite{marcusElectronTransfersChemistry1985}),  decouple the tunneling effects from the activation energy by factorizing the rate into two terms \cite{antoniouLargeKineticIsotope1997, antoniouInternalEnzymeMotions2001, knappTemperatureDependentIsotopeEffects2002,knappEnvironmentallyCoupledHydrogen2002}. The first term, which requires an activation energy, describes the process of rearranging the heavy atoms into an state ready for tunneling. The second term describes the tunneling process and is therefore  largely determined by the properties of the atom being transferred (the hydrogen or deuterium atom). This separation allows both temperature dependent rates of reaction, with temperature independent KIEs \cite{klinmanHydrogenTunnelingLinks2013, knappEnvironmentallyCoupledHydrogen2002}. Other theories have also been suggested with require effects arising from non-thermal equilibrium effects.  


The implication of tunneling in the reaction mechanism has inserted AADH into the ongoing debate around the r\^ole of enzyme dynamics in catalysis \cite{klinmanHydrogenTunnelingLinks2013, puMultidimensionalTunnelingRecrossing2006, mcgeaghProteinDynamicsEnzyme2011,glowackiTakingOckhamRazor2012b,glowackiProteinDynamicsEnzyme2012}. The 




The term `dynamics' in this context implies a much broader range of effects that those described in the proceeding sections. `Conformational dynamics' will refer to the situations described earlier which are governed by a first order, memoryless rate equation (equation \ref{eqn:masterequation}) with a separation of timescales, resulting in the enzyme transitioning between long-lived metastable conformations. These transition will typically be the slowest motions of interest in the system. At the other end of the spectrum are enzyme bond vibrations, including the vibration of the breaking bond as it transitions to the product state. It is the fast bond vibrations which have prompted some to link protein motion, catalysis and quantum tunnelling \cite{klinmanHydrogenTunnelingLinks2013, puMultidimensionalTunnelingRecrossing2006, antoniouInternalEnzymeMotions2001}.  





It has been suggested that the temperature dependence of the KIEs for different enzymes, including AADH, cannot be explained by transition state theory (TST) \cite{agrawalVibrationallyEnhancedHydrogen2004,kohenEnzymeCatalysisClassical1998}. However, the authors of reference \cite{glowackiProteinDynamicsEnzyme2012a} showed that by using TST to model the reaction as two rapidly interconverting metastable states which can both react to form the product, the temperature dependence of the KIE could be explained. To test this hypothesis, the conformational landscape of AADH has to be determined. 

That conformational dynamics is associated with steps in the catalytic process is undeniable, as the examples of DHFR and TIM given earlier, as well as others \cite{hammesMultipleConformationalChanges2002, hammes-schifferRelatingProteinMotion2006a} show.  However to answer the question of whether a particular feature of the enzyme 


\textbf{Chapter \ref{chap:aadh}} brings together the hyperparameter optimisation work of chapter  \ref{chap:msm} and the hidden state selection work of chapter \ref{chap:hmm} and applies it to the case of AADH. An expanded set of hyperparameters is optimised and understood in terms of the response surface and in addition to the optimized parameters, a set of sensitivity models are proposed. The model selection criteria of chapter \ref{chap:hmm} are used to select the appropriate number of hidden states in the coarse grained HMMs and the results of the sensitivity models compared and discussed. 

\textbf{Chapter \ref{chap:conclusions}}, discusses the conclusions of this thesis and sets out concrete steps for further work in this area. 


% using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 


% Gaussian Processes (GPs) have been


% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 


% with a concomitant increase in the number of software packages 


%  - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% % Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}




% \textbf{From art to science:} 
% MSMs are master equations (ME review [6])
% Model is n x n matrix, spans the conformational space, conditional probabilities as elements. 
% State populations are give thermodynamics, off-diagonal elements give kinetics. 
% If we assume thermodynamic equilibrium then eigendecomposition is useful [description of eigenvectors]
% the n states should capture the dyanmics. 

% Memoryless transition networks come from Zwanzig [1] then Van Kampen [2] key MSM papers [7-9]. 

% creating meaningful states is difficult [33,34] Karpen did dihedrals [10] de Groot [20] did PCA and k-medioids.

% can stitch together different trajectories: McCammon [11] now Folding@home [35], Luty [12] suggested stitching together different trajs for ligand binding. 

% Hardware devs: FoldingAtHome, BlueGened [39-41], GPUGRID [43,44]

% ITS plots [9] CKtest [46]

% Different features different dynamics [45]

% Errors are (1) state decomposition and (2) finite sampling [47]

% Global descriptors worse than internal degrees of freedom [49]

% Sarich [73, 92]: discretization error decreases and partitioning become finer and the lag time increases. 

% Djurdjevac [93]: upper bounds for error between MSM and trajectories decreases with lag time. 

% TICA\c variance explained [115]:
% This MSM score was termed the GMRQ, which stands for
% generalized matrix Rayleigh quotient, the form of the approx-imator (also referred to as the Rayleigh trace).124
% The GMRQ on the validation set will be poor if the model was overfit on the
% training set but better if the model identifies the underlying
% dynamics common to both sets. In 2016, Noé and Clementi115 demonstrated that kinetic variance in a data set can be explained area.
% by summing the squared tICA eigenvalues. Since the variational principle derived in Noé and Nüske95 holds for any strictly nonincreasing weights applied to the scored eigenvalues,96 the kinetic variance can also be used to score models, or to deter- mine how many tICs are needed to explain a given amount of kinetic variance in the data.

% \textbf{Simple MSM}
% Analysis of three water molecules \cite{schulzCollectiveHydrogenbondRearrangement2018} to understand the collective hydrogen bond rearrangement, uses both Euler angles and spherical coordinates for dofs. 








% \textbf{Coarse graining and HMMs}


% \textbf{Hyperparameter search}
% Feature selection: \cite{schererVariationalSelectionFeatures2019}


% \cite{bergstraHyperoptPythonLibrary2013} Hyperopt - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}

% Optunity: \cite{claesenEasyHyperparameterSearch2014}

% \cite{pmlr-v32-hutter14} use random forest and ANOVA to assess parameter importance. 
% \cite{gramacyVariableSelectionSensitivity2013} using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 

% \cite{falknerBOHBRobustEfficient2018a} goes beyond BO and random bandit. 

% \cite{di2018genetic} genetic algorithm for hyperparameter search. 

% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 




















