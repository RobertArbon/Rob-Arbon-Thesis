%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Introduction}
\label{chap:intro}
% This thesis describes the use of Markov models (MMs) to describe the slow conformational  dynamics of two systems at different ends of the complexity scale: water diffusing through an sucrose matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH). In particular this thesis describes contributions to the methods of optimising and selecting the hyperparameters of MMs.

\begin{highlighted}
\textbf{Note on corrections}.  I have highlighted corrections which are pure additions and minor edits in yellow. Unfortunately, the highlighting process introduces new paragraphs and where this is not intended (e.g., I have added content in the middle of an existing paragraph) I have indicated this with a $\hookleftarrow$ symbol. The exception to the highlighting rule are the first three subsections of this chapter which have undergone major restructuring and large amounts of additional text, making highlighting infeasible. 
\end{highlighted}



This thesis describes the use of statistical and machine learning  model selection and optimisation techniques applied to Markov models (MM) for describing the slow conformational dynamics of two biomolecular systems: water diffusing through an sucrose matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH).

\section{Learning conformational dynamics from molecular simulations}


Quantitatively and qualitatively describing the slow conformational dynamics of biomolecular systems is of central importance for understanding their function, chemical, physical and biological properties. For example, conformational changes are at the heart of enzyme catalysis \cite{hammesMultipleConformationalChanges2002b,hammes-schifferRelatingProteinMotion2006, klinmanHydrogenTunnelingLinks2013}: the `induced fit' model of enzyme catalysis \cite{koshlandCorrelationStructureFunction1963} whereby substrate binding induces a conformational change in the enzyme,  explains the ability of enzymes to catalyse only a narrow range of compounds (their specificity). Kinases are a large and important class of enzymes which are involved in a wide variety of cellular processes \cite{edelmanProteinSerineThreonine1987a}. Their active sites are defined by the relative conformations of a highly conserved aspartate-phenylalanine-glycine (DFG) motif and an alpha-helix \cite{ungRedefiningProteinKinase2018a} sequence. Understanding their dynamics is an important part of rational drug design, especially for many cancers in which they play significant role \cite{shuklaActivationPathwaySrc2014}.  Triosephosphate isomerase (TIM) is a paradigmatic system for enzyme catalysis \cite{richardParadigmEnzymeCatalyzedProton2012a}, where loop 6 opens to  allow substrate binding, transitions through \SI{7}{\angstrom} to close and allow the isomerisation to occur, opening again to allow product release \cite{LoopMotionTriosephosphate}. Other biochemical examples include G protein-coupled receptors, a large family of transmembrane proteins involved in extracellular sensing and signalling and are responsible for olfaction, vision and taste \cite{rosenbaumStructureFunctionGproteincoupled2009}.  They transmit information from outside of the cell by way of ligand binding which inducing a series of conformational changes which in turn triggers the response within the cell \cite{bockenhauerConformationalDynamicsSingle2011a}. Large conformational changes are also implicated in the mechanism by which proteins associate with specific parts of DNA, thus enabling a whole host of cellular functions from gene regulation to DNA replication \cite{vandervaartCoupledBindingBending2015}.  

Computational approaches to studying conformational dynamics are important because their allow descriptions with high temporal and spatial resolution beyond the scope of most experimental techniques \cite{hugginsBiomolecularSimulationsDynamics2019}. Typically, one evolves the systems equations of motion using molecular dynamics (MD) to produce a set of trajectories through phase space. These trajectories can be analysed to reveal conformational transition pathways and metastable states and other properties of interest \cite{rohrdanzDiscoveringMountainPasses2013a}. 

There are a wide range of techniques for understanding conformational dynamics from MD simulations which depend on current knowledge of the system and the questions being asked. Path based techniques such as transition path sampling (TPS) \cite{bolhuisTRANSITIONPATHSAMPLING2002, dellagoTransitionPathSampling1999, dellagoTransitionPathSampling2002a}, transition interface sampling (TIS) \cite{vanerpNovelPathSampling2003} and forward flux sampling (FFS) 
\cite{allenSamplingRareSwitching2005} all start with two specified metastable conformations, A \& B, and can be used to estimate rate constants and reaction coordinates of A $\rightleftharpoons$ B without previous knowledge of transition states. TPS works by first proposing an reaction pathway between A and B. Then a statistical ensemble of pathways is generated from this initial path using Metropolis Monte Carlo. TIS and FFS are similar but define interfaces based on an order parameter\footnote{a quantity such as the root mean square deviation, which, while not a true reaction coordinate, varies between the two states.} which separate A and B. Molecular dynamics simulations are used to estimate the flux between the interfaces and hence the rate between A \& B with FFS relaxing the assumption of equilibrium dynamics present in TIS and TPS. These techniques have been applied \cite{juraszekSamplingMultipleFolding2006, juraszekRateConstantReaction2008,velez-vegaKineticsMechanismUnfolding2010} to the model protein folding system Trp-cage \cite{neidighDesigning20residueProtein2002} to elucidate the folding pathway and have replicated some of the experimental microscopic rates in the folding pathway. String methods posit a discrete set of states along a path (or string) of fixed length and moves these states such that the string corresponds to minimum free energy path between A and B \cite{weinane.TransitionPathTheoryPathFinding2010}. String methods have been used to determine the mechanism and binding free energy of platinum based drugs to DNA \cite{elderSequenceSpecificRecognitionCancer2012}. 

Directional milestoning \cite{faradjianComputingTimeScales2004, majekMilestoningReactionCoordinate2010,kirmizialtinRevisitingComputingReaction2011a} is a technique which doesn't require knowledge of specific metastable states, only a  collective variable known to be related to the important conformational changes. Using directional milestoning one can ... First a set of states which cover the relevant conformational space, known as anchors, are selected. The milestones are sets of conformations which separate (as measured by the collective variable) the anchors from one another, and are used to restart trajectories, calculate reaction coordinates and conformational kinetics. Milestoning has been used to understand the selectivity of DNA reverse transcriptase \cite{kirmizialtinHowConformationalDynamics2012} and to understand the permeation mechanism and rate of tryptophan through cell membranes \cite{cardenasUnassistedTransportAcetyl2012}.  

When no previous information such as known metastable states or appropriate collective variables are known, more abstract statistical and machine learning methods have been increasingly shown to be important. Principal component analysis (PCA) finds the linear combinations of coordinates or features of the molecule, such as $\alpha$-carbon coordinates, which maximise the variance \cite{pearson1901liii}. PCA of the heavy atoms of the ribonuclease barnase \cite{noldeEssentialDomainMotions2002} was used to identify high variance/flexible regions of the enzyme which partially explained the role of distal residues in its activity and stability. The authors of \cite{amadeiEssentialDynamicsProteins1993a} used PCA in the  protein lysozyme to identify highly flexible ``essential'' degrees of freedom which are related to the opening and closing of the active site.  One of the draw backs of PCA is that only linear combinations of features make up the principal components.  Kernel PCA, which incorporates non-linear transformations of input features, has also been developed and used with TPS simulations to extract a reaction coordinate for the reaction of lactate  dehydrogenase \cite{antoniouIdentificationReactionCoordinate2011,quaytmanReactionCoordinateEnzymatic2007}. Multidimensional scaling (MDS) \cite{borg1997modern} is similar to PCA in that it seeks to represent high dimensional data with a smaller number of  combinations of input features. Rather than finding components to capture the variance, MDS preserves, as far as possible, distances between observations.  MDS has been used to characterise the conformational states and track simulation convergence of bovine pancreatic trypsin inhibitor \cite{troyerProteinConformationalLandscapes1995}. Similar machine learning methods, which find low dimensional representations of the dynamics while preserving various metrics, have also been used.  Isomap \cite{tenenbaumGlobalGeometricFramework2000} preserves the distances between conformations on a curved surface (manifold), the geometry of which is inferred from the observed conformations. A computationally efficient method of Isomap (SciMAP) \cite{dasLowdimensionalFreeenergyLandscapes2006} was used to determine the protein folding reaction coordinate for SH3 domain. Sketch-map \cite{ceriottiSimplifyingRepresentationComplex201} preserves only certain subsets of distances deemed to be important and has been used to understand the unfolding dynamics and the effect of point mutations of a beta hairpin polypeptide \cite{ardevolProbingUnfoldedConfigurations2015}. 
Diffusion map \cite{fergusonNonlinearDimensionalityReduction2011} and locally scaled diffusion map, preserve diffusion distances (i.e., how easily states can diffuse to one another). They have been used to characterise folding pathways in a number of small proteins:  Trp-cage \cite{kimSystematicCharacterizationProtein2015}, a beta-hairpin \cite{zhengDelineationFoldingPathways2011} and in Microcin J25 \cite{fergusonNonlinearDimensionalityReduction2011}. 


\section{Markov models and their applications}
Markov models provide a framework for classifying conformations into metastable states, finding reaction pathways and estimating kinetic and thermodynamic quantities. While they are able to incorporate knowledge of important order parameters or features they do not require such knowledge \cite{husicMarkovStateModels2018, pandeEverythingYouWanted2010}.

The central idea \cite{zwanzigClassicalDynamicsContinuous1983a} is that for complex systems, over a sufficiently long periods of time, transitions rates between regions of the systems total configurational space are independent on their path history. If the configurational space is partitioned into $n$ discrete states then the dynamics of the system can be described by an $n\times n$ transition matrix $\mathbf{T}$ \cite{prinzMarkovModelsMolecular2011}. Each element of $\mathbf{T}$ describes the conditional probability of the system jumping between states over a time period, $\tau$, the Markov lag time \cite{prinzMarkovModelsMolecular2011}. The eigenvectors and eigenvalues of $\mathbf{T}$ represent the associated slow dynamic processes and their associated timescales \cite{prinzMarkovModelsMolecular2011}. 

Applications of MMs are concentrated on biomolecular systems and form an intrinsic part of the bimolecular simulation tool-box \cite{hugginsBiomolecularSimulationsDynamics2019}. Applications include modelling both protein folding pathways \cite{singhalUsingPathSampling2004,swopeDescribingProteinFolding2004} as well as intrinsically discorded proteins \cite{schorAnalyticalMethodsStructural2016a}. 
MMs have been applied to enzyme systems and used to elucidate, for example, ligand docking pathways \cite{ahalawatMappingSubstrateRecognition2018a} and regioselectivity mechanisms in cytochrome p450 \cite{dodaniDiscoveryRegioselectivitySwitch2016a}, the conformational heterogeneity in the important cancer target SETD8 \cite{chenDynamicConformationalLandscape2019a}, loop dynamics in triosephosphate isomerase \cite{LoopMotionTriosephosphate}, and allosteric effects in cyclophilin A \cite{wapeesittipanAllostericEffectsCyclophilin2019}. Other applications include self-assembly \cite{senguptaAutomatedMarkovState2019} and dimer formation \cite{leahyCoarseMasterEquations2016} of amyloid peptides, identifying important conformations in drug targets to improve drug docking free energy calculations \cite{amaroEnsembleDockingDrug2018}, and rational drug design \cite{gervasioBiomolecularSimulationsStructureBased2019}. There have been comparatively little use of MM on smaller systems (whose kinetics tend to be derived from quantum mechanical and thermodynamic data \cite{glowackiMESMEROpenSourceMaster2012, pillingMasterEquationModels2003}, rather than statistically estimated from MD data), however, one recent example used MMs to determine hydrogen bond rearrangement in liquid water \cite{schulzCollectiveHydrogenbondRearrangement2018}. 

% MMs are a varied collection of techniques covering a range of different simulation conditions and models of dynamics and for recent reviews of the full range of techniques and applications see references \cite{husicMarkovStateModels2018, noeMarkovModelsMolecular2019b}. 
Early MM construction consisted of describing conformational dynamics of systems in thermal equilibrium by constructing only a handful of discrete states, and modelling the dynamics as Markov chain also known as a \emph{Markov state model} (MSM). For example the authors of \cite{degrootEssentialDynamicsReversible2001} clustered frames of a $\beta$-heptapeptide into four states to describe its folding dynamics. However, the more common approach \cite{husicMarkovStateModels2018, noeMarkovModelsMolecular2019b} is a two stage process:  first, molecular conformations are clustered into $n$, geometrically similar, discrete \emph{microstates} (where typically $n \lesssim 1000$). Second, coarse-graining these microstates into a handful, $m$, of \emph{macrostates} based on the kinetic properties of the microstates.  The macrostates are defined such that microstates have a low probability of transitioning between the macrostates, compared to inter-conversion within a macrostate \cite{schutteDirectApproachConformational1999,swopeDescribingProteinFolding2004, prinzMarkovModelsMolecular2011}.  These macrostates are then known as the metastable conformations or metastable states of the system.  

% In order to recover the metastable conformations from an MSM a number of \emph{coarse graining} schemes have been introduced. One of the first was Perron-cluster cluster analysis, PCCA \cite{deuflhardIdentificationAlmostInvariant2000a} and robust PCCA \cite{deuflhardRobustPerronCluster2005b}, which used the eigenvectors of $\mathbf{T}$ to determine the metastable states. Later, hidden Markov models (HMMs), an already well known statistical model \cite{rabinerTutorialHiddenMarkov1989}, were introduced  which  identified the metastable states of the system with the hidden states of a HMM \cite{noeMarkovModelsMolecular2019b}. 

% [Example of lumping with simple MSM]

With respect to the first stage, the key to creating accurate MSMs are finding the ``essential degrees of freedom'' of the system \cite{zwanzigClassicalDynamicsContinuous1983a, schutteDirectApproachConformational1999} i.e., a small number of collective variables or order parameters which are involved in the slow processes being studied. Clustering conformations into microstates using these variables, rather than the atomic coordinates, mitigates the problem of discretizing high-dimensional systems.  As already discussed there have been many statistical and machine learning techniques for finding collective variables for describing the slow conformational dynamics. However, many of these techniques do not directly address capturing the slow dynamics. For example, the problem with using PCA as a preprocessing step before clustering into microstates is that the principal components retain the greatest \emph{configurational} variance \cite{friedman2001elements}. A similar technique, time-lagged independent component analysis, TICA \cite{perez-hernandezIdentificationSlowMolecular2013a, schwantesImprovementsMarkovState2013}, was introduced which identifies linear combinations of the atomic positions which are maximally time correlated at a given lag time. Using TICA to reduce dimension by keeping the first $m$ TICA components retains the greatest \emph{kinetic} variance \cite{noeKineticDistanceKinetic2015}. The total kinetic variance describes the ability  the TICA components (or any basis set) to capture the slow dynamics of the system. Using TICA as a preprocessing step in MSM construction has been systematically investigated and shown to be more accurate than both PCA and no preprocessing at all \cite{husicOptimizedParameterSelection2016} in capturing the slow dynamics. However, it brings with it two new modelling choices (aside from the number of microstates, $n$): the TICA lag-time (also called $\tau$ but not necessarily the same as the Markov lag-time $\tau$) and the number of independent components to retain, $m$.  

% With large biomolecules, determining the essential degrees of freedom is difficult because of the systems high ambient dimension. This was apparent outside the context of MSMs as the authors of  reference \cite{karpen1993statistical} note in 1992: 
% ``It is becoming essential to develop methods which reduce the complexity of data resulting from these long simulations while retaining the relevant information." The method they proposed to investigate the mechanism of peptide turn formation was to use a statistical clustering algorithm (where the data determine the cluster definitions) to cluster the pentapeptide configurations in the space of the backbone ($\phi, \psi$) dihedral angles. Rather than data based clustering, the authors of reference \cite{mckelveyCHARMMAnalysisConformations1991} use predetermined regions of the ($\phi, \psi$) plane to describe the conformational landscape of the metastasis-inhibiting laminin peptide.  

The next step in the development of MMs was the realisation that estimating MSMs and TICA could be cast as variational optimisation problem (the variational approach to conformational dynamics, VAC \cite{nuskeVariationalApproachMolecular2014}). While VAC showed that TICA and MSMs were the optimal description of the slow dynamics for a \emph{given} linear continuous and discrete basis set (respectively), the authors of reference \cite{mcgibbonVariationalCrossvalidationSlow2015} showed that the same variational principle could optimize the basis sets themselves. The key innovation of this work was to combine cross-validation \cite{arlotSurveyCrossvalidationProcedures2009} and the variational principal to score a given basis set using the generalized matrix Rayleigh coefficient, GMRQ. The theory was then broadened with Koopman models to encompass simulations of systems out of thermal equilibrium \cite{wuVariationalKoopmanModels2017}. With the variational approach to Markov processes (VAMP) the theory of MSMs and Koopman models was unified into one conceptual framework. This increased the scope of MMs and presented a range of model scoring metrics (VAMP scores), of which the GMRQ was a special case. These theoretical advances have allowed the development the following iterative optimisation MM pipeline, starting with a set of MD trajectories: project coordinates on to important features, cluster into discrete states, estimate MSM and score using VAMP or GMRQ. This process can be repeated by varying the type of feature, number of discrete states, etc., until a satisfactory VAMP score has been achieved.   

Other approaches to building MSMs exist. VAMPnets \cite{mardtVAMPnetsDeepLearning2018}, still utilises the variational framework but instead of discretising MD trajectories, uses a deep neural networks to learn continuous, non-linear estimates of the eigenvectors of $\mathbf{T}$.  Enspara \cite{porterEnsparaModelingMolecular2019} is a package which facilitates clustering large volumes of MD data without the need to perform dimensionality reduction (e.g., PCA or TICA) first. For example, coordinate trajectories of a cefotaximase \cite{hartModellingProteinsHidden2016} were clustered into based on similar RMSD values of important atoms.  By not coarse graining these microstates into macrostates, or projecting the coordinates onto the slowest collective variables, this technique emphasizes are larger range of dynamic processes (not just the slowest ones). As a result, the authors were were able to reveal important conformations, not captured in X-ray and other structural data to explain the enzyme's specificity and antibiotic resistance.  


textbf{Chapter \ref{chap:theory}} sets out the theory of MMs relevant to this thesis which focuses on MSM estimation, TICA for preprocessing, variational optimisation of basis sets using VAMP scores. 

% The importance of selecting appropriate features for MSM analysis also quickly became important. In combination with the domain knowledge of chemists and biochemists, dimensionality reduction \cite{friedman2001elements} techniques became popular.  Principal component analysis (PCA) \cite{leverPrincipalComponentAnalysis2017} identifies linear combinations, or `principal components', of variables which explain the variance in a given set of data. The first principal component (PC1) is a vector which points in the direction of the greatest variance, PC2 is orthogonal to PC1 and lies in the direction of the second-most variance etc. Projecting the data onto the first $m$ principal components (where $m$ is typically much less than the number of original dimensions \cite{leverPrincipalComponentAnalysis2017}) which explain the majority of the variance, while ignoring the rest, is a way of reducing the dimension of the system. Using MD simulations of a $\beta$-heptapeptide, the authors of reference \cite{degrootEssentialDynamicsReversible2001} used the first three principal components as features and clustered the trajectories to understand the folding process in terms of its hydrogen bonding network. 
% Determining these metastable states, their kinetics, and structural characteristics from MD simulations are the main goals of a MM analysis and the remain a topic of continued interest for both method development and applications \cite{husicMarkovStateModels2018,noeMarkovModelsMolecular2019b, wangConstructingMarkovState2018c}.  



\section{Simplified MSM construction}
The MSM literature has concentrated on large biomolecules because of the requirement of a free energy surface which is ``sufficiently complex'' that memoryless transitions assumption holds \cite{zwanzigClassicalDynamicsContinuous1983a}. As already mentioned, the success of the MM process is the creation of a good set of microstates to represent the slow dynamics of the system using the essential degrees of freedom as an initial set of variables over which to cluster. However, for biomolecules the process is complicated by the large number of potentially relevant degrees of freedom which cannot be determined a-priori \cite{shallowayMacrostatesClassicalStochastic1996}. However, for systems with a much smaller number of relevant degrees of freedom, chemical intuition and  visualisation techniques can be used to guide the choice of collective variable.
\textbf{Chapter \ref{chap:water}} describes the diffusion of a single water molecule through a sucrose matrix, as may be found in, for example, organic aerosol particles \cite{songTransientCavityDynamics2020a}. This system is complex enough such that the Markovian assumptions holds but has only a small number of relevant degrees of freedom - the water molecule and its immediate molecular environment.  This allowed a simplified MSM construction procedure,  rather than the iterative optimisation procedure described in the previous section, to construct valid and informative MSMs. 
In this system, the complex free energy surface is created by the sucrose matrix and essential degrees of freedom are the coordinates of the center-of-mass motion of the water molecule. These systems are important to understand from a basic science perspective because the observed diffusion rates are larger than those predicted from the Stokes-Einstein diffusion \cite{songTransientCavityDynamics2020a}. In addition, these type of systems have applications from understanding the properties of pharmaceutical delivery systems to airborne pollution. 

\section{Evaluating MSM performance}\label{sec:intro_msm_perf}
The MM analysis pipeline described so far, consists of first transforming MD trajectories into features (the essential degrees of freedom, $\chi$), then reducing the dimension with TICA,  discretizing the TICA components into $n$ microstates, and finally estimating the MSM. The modelling choices or \emph{hyperparameters}, ($\chi, \tau, m, n$), create the MSM basis set, which in turn determine the accuracy of the resulting MSM, and so a method of evaluating the performance of these hyperparameters  is needed. While the ground truth of the kinetic processes is not available, the initial way forward came through cross-validation and the GMRQ \cite{mcgibbonVariationalCrossvalidationSlow2015}. 

The innovation in reference \cite{mcgibbonVariationalCrossvalidationSlow2015} was to create a model score, the GMRQ (the Rayleigh trace from quantum mechanics), which could be used to judge the quality of the model choices while accounting for the tendency of models to fit to noisy signals in the data. This was achieved through cross-validation \cite{arlotSurveyCrossvalidationProcedures2009}: a model is estimated using a portion of the data and scored on the remaining data. Maximizing the cross-validated GMRQ by varying the hyperparameters increases the accuracy of the eigenvectors \cite{mcgibbonVariationalCrossvalidationSlow2015}. The GMRQ is a special case of the first VAMP score, VAMP-1, while maximizing the total kinetic variance is the same as maximizing the VAMP-2 score. These VAMP metrics completed the analysis pipeline \cite{schererVariationalSelectionFeatures2019} which now can be summarised as: i) transform MD trajectories into features, $\chi$, ii) select reasonable choices of  hyperparameters (features, TICA parameters, number of discrete states) and calculate the cross-validated VAMP-2 score, iii) change the hyperparameters and repeat analysis, iv) stop when the VAMP-2 score stops increasing. 

\section{Hyperparameter optimisation}\label{sec:intro_hyper_opt}
Choosing the hyperparameters which maximize the VAMP-2 score is a `black-box' optimisation problem \cite{jonesEfficientGlobalOptimization1998a}, so called because no gradient information on the response of the VAMP to the hyperparameters is available. This is a common problem in the machine learning community where models have many parameters and may take days to train \cite{feurer2019hyperparameter}. In this case it is not feasible to exhaustively search through combinations of hyperparameters. A popular method for optimising large sets of hyperparameters is Bayesian optimisation (also known as sequential model based optimisation, SMBO) \cite{hutterSequentialModelbasedOptimization2011,NIPS2012_4522,bergstraAlgorithmsHyperParameterOptimizationa,bergstraMakingScienceModel2013}. The idea behind Bayesian optimisation is that there is an objective function which is costly to optimise \cite{brochuTutorialBayesianOptimization2010,shahriariTakingHumanOut2016} (in this case the VAMP-2 score). So instead of optimising this directly, the BO procedure builds an statistical model of  objective function known as a \emph{surrogate function} or \emph{response surface}, using randomly sampled  values of the objective function. Having built an initial response surface, searching for the next hyperparameter to evaluate is guided by an \emph{acquisition function}. These can be selected or adjusted to trade off high-uncertainty regions (the `explore' regime) of the response surface with the high-value, low-uncertainty  regions (the `exploit' regime) \cite{shahriariTakingHumanOut2016}. A suggestion is evaluated, the response surface updated and the process repeats. Bayesian optimisation for hyperparameter optimisation is popular, as the number of packages designed for this purpose will attest (this list is non-exhaustive): Hyperopt \cite{bergstraHyperoptPythonLibrary2013}; sequential model-based algorithm configuration \cite{hutterSequentialModelbasedOptimization2011}, SMAC; BayesOpt \cite{martinez-cantinBayesOptBayesianOptimization2014}; Spearmint \cite{DBLP:conf/uai/GelbartSA14,snoekAbstractBayesianOptimization2013,snoekInputWarpingBayesian2014a,NIPS2013_5086,NIPS2012_4522}, GPyOpt \cite{gpyopt2016}, DragonFly \cite{JMLR:v21:18-223}; Auptimiser \cite{liuAuptimizerExtensibleOpenSource2019}; and Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016}. A popular choice of response surface model is a Gaussian process (GP) \cite{rasmussenGaussianProcessesMachine2006}, a highly flexible type of model which fits naturally within the Bayesian optimisation paradigm \cite{shahriariTakingHumanOut2016}. Indeed, six of the eight packages listed here all implement some kind of Gaussian process  as their response surface model.

\textbf{Chapter \ref{chap:msm}} demonstrates the use Bayesian optimisation to optimise the MSM hyperparameters using cross-validated VAMP-2 scores of the model system alanine dipeptide. In addition, the parameters of GPs are explored as a way to describe the relevance of hyperparameters in determining the VAMP-2 score. This chapter lays the ground-work for performing a similar analysis on AADH in chapter \ref{chap:aadh}, in particular: how to fit and interpret GPs and how to use GPs with Bayesian optimisation to optimise hyperparameters. 

\section{Metastable states}

Having arrived at $n$ microstates via an optimal set of MSM hyperparameters by maximising the kinetic variance (VAMP-2), the second  stage in the MM pipeline is to coarse grain potentially thousands of microstates into a handful of macrostates to create a more interpretable model. 
\begin{highlighted}

However, coarse graining an existing MSM is not the only approach to gaining insight into the conformational landscape of biomolecules.  There are other statistical clustering techniques that have been used for this purpose. The authors of \cite{troyerProteinConformationalLandscapes1995} used hierarchical clustering \cite[chapter 10 of]{friedman2001elements} to group MD frames into groups with mutual root mean square deviation (RMSD) in their alpha-carbon positions below some small threshold value. Hierarchical clustering shows how conformations cluster together as the threshold RMSD is increased.  In this way the conformational landscape at different levels of spatial resolution can be determined and the number of clusters determined by other criteria (the authors considered a RMSD separation which give rise to different minimized conformations).  In \cite{karpen1993statistical} the authors used a neural network clustering algorithm, ART-2$^{\prime}$ \cite{carpenterARTSelforganizationStable1987} to investigate the folding mechanism of a pentapeptide. Folding events were described by up to six different clusters where the clustering took place in the space of residue dihedral angles. The number of clusters was determined by considering the extent of the clusters in the dihedral space, compared to those expected from thermal fluctuations, as opposed to those from conformational change. 

The main drawback of clustering based on geometric measures of similarity are that metastable macrostates are actually defined by their kinetic properties (i.e., conformations in a metastable macrostate undergo rapid, mutual, inter-conversion over a given timescale) which are not the same \cite{schutteDirectApproachConformational1999}. Kinetic clustering dates back to at least 1969 when Kuo and Wei \cite{weiLumpingAnalysisMonomolecular1969, kuoLumpingAnalysisMonomolecular} investigated the conditions under which both exact and approximate coarse graining of systems of coupled first order reactions could occur. The term \emph{exact} implying that the coarse-grained description gave rise to kinetic description consistent with the underlying microscopic kinetics.  Hummer and Szabo \cite{hummerOptimalDimensionalityReduction2015a} tackled the problem of how to define an appropriate coarse-grained rate matrix (where the entries relate kinetic rates of transitioning from one microstate to another, rather than probabilities in the transition matrix picture) for a given coarse-graining scheme. i.e., given a mapping of micro- to macrostates, what is the most appropriate way of defining the rate matrix.  They derived expressions for rate matrices which are exact for non-Markovian dynamics (i.e., for systems where transition probabilities are dependent on the history of states visited) for a given coarse graining scheme.  They also derived expressions for the case of Markovian dynamics which, while approximate, still ensured that the macrostate relaxation timescales were exact. In addition, they proposed an algorithm for optimally determining this coarse-graining by iteratively partitioning microstates, such that the timescales implied in their rate matrices are maximized at each step. The authors of \cite{kellsCorrelationFunctionsMean2020} went on to show that this scheme was equivalent to constructing coarse-grained rate matrices which preserve the mean-first passage times of the system. This work has been developed to identify not only metastable macrostates but also the comparatively short lived transition states \cite{martiniVariationalIdentificationMarkovian2017}. 

Other methods, solely based on identifying metastable macrostates have been developed. Perron Cluster Cluster Analysis (PCCA) \cite{deuflhardIdentificationAlmostInvariant2000a} and its subsequent `robust' alternative PCCA+ \cite{deuflhardRobustPerronCluster2005b} use the sign structure of a given number of slow eigenvectors of $\mathbf{T}$ to determine the boundaries between macrostates: two microstates are in the same macrostate if they have the same sign for a given eigenvector.  The hierarchical Nystr{\"o}m exstension graph method is a hierarchical clustering method based on the Nystr{\"o}m approximation for approximating $\mathbf{T}$ \cite{yaoHierarchicalNystromMethods2013a} with a submatrix of well sampled states. This submatrix retains the same sign structure of the full matrix and uses this sign structure with PCCA to cluster microstates. The Bayesian aglomerative cluster engine (BACE) \cite{bowmanImprovedCoarsegrainingMarkov2012a} uses Bayesian statistics to assess whether two microstates have the same transition rates to each putative macrostate - if they do then they are lumped into the same state. Other methods include the most probable path \cite{jainIdentifyingMetastableStates2012a} which assigns microstates to the same macrostate if they occur on the sequence of states with the highest transition probabilities; methods based on the renormalisation group \cite{orioliDimensionalReductionMarkov2016c};  minimum variance cluster analysis, MVCA \cite{husicMinimumVarianceClustering2018} which uses the hierachical clustering to merge rows of the transition matrix; and projected Markov models, PMMs \cite{noeProjectedHiddenMarkov2013a} which include observer operator models \cite{wuProjectedMetastableMarkov2015} and hidden Markov models \cite{noeProjectedHiddenMarkov2013a} (HMM) which are dynamical models in which the microstate/macrostate coarse-grained structure is directly incorporated into the model structure. 

When coarse-graining MSMs or performing any type of cluster analysis a key parameter is the number of clusters $m$, e.g., does the data support the hypothesis of $m=2$ or $m=3$ (say) clusters \cite{milliganExaminationProceduresDetermining1985}. As kinetic clustering is always in relation to some timescale (in this case, the Markov lag time), the number of macrostates is ill defined when this timescale is unspecified. However, even given this timescale, the coarse graining and clustering methods so far described do not automatically select the number of macrostates - although some methods provide more information than others to help guide selection. The variationally optimized coarse-graining of reference \cite{martiniVariationalIdentificationMarkovian2017} allows the user to test whether a transition macrostate has been found but leaves the decision to continue finding more macrostates up to the user. Hierarchical approaches such as BACE,  HNEG and MVAC, automatically show how the microstates group together for different values of the clustering criteria, allowing the user to judge the most useful clustering.  A more general method for MSMs using Bayesian statistics has been developed, which takes as its data the mapping between the micro- and macrostates \cite{bacalladoBayesianComparisonMarkov2009a} and so is independent of clustering method. To decide on an appropriate number of macrostates, the Bayes factor (the Bayesian weight of evidence for a particular hypothesis) \cite{kassBayesFactors1995} for different numbers of macrostates is calculated and used to select $m$. 


Projected Markov models are distinct to the other techniques described in that they are estimated by maximizing a likelihood \cite{wuProjectedMetastableMarkov2015, noeProjectedHiddenMarkov2013a} (i.e, the probability of observing model parameters given a set of data).  This opens up a much wider range of techniques which are not explicitly related to Markov processes because the Markov property is subsumed into the likelihood function \cite[chapter 7]{friedman2001elements}.  

Even more methods exist for assessing general clustering methods i.e., which don't take into account the Markov property of the data \cite{milliganExaminationProceduresDetermining1985, mclachlanFiniteMixtureModels2000} when the model is based on a likelihood (the probability of model parameters given a data set \cite[chapter 3]{8208528151144ef1805212918755aa1b}) as is the case for HMMs.  

Among the most popular of these are  the Akaike information criterion (AIC) \cite{akaikeInformationTheoryExtension1998},  Bayesian information criterion (BIC) \cite{schwarzEstimatingDimensionModel1978a} and cross-validation \cite{arlotSurveyCrossvalidationProcedures2009} (CV). These are general purpose methods for selecting model hyperparameters, not just for clustering algorithms for but for regression and other machine learning techniques \cite[chapter 6]{friedman2001elements}.  In their own ways, these techniques aim to estimate either the predictive performance of a model (in the case of AIC and CV) or Bayes factors (in the case of BIC) using 

\end{highlighted}

% Chapter \ref{chap:msm} demonstrated using response surface methods and Bayesian optimisation to arrive at an optimal MSM. In order to make sense of the potentially thousands of MSM microstates, it is common practice to kinetically lump them into a smaller number of macrostates. There have been a large number of different methods proposed and coarse-graining has been studied since at least 1969 \cite{kuoLumpingAnalysisMonomolecular, weiLumpingAnalysisMonomolecular1969}. The first among the most recent applications of coarse-graining MSMs was Perron cluster cluster analysis, PCCA \cite{deuflhardIdentificationAlmostInvariant2000a}) and its successor, robust PCCA or PCCA+ \cite{deuflhardRobustPerronCluster2005b}. Other methods include the hierarchical Nystr{\"o}m exstension graph, HNEG \cite{yaoHierarchicalNystromMethods2013a}, the Bayesian agglomerative clustering engine, BACE \cite{bowmanImprovedCoarsegrainingMarkov2012a}, methods based on the renormalisation group \cite{orioliDimensionalReductionMarkov2016c, hummerOptimalDimensionalityReduction2015a}, the most probable path algorithm \cite{jainIdentifyingMetastableStates2012a}, a method based on variationally optimising the coarse grained transition matrix \cite{martiniVariationalIdentificationMarkovian2017a}, minimum variance cluster analysis, MVCA \cite{husicMinimumVarianceClustering2018}; and projected Markov models, PMMs \cite{noeProjectedHiddenMarkov2013a}. PMMs are Markov processes projected onto a coarse grained set of states and are exactly described by observable operator models, OOMs \cite{wuProjectedMetastableMarkov2015}, and approximated by hidden Markov models, HMMs \cite{noeProjectedHiddenMarkov2013a} under certain conditions (HMMs have also been proposed to describe protein dynamics, from other, non-coarse-graining perspectives \cite{mcgibbonUnderstandingProteinDynamics}). OOMs are a generalisation of HMMs \cite{jaegerDiscretetimeDiscretevaluedObservable} and despite their theoretical advantages it is HMMs that have been more widely used to coarse grain molecular dynamics simulations \cite{mondalAtomicResolutionMechanism2018a, plattnerCompleteProteinProtein2017, panConformationalHeterogeneityMichaelis2016, juarez-jimenezDynamicDesignManipulation2020, wangDynamicalBehaviorVLactamases2019,FastFoldingPathwaysThrombinBinding2018,remingtonFluorescenceQuenching2aminopurinelabeled2019,curado-carballadaHiddenConformationsAspergillus2019,furiniIontriggeredSelectivityBacterial2018,yangMappingPathwayDynamics2018,ahalawatMappingSubstrateRecognition2018,olaposiMembraneBoundTranscriptionFactor2019, xiaoNaBindingModes2019, hansonWhatMakesKinase2019}. 


This is often done by fitting a hidden Markov model (HMM) to the same microstates used to estimate the MSM \cite{noeMarkovModelsMolecular2019b}, although many other techniques exist (e.g., observer operator models \cite{wuProjectedMetastableMarkov2015}, see chapter \ref{chap:hmm} for a full discussion). A HMM is a model of a Markovian process between $g$ \emph{hidden} states i.e., states which are not directly observed in the data. While in a hidden state the system emits randomly, according to a probability distribution, to one of a set of \emph{observed} states, which are seen in the data. The transition rates and emission probabilities are estimated from the data.  The only hyperparameter stipulated by the user (at least for the discrete HMMs considered here) is the number of hidden states, $g$. To model the metastable dynamics, the number of hidden states of the HMM is set equal to the number of metastable states of the system, $r$ \cite{noeProjectedHiddenMarkov2013a}. The number of metastable states is determined by looking for gaps in the eigenvalue spectrum of the MSM \cite{prinzMarkovModelsMolecular2011,noeProjectedHiddenMarkov2013a}. However, due to poor discretisation or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}. This observation, along with the plethora of other coarse-graining schemes motivated the introduction of Bayes factors \cite{kassBayesFactors1995} for both judging the quality of the coarse graining scheme and the number of metastable states \cite{bacalladoBayesianComparisonMarkov2009a}. The Bayes factor is the ratio of the integrated likelihood of the data given the model parameters \cite{kassBayesFactors1995}, for two competing models. Bayes factors measure the relative evidence of a model (its parameters and hyperparameters) given a trajectory of \emph{observed states} (the microstates). For example, if the Bayes factor between a HMM with $g=2$ and  $3$ hidden states is $100$ then the $g=2$ model is more strongly favoured. 

The integrated likelihood only takes into account how the model explains the observed states \cite{biernackiAssessingMixtureModel2000a,mclachlanFiniteMixtureModels2000}. It does not factor in directly the hidden state structure of the model. Rather than view the HMM as a coarse graining technique, another approach is to view an HMM and other hidden mixture models as a clustering algorithm \cite{mclachlanFiniteMixtureModels2000} where observed states are classified as belonging to hidden states. An important analogue of the integrated likelihood in mixture model community is the integrated classification likelihood \cite{mclachlanFiniteMixtureModels2000}. This measures the evidence for  both the observed states \emph{and the hidden states} given the model parameters and hyperparameters. 

\textbf{Chapter \ref{chap:hmm}} explores the utility of approximations to the Bayes factor and similar criteria for determining the optimal value of $g$: the Bayesian information criterion, BIC \cite{schwarzEstimatingDimensionModel1978a}, the integrated complete data likelihood criterion, ICL \cite{biernackiAssessingMixtureModel2000a}, the Akaikie information crteria, AIC \cite{akaikeInformationTheoryExtension1998}, and cross-validated log-likelihood, CVLL \cite{celeuxSelectingHiddenMarkov2008}. This extends previous work \cite{mcgibbonStatisticalModelSelection2014a} which looked at the AIC and BIC for selecting parameters of MSMs. Like chapter \ref{chap:msm} this lays the ground-work for application to the case of AADH in chapter \ref{chap:aadh}. The example used is a four well potential, the Prinz potential \cite{prinzMarkovModelsMolecular2011}, with a well defined number of metastable states against which the BIC, ICL, AIC and CVLL can be judged. The benefits of the these information criteria (except CVLL) are that they do not required extra calculation after a HMM has been estimated and so lend themselves to situations in which a large number of models need to be tested, a situation that will be encountered in this thesis. 

\section{Aromatic amine dehydrogenase}
Aromatic amine dehydrogenase (AADH) is an enzyme involved in the on-going debate over the role of dynamics in tunneling and catalysis \cite{glowackiTakingOckhamRazor2012b,glowackiProteinDynamicsEnzyme2012a,mcgeaghProteinDynamicsEnzyme2011}. As the name suggests, AADH oxidizes aromatic amines such as tryptamine with a proton transfer from a Schiff base intermediate to an acceptor aspartate residue as its rate limiting step \cite{masgrauAtomicDescriptionEnzyme2006}.  Substituting deuterium in the substrate causes the rate to drop by up to a factor of $55$ (the kinetic isotope effect, KIE), well below the decrease expected from the difference in zero-point energy between C---H and C---D, implicating significant quantum mechanical tunneling in the mechanism \cite{masgrauAtomicDescriptionEnzyme2006, klinmanbeyond2009}. It has been suggested that the temperature dependence of the KIEs for different enzymes, including AADH, cannot be explained by transition state theory (TST) \cite{agrawalVibrationallyEnhancedHydrogen2004,kohenEnzymeCatalysisClassical1998}. However, the authors of reference \cite{glowackiProteinDynamicsEnzyme2012a} showed that by using TST to model the reaction as two rapidly interconverting metastable states which can both react to form the product, the temperature dependence of the KIE could be explained. To test this hypothesis, the conformational landscape of AADH has to be determined. 

\textbf{Chapter \ref{chap:aadh}} brings together the hyperparameter optimisation work of chapter  \ref{chap:msm} and the hidden state selection work of chapter \ref{chap:hmm} and applies it to the case of AADH. An expanded set of hyperparameters is optimised and understood in terms of the response surface and in addition to the optimized parameters, a set of sensitivity models are proposed. The model selection criteria of chapter \ref{chap:hmm} are used to select the appropriate number of hidden states in the coarse grained HMMs and the results of the sensitivity models compared and discussed. 

\textbf{Chapter \ref{chap:conclusions}}, discusses the conclusions of this thesis and sets out concrete steps for further work in this area. 


% using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 


% Gaussian Processes (GPs) have been


% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 


% with a concomitant increase in the number of software packages 


%  - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% % Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}




% \textbf{From art to science:} 
% MSMs are master equations (ME review [6])
% Model is n x n matrix, spans the conformational space, conditional probabilities as elements. 
% State populations are give thermodynamics, off-diagonal elements give kinetics. 
% If we assume thermodynamic equilibrium then eigendecomposition is useful [description of eigenvectors]
% the n states should capture the dyanmics. 

% Memoryless transition networks come from Zwanzig [1] then Van Kampen [2] key MSM papers [7-9]. 

% creating meaningful states is difficult [33,34] Karpen did dihedrals [10] de Groot [20] did PCA and k-medioids.

% can stitch together different trajectories: McCammon [11] now Folding@home [35], Luty [12] suggested stitching together different trajs for ligand binding. 

% Hardware devs: FoldingAtHome, BlueGened [39-41], GPUGRID [43,44]

% ITS plots [9] CKtest [46]

% Different features different dynamics [45]

% Errors are (1) state decomposition and (2) finite sampling [47]

% Global descriptors worse than internal degrees of freedom [49]

% Sarich [73, 92]: discretization error decreases and partitioning become finer and the lag time increases. 

% Djurdjevac [93]: upper bounds for error between MSM and trajectories decreases with lag time. 

% TICA\c variance explained [115]:
% This MSM score was termed the GMRQ, which stands for
% generalized matrix Rayleigh quotient, the form of the approx-imator (also referred to as the Rayleigh trace).124
% The GMRQ on the validation set will be poor if the model was overfit on the
% training set but better if the model identifies the underlying
% dynamics common to both sets. In 2016, No and Clementi115 demonstrated that kinetic variance in a data set can be explained area.
% by summing the squared tICA eigenvalues. Since the variational principle derived in No and Nske95 holds for any strictly nonincreasing weights applied to the scored eigenvalues,96 the kinetic variance can also be used to score models, or to deter- mine how many tICs are needed to explain a given amount of kinetic variance in the data.

% \textbf{Simple MSM}
% Analysis of three water molecules \cite{schulzCollectiveHydrogenbondRearrangement2018} to understand the collective hydrogen bond rearrangement, uses both Euler angles and spherical coordinates for dofs. 








% \textbf{Coarse graining and HMMs}


% \textbf{Hyperparameter search}
% Feature selection: \cite{schererVariationalSelectionFeatures2019}


% \cite{bergstraHyperoptPythonLibrary2013} Hyperopt - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}

% Optunity: \cite{claesenEasyHyperparameterSearch2014}

% \cite{pmlr-v32-hutter14} use random forest and ANOVA to assess parameter importance. 
% \cite{gramacyVariableSelectionSensitivity2013} using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 

% \cite{falknerBOHBRobustEfficient2018a} goes beyond BO and random bandit. 

% \cite{di2018genetic} genetic algorithm for hyperparameter search. 

% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 




















