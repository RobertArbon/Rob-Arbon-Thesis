%
% File: chap01.tex
% Author: Victor F. Brena-Medina
% Description: Introduction chapter where the biology goes.
%
\let\textcircled=\pgftextcircled
\chapter{Introduction}
\label{chap:intro}



This thesis describes the use of statistical model selection and optimisation techniques applied to Markov models (MM) for describing the slow conformational dynamics of two biomolecular systems: water diffusing through a sucrose matrix and the conformational landscape of aromatic amine dehdyrogenase (AADH).

\section{The importance of conformational changes in biochemical systems}

Quantitatively  describing the conformational changes in biomolecular systems is of central importance for understanding their function, chemical and biological properties. Conformational changes are at the heart of enzyme catalysis \cite{hammes-schifferRelatingProteinMotion2006, karplusMolecularDynamicsProtein2005, hammesMultipleConformationalChanges2002, roca2008relationship}. For example, triosephosphate isomerase (TIM) catalyses the isomerisation dihydroxyacetone phosphate and (R)-glyceraldehyde-3-phosphate \cite{LoopMotionTriosephosphate} and is considered a paradigmatic system for enzyme catalysis \cite{richardParadigmEnzymeCatalyzedProton2012a}, having been studied by molecular simulation since at least 1987 \cite{brownMolecularDynamicsSimulations1987, josephAnatomyConformationalChange1990}.  The catalytic process requires the closure of a loop after ligand binding through approximately \SI{7}{\angstrom}, which creates the necessary conditions to allow the isomerisation to occur,  opening again to allow product release \cite{LoopMotionTriosephosphate}. More extensive conformational changes are present in other systems.  Dihydrofolate reductase (DHFR) catalyses the reduction of 7,8-dihydrofolate to 5,6,7,8-tetrahydrofolate  \cite{schnellStructureDynamicsCatalytic2004a} with at least five kinetically distinct intermediates detected across the catalytic cycle  \cite{fierkeConstructionEvaluationKinetic1987}. The Met20 loop adopts at least three distinct conformations across the catalytic cycle \cite{sawayaLoopSubdomainMovements1997} with mutation experiments demonstrating its importance in rate determining step \cite{liFunctionalRoleMobile1992}. The importance of  conformational change in the relative positions of its two sub-domains has also been implied in transition state stabilisation and product release \cite{sawayaLoopSubdomainMovements1997}. Conformational changes have also been  invoked to  understand non-linear Arrhenius plots (which relate the rate of reaction to its activate barrier)  \cite{truhlarConvexArrheniusPlots2001,danielNewUnderstandingHow2010}. The cold adapted psychrophilic $\alpha$-amylase enzyme has an optimum rate for catalysis well below its melting temperature \cite{aqvistHiddenConformationalStates2020}. Macromolecular rate theory \cite{arcusTemperatureDynamicsEnzymeCatalyzed2020} posits changing heat capacities as an explanation, but simulation studies found an equilibria between a reactive and unreactive enzyme-substrate complex a more fitting explanation \cite{aqvistHiddenConformationalStates2020}.  There is evidence that population shifts in the conformational landscape is the mechanism by which  enzymes evolve to catalyse different substrates and reactions \cite{maria-solanoRoleConformationalDynamics2018, campbellRoleProteinDynamics2016,romero-riveraRoleConformationalDynamics2017}. Other biochemical examples include G protein-coupled receptors, a large family of transmembrane proteins involved in extracellular sensing and signalling which are responsible for olfaction, vision and taste \cite{rosenbaumStructureFunctionGproteincoupled2009}.  They transmit information from outside of the cell by way of ligand binding which inducing a series of conformational changes which in turn triggers the response within the cell \cite{bockenhauerConformationalDynamicsSingle2011a}. Large conformational changes are also implicated in the mechanism by which proteins associate with specific parts of DNA, thus enabling a whole host of cellular functions from gene regulation to DNA replication \cite{vandervaartCoupledBindingBending2015}.  

\section{Learning conformational dynamics from molecular simulations}

Computational approaches to studying conformational dynamics are important because they allow descriptions with high temporal and spatial resolution beyond the scope of most experimental techniques \cite{hugginsBiomolecularSimulationsDynamics2019}. A popular approach is to evolve the systems equations of motion using molecular dynamics (MD) to produce a set of trajectories through phase space. These trajectories can be used to reveal conformational transition pathways, metastable states and other properties of interest by estimating the relevant regions of the free energy landscape (the free energy with respect to a given set of coordinates)  \cite{rohrdanzDiscoveringMountainPasses2013a}.  

There are a wide range of techniques for understanding conformational dynamics from MD simulations, the suitability of which depend on current knowledge of the system and the questions being asked. Path based techniques such as transition path sampling (TPS) \cite{bolhuisTRANSITIONPATHSAMPLING2002, dellagoTransitionPathSampling1998, dellagoTransitionPathSampling2002a}, transition interface sampling (TIS) \cite{vanerpNovelPathSampling2003} and forward flux sampling (FFS) 
\cite{allenSamplingRareSwitching2005} all start with two specified metastable conformations, A \& B, and can be used to estimate rate constants and reaction coordinates of the reaction A $\rightleftharpoons$ B without previous knowledge of reaction pathways. TPS works by first proposing an reaction pathway between A and B. Then, a statistical ensemble of pathways is generated from this initial path using a stochastic algorithm which takes into account the potential energy of the system (Metropolis Monte Carlo). TIS and FFS are similar to TPS but define interfaces which separate A and B based on an order parameter\footnote{a quantity such as the root mean square deviation, which, while not a true reaction coordinate, varies between the two states.}. Molecular dynamics simulations are then used to estimate the flux between the interfaces and hence the transition rate between A \& B (FFS relaxes the assumption of equilibrium dynamics required in TIS and TPS). These techniques have been applied \cite{juraszekSamplingMultipleFolding2006, juraszekRateConstantReaction2008,velez-vegaKineticsMechanismUnfolding2010} to the model protein folding system Trp-cage \cite{neidighDesigning20residueProtein2002} to elucidate its folding pathway and have replicated some of the experimental microscopic rates in the folding pathway. String methods are similar but posit a discrete set of states along a path (or string) of fixed length and moves these states such that the string corresponds to minimum free energy path between A and B \cite{weinane.TransitionPathTheoryPathFinding2010, jnssonNudgedElasticBand1998}. String methods have been used to determine, for example, the mechanism and binding free energy of platinum based drugs to DNA \cite{elderSequenceSpecificRecognitionCancer2012}. 

Directional milestoning \cite{faradjianComputingTimeScales2004, majekMilestoningReactionCoordinate2010,kirmizialtinRevisitingComputingReaction2011a} is a technique which doesn't require knowledge of specific metastable states, only a  collective variable known to be related to the important conformational changes.  First a set of states which cover the relevant conformational space, known as anchors, are selected. The milestones are sets of conformations which separate (as measured by the collective variable) the anchors from one another, and are used to restart trajectories, calculate reaction coordinates and kinetics rate constants. Milestoning has been used to understand the selectivity of DNA reverse transcriptase \cite{kirmizialtinHowConformationalDynamics2012} and the  mechanism and rate of tryptophan permeation through cell membranes \cite{cardenasUnassistedTransportAcetyl2012}.  

When no previous information such as known metastable states or appropriate collective variables are known, more abstract statistical and machine learning methods have been increasingly shown to be important. Principal component analysis (PCA) finds the linear combinations of features of a molecule, such as $\alpha$-carbon coordinates, which explain the variance observed in a trajectory (the principal components are mutually orthogonal and explain decreasing amounts of the variance)\cite{pearson1901liii}  . Molecular motion can then be approximately described by a handful of principal components, rather than the full set of atomic coordinates. PCA of the heavy atom coordinates of the ribonuclease barnase \cite{noldeEssentialDomainMotions2002} was used to identify the highly flexible (high variance) regions of the enzyme which they related to the enzyme's  activity and stability. The authors of reference \cite{amadeiEssentialDynamicsProteins1993a} used PCA in the  protein lysozyme to identify highly flexible  regions which are related to the opening and closing of the active site (the ``essential'' degrees of freedom in their language).  One of the draw backs of PCA is that only linear combinations of features make up the principal components.  Kernel PCA, which incorporates non-linear transformations of input features, has also been developed and used with TPS simulations to extract a reaction coordinate for the reaction of lactate  dehydrogenase \cite{antoniouIdentificationReactionCoordinate2011,quaytmanReactionCoordinateEnzymatic2007}. Multidimensional scaling (MDS) \cite{borg1997modern} is similar to PCA in that it seeks to represent high dimensional data with a smaller number of  combinations of input features. Rather than finding components to capture the variance, MDS preserves distances between observations.  MDS has been used to characterise the conformational states and track simulation convergence of bovine pancreatic trypsin inhibitor \cite{troyerProteinConformationalLandscapes1995}. Similar machine learning methods, which find low dimensional representations of the dynamics while preserving various metrics, have also been used.  Isomap \cite{tenenbaumGlobalGeometricFramework2000} preserves the distances between conformations on a curved surface (manifold), the geometry of which is inferred from the observed conformations. A computationally efficient method of Isomap (SciMAP)  was used to determine the protein folding reaction coordinate for SH3 domain \cite{dasLowdimensionalFreeenergyLandscapes2006}. Sketch-map \cite{ceriottiSimplifyingRepresentationComplex2011} preserves only certain subsets of distances deemed to be important and has been used to understand the unfolding dynamics and the effect of point mutations of a beta hairpin polypeptide \cite{ardevolProbingUnfoldedConfigurations2015}. 
Diffusion map \cite{fergusonNonlinearDimensionalityReduction2011} and locally scaled diffusion map, preserve diffusion distances (i.e., how easily states can diffuse to one another). They have been used to characterise folding pathways in a number of small proteins:  Trp-cage \cite{kimSystematicCharacterizationProtein2015}, a beta-hairpin \cite{zhengDelineationFoldingPathways2011} and in Microcin J25 \cite{fergusonNonlinearDimensionalityReduction2011}. 

\section{Markov models and their applications}\label{sec:mm_applications}
An alternative to the techniques described above are Markov models.  Markov models provide a framework for classifying conformations into metastable states, finding reaction pathways and estimating kinetic and thermodynamic quantities. While they are able to incorporate knowledge of important order parameters or features they do not require such knowledge \cite{husicMarkovStateModels2018, pandeEverythingYouWanted2010}.

The central idea \cite{zwanzigClassicalDynamicsContinuous1983a} is that for complex systems, over a sufficiently long periods of time, the rate at which the system transitions out of region $A$ and into region $B$ of configurational phase space is not dependent on how the system arrived at $A$. In other words, these transitions are ``memoryless''. In mathematical notation this is \cite{noeTransitionNetworksModeling2008}: 
\begin{equation}\label{eqn:masterequation}
    \frac{\mathrm{d}\mathbf{p}(t)}{\mathrm{d}t} = \mathbf{p}(t)\mathbf{K},
\end{equation}

where the $i$'th element of $\mathbf{p}$ represents the probability of the system being in a region of phase space labelled by $i$; $K_{ij}$ is the rate of transitioning from region $i$ to $j$, and $K_{ii} = -\sum_{j\ne i} K_{ij}$. In addition to being ``memoryless'', equation \ref{eqn:masterequation} also implies that the rate matrix $\mathbf{K}$ does not change with time, i.e., the system is ``stationary'' \cite{zwanzigClassicalDynamicsContinuous1983a}.  The solutions to this equation describe how the probability of the system being in discrete regions of phase space changes smoothly over time. The justifying assumption is that biomolecular systems have a free energy surface (the free energy with respect to some set of coordinates) which is is characterised by many local minima, arising due to the many degrees of freedom afforded by its large structure (e.g., rotations about bonds or dihedral angles). The system resides in these minima for a sufficiently long period of time that transitions between them become independent of one another.  While this is not the case for very short timescales, over a sufficiently long time $\tau$, the Markov lag time, this becomes a valid assumption. 

The Markov model approach to solving equation \ref{eqn:masterequation} is to consider a discrete time process ($t = k\tau,\ k = 1, 2, \ldots$) and to partition the configurational phase space into $n$ discrete states so that  the dynamics of the system can be described by an $n\times n$ transition matrix $\mathbf{T}$ \cite{prinzMarkovModelsMolecular2011}. Each element of $\mathbf{T}$ describes the conditional probability of the system jumping between states over the Markov lag time, $\tau$ i.e., \cite{noeTransitionNetworksModeling2008}: 
\begin{equation}\label{eqn:discretemasterequation}
    \mathbf{p}((k+1)\tau) = \mathbf{p}(k\tau)\mathbf{T}
\end{equation}
The eigenvectors and eigenvalues of $\mathbf{T}$ represent the associated slow dynamic processes (e.g., protein folding or loop opening and closing) and their associated timescales \cite{prinzMarkovModelsMolecular2011}. 

Applications of MMs are concentrated on biomolecular systems and form an intrinsic part of the biomolecular simulation tool-box \cite{hugginsBiomolecularSimulationsDynamics2019}. Applications include modelling both protein folding pathways \cite{singhalUsingPathSampling2004,swopeDescribingProteinFolding2004} as well as intrinsically discorded proteins \cite{schorAnalyticalMethodsStructural2016a}. 
MMs have been applied to enzyme systems and used to elucidate, for example, ligand docking pathways \cite{ahalawatMappingSubstrateRecognition2018a} and regioselectivity mechanisms in cytochrome p450 \cite{dodaniDiscoveryRegioselectivitySwitch2016a}, the conformational heterogeneity in the important cancer target SETD8 \cite{chenDynamicConformationalLandscape2019a}, loop dynamics in triosephosphate isomerase \cite{LoopMotionTriosephosphate}, and allosteric effects in cyclophilin A \cite{wapeesittipanAllostericEffectsCyclophilin2019}. Other applications include self-assembly \cite{senguptaAutomatedMarkovState2019} and dimer formation \cite{leahyCoarseMasterEquations2016} of amyloid peptides, identifying important conformations in drug targets to improve drug docking free energy calculations \cite{amaroEnsembleDockingDrug2018}, and rational drug design \cite{gervasioBiomolecularSimulationsStructureBased2019}. Therehas been comparatively less use of MM on smaller systems (whose kinetics tend to be derived from quantum mechanical and thermodynamic data \cite{glowackiMESMEROpenSourceMaster2012, pillingMasterEquationModels2003}, rather than statistically estimated from MD data), however, one recent example used MMs to determine hydrogen bond rearrangement in liquid water \cite{schulzCollectiveHydrogenbondRearrangement2018}. 


Early MM construction consisted of describing conformational dynamics of systems in thermal equilibrium by constructing only a handful of discrete states, and modelling the dynamics as Markov chain also known as a \emph{Markov state model} (MSM). For example the authors of reference \cite{degrootEssentialDynamicsReversible2001} investigated the folding of a heptapeptide into a $\beta$-hairpin conformation in a solution of methanol.  To decide whether the folding process is a memoryless process (i.e., conforms to equation \ref{eqn:masterequation}) they estimated a four state MSM and compared the transition probabilities implied by the model (the elements of the $4\times 4$ matrix $\mathbf{T}$) to those observed in the MD trajectory. The four states were based on their geometric similarity in the space of a principal component analysis of the peptide backbone coordinates. They found them to be in good agreement and so concluded that their reduced dimension description of the folding process was valid. 

However, the more common approach \cite{husicMarkovStateModels2018, noeMarkovModelsMolecular2019b} is a two stage process. In the first stage, frames from MD simulations are  geometrically clustered into $n$ discrete \emph{microstates} (where typically $n \lesssim 1000$) and the elements of $\mathbf{T}$ are estimated in this microstate basis. The purpose of this discretization is to allow an precise description of the eigenvectors of $\mathbf{T}$ in terms of these microstates, the eigenvectors in turn describe the various conformational transitions \cite{perez-hernandezIdentificationSlowMolecular2013a}. The assumption behind the validity of this approach is that with a fine-grained definition of microstates (i.e., each microstate is structurally very similar) their geometric similarity is enough to guarantee their kinetic similarity.  In other words, if a set of MD frames are clustered into the same state $i$ (the elements of $\mathbf{p}$ in equation \ref{eqn:masterequation}) then they are all accurately described by the same set of rates to other discrete states ($K_{i, j}$). Care must be taken here as geometric similarity does not always imply kinetic similarity, so that structure which appear similar according to some metric may have very dissimilar kinetic properties \cite{bowmanUsingGeneralizedEnsemble2009a, krivovHiddenComplexityFree2004, noeTransitionNetworksModeling2008, berezovskaAccountingKineticsOrder2012}. The methods for creating this fine-grained kinetic model as well as alternatives to geometric clustering will be described in this section.

The second stage makes use of the fact that very often the case that there is a separation in timescales between the slow processes of interest and other processes. To take the triosephosphate isomerase example of earlier, loop 6 opens and closes on the timescale of \SI{100}{\micro\second} \cite{LoopMotionTriosephosphate}, whereas bond vibrations and rotations which are not immediately relevant to the loop motion are $<\SI{1}{\nano\second}$. This fact allows coarse-graining these microstates into a handful, $g$, of \emph{macrostates} based on their kinetic properties. The macrostates are usually defined such that microstates have a low probability of transitioning between the macrostates, compared to inter-conversion within a macrostate \cite{schutteDirectApproachConformational1999,swopeDescribingProteinFolding2004, prinzMarkovModelsMolecular2011}. Coarse-graining will be discussed in depth in sections \ref{sec:intro_coarse} and \ref{sec:num_metastable}. 


One approach to creating accurate MSMs is to focus on finding the ``essential degrees of freedom'' of the system \cite{zwanzigClassicalDynamicsContinuous1983a, schutteDirectApproachConformational1999} i.e., a small number of features  (compared to the number of atomic coordinates)  which focus on describing the slowest conformational processes in the system. This is justified when the slowest conformational changes are the ones of interest. Examples of features for describing protein folding include the root mean square deviation from the crystal structure, the fraction of contacts found in the crystal structure, or even thermodynamic quantities like energies arising from solvent interactions \cite{chongExaminingThermodynamicOrder2018}. There are several benefits to identifying these features before clustering microstates to estimate an MSM. First, geometric similarity as measured in the space of the features is more likely to correlate with kinetic similarity than atomic coordinates.  Second, it reduces the computational effort required to cluster MD frames into microstates. K-means clustering, a popular method for performing geometric clustering, has a computational complexity which scales with the number dimensions. So reducing the number of variables from \numrange{1000}{10000} (the order of the number of atomic coordinates for a typical protein\footnote{The modal number of atoms of structures in the Protein Data Bank is \numrange{2000}{3000} according \cite{bankPDBStatisticsPDB}}) to \numrange{10}{100} for typical number of molecular features, e.g., dihedral angles of the protein backbone, represents a large reduction in computational complexity.. 


As already discussed there have been many  machine learning techniques, for reducing the dimension MD trajectories, e.g., PCA, multidimensional scaling iso-map, sketch-map. However,  these techniques do not directly address capturing \emph{slow} dynamics. For example, the problem with using PCA as a preprocessing step before clustering into microstates is that the principal components explain the greatest \emph{configurational} variance \cite{perez-hernandezIdentificationSlowMolecular2013a}. To address this drawback, a similar technique, time-lagged independent component analysis, TICA \cite{perez-hernandezIdentificationSlowMolecular2013a, schwantesImprovementsMarkovState2013}, was introduced which captures the greatest \emph{kinetic} variance. The total kinetic variance describes the ability of the TICA components (or any basis set) to capture the slow dynamics of the system \cite{noeKineticDistanceKinetic2015}.  TICA identifies linear combinations of the atomic positions which are maximally correlated at a given lag-time (also called $\tau$ but not necessarily the same as the Markov lag-time). TICA is a stand-alone technique whose eigenvectors are the optimal linear approximation to the eigenvectors of $\mathbf{T}$ \cite{nuskeVariationalApproachMolecular2014}. But TICA can also be used a preprocessing step to reduce the dimensionality of MD data prior to estimating a MSM.  Using TICA as a preprocessing step in MSM construction has been systematically investigated and shown to be more accurate than both PCA and no preprocessing at all in capturing the slow dynamics \cite{husicOptimizedParameterSelection2016}. However, it brings with it two new modelling choices: the TICA lag-time $\tau$ and the number TICA components, $m$, onto which the atomic coordinates are projected. 


The variational approach to conformational dynamics, (VAC \cite{nuskeVariationalApproachMolecular2014}), cast estimating MSMs and TICA as a variational optimisation problem. While VAC showed that TICA and MSMs were the optimal description of the slow dynamics for a \emph{given} continuous and discrete basis set (respectively), the authors of reference \cite{mcgibbonVariationalCrossvalidationSlow2015} showed that the same variational principle could optimize the basis sets themselves. The key innovation of this work was to combine cross-validation \cite{arlotSurveyCrossvalidationProcedures2009} and the variational principle to score a given basis set using the generalized matrix Rayleigh coefficient, GMRQ \footnote{similar to finding variationally optimised electronic wave function basis sets}. The theory was then broadened with Koopman models to encompass simulations of systems out of thermal equilibrium \cite{wuVariationalKoopmanModels2017}. With the variational approach to Markov processes (VAMP \cite{wuVariationalApproachLearning2020c}) the theory of MSMs and Koopman models was unified into one conceptual framework. This increased the scope of MMs and presented a range of model scoring metrics (VAMP scores), of which the GMRQ was a special case. These theoretical advances have allowed the development the following iterative optimisation MM pipeline, starting with a set of MD trajectories:

\begin{enumerate}
    \item project atomic coordinates on to important features;
    \item geometrically cluster onto discrete microstates;
    \item estimate an MSM and score using VAMP score;
    \item repeat the previous steps by varying the type of feature, number of discrete states, etc., until a satisfactory VAMP score has been achieved.   
\end{enumerate}

Other approaches to building MSMs exist which don't focus on  projecting onto suitable features as a preprocessing step. VAMPnets \cite{mardtVAMPnetsDeepLearning2018} still utilises the variational framework but instead of discretising MD trajectories it uses a deep neural network to learn continuous, non-linear estimates of the eigenvectors of $\mathbf{T}$. It uses the atomic coordinates of MD trajectories directly mitigating the need to iteratively select and score features.  Enspara \cite{porterEnsparaModelingMolecular2019} is a package which facilitates clustering large volumes of MD data without the need to perform dimensionality reduction first. For example, coordinate trajectories of a cefotaximase \cite{hartModellingProteinsHidden2016} were clustered into fine grained microstates based on similar values of RMSD relative to a crystal structure.  By focusing on producing fine-grained microstates without the need to project the coordinates onto the slowest collective variables, this technique retains a larger range of dynamic processes (not just the slowest ones). As a result, the authors were were able to reveal important conformations, not captured in X-ray and other structural data, to explain the enzyme's specificity and antibiotic resistance.  

Another similar approach to Markov state models  for understanding conformational dynamics is discrete path sampling \cite{walesEnergyLandscapesCalculating2006}. DPS solves equation \ref{eqn:masterequation} by creating  microstates based on their kinetic properties rather than their geometric properties, using the potential energy surface rather than MD trajectories. First a database of local minima (which define the discrete microstates) and saddle points (corresponding to transition states between the microstates) are created by geometry optimisation of the potential energy surface of the system. Then, the elements of $\mathbf{K}$ can be estimated from the values of the potential energy in the microstates and transition states using transition state theory \cite{wales_2004}. This method is limited by the number of degrees of freedom (which increase the fluctuations in the potential energy surface) which for biomolecules can become prohibitively large \cite{noeTransitionNetworksModeling2008}. However, using implicit solvent models to limit the number of degrees of freedom the conformational dynamics of small and medium sized systems have been investigated. These include the folding dynamics of met-enkephalin \cite{evansFreeEnergyLandscape2003a} and tryptophan zipper peptide \cite{josephStructureThermodynamicsFolding2016}, characterising the free energy surface of intrinsically disordered proteins 
\cite{josephIntrinsicallyDisorderedLandscapes2018}, and the effect of point mutations on the a coiled-coil peptide \cite{roderTransformingEnergyLandscape2017}. 

\textbf{Chapter \ref{chap:theory}} sets out the theory of MMs relevant to this thesis which focuses on MSM estimation, TICA for preprocessing, variational optimisation of basis sets using VAMP scores. 


\section{Simplified Markov state model construction}
 The MSM literature has concentrated on large biomolecules because their hierarchy of atomic motions \cite{frauenfelderEnergyLandscapesMotions1991} give rise to a rugged, free energy surface with ``memoryless'' conformational transitions. As already mentioned, the key to success of the MM process is the creation of a good set of microstates to represent the dynamics of the system. When the focus is on the slow dynamics, the atomic coordinates are projected onto a set of features related to the slow processes, the essential degrees of freedom are estimated using TICA and then clustering into microstates in this performed in this reduced dimensional space. For biomolecules the process is complicated by the large number of potentially relevant features and other modelling choices which cannot be determined a-priori \cite{shallowayMacrostatesClassicalStochastic1996}. Instead, the iterative optimisation process (the variational approach to Markov processes) is used to choose the best set of modelling choices, however this can be computationally intensive.  In contrast, for systems with a much smaller number of relevant degrees of freedom, chemical intuition and  visualisation techniques can be used to guide the choice of collective variable.

\textbf{Chapter \ref{chap:water}} describes computational and experimental work designed to understand the diffusion of a single water molecule through a sucrose matrix, designed to mimic the conditions of water diffusion in secondary organic aerosol (SOA) droplets  \cite{songTransientCavityDynamics2020a} (i.e., aerosol consisting of organic molecules dissolved in water \cite[chapter 1]{stepheningramCausesMagnitudesAtmospheric2019}). 

%important source of total atmosphereic aerosol [SI10-12]

The motivation for studying aerosol in general is that they have wide ranging impacts on human and planetary health \cite{Ingram2017}, from smog in cities \cite{StopDenyingRisks2019}, directly affecting the radiative balance of the atmosphere by altering its chemical composition \cite{irvineHalvingWarmingIdealized2019}, and indirectly through their effect on cloud formation \cite{farmerAtmosphericProcessesTheir2015}. As to SOAs in particular, they have been increasing recognised as an important source of total atmospheric aerosol, alongside the more well known primary sources such as ocean spray, smoke from natural and man-made sources et cetera \cite{mcconnellSeasonalVariationsPhysical2008}. The water content of SOA influences its chemical reactivity \cite{varutbangkulHygroscopicitySecondaryOrganic2006} and physical properties like size and refractive index \cite{steimerElectrodynamicBalanceMeasurements2015,tangSimultaneousDeterminationRefractive1991}. Predicting water diffusion in SOA is therefore important to explaining a range of SOA phenomena. 

The Stokes-Einstein (S-E) definition of diffusion, $D$, relates the viscosity of the solvent, $\eta$, (sucrose, in this case) to the hydrodynamic radius of the solute, $a$, at a temperature $T$ is given by \cite[chapter  17]{dill2010molecular}:
\begin{equation}\label{eqn:diffusion_intro}
D=\frac{k_{\mathrm{B}} T}{C \pi \eta a},
\end{equation}
($C$ and $k_{B}$ are constants).  For SOA droplets existing in the low humidity parts of the atmosphere, water evaporates to the point that the organic constituents of the particle become the dominant mole fraction leaving water as the solute \cite{powerTransitionLiquidSolidlike2013, Price2014, Molinero2005}. In these regimes, large deviations from S-E diffusion occur \cite{powerTransitionLiquidSolidlike2013,Price2015,Chenyakin2017}. There is a continuing debate over the applicability of the S-E description of diffusion in SOA droplets (see chapter 7 of reference \cite{Ingram2017} for a review), with different ad-hoc modifications of the S-E being suggested \cite{Harris2009,price2016sucrose, Fernandez-Alonso2007} as well as a case being made for entirely new explanations \cite{saltzmanActivatedHoppingDynamical2006}. For the system studied in this thesis, the observed diffusion rate is much larger than that predicted from the observed viscosity of the sugar component and the water radius using the S-E equation. These deviations occur when the viscosity is so high that the aerosol droplets start to transition to a glassy state \cite{Bones2012}. The motion of sucrose matrix in this case becomes slow on the timescale of the motion diffusing water molecules, but not so slow that it can be considered stationary. 

The aim of chapter \ref{chap:water} is to both add to the debate over water diffusion in SOA by suggesting a microscopic mechanism for water diffusion in a system with large deviations from S-E behaviour, and to show that the iterative,  variational approach to building Markov states models described in the previous section is not always necessary. Instead a simplified approach  utilising chemical knowledge and intuition can be used to construct valid and informative MSMs. The MSM approach is justified because the interactions of the water molecule with the much large sucrose molecules creates a sufficiently complex free energy landscape that the ``memoryless'' assumption for configurational transitions holds.  However, the assumption that the transition rates do not change with time was not met due to the slow but persistent motion of the sucrose matrix. Another aim of this  chapter was therefore to demonstrate a simple way of accounting for non-stationary transition rates when constructing MSMs.  

\section{Evaluating Markov state model performance}\label{sec:intro_msm_perf}
The MM analysis pipeline described so far, consists of first transforming MD trajectories into features (the essential degrees of freedom, $\chi$), then reducing the dimension with TICA,  discretizing the TICA components into $n$ microstates, and finally estimating the MSM. The modelling choices or \emph{hyperparameters}, ($\chi, \tau, m, n$), create the MSM basis set, which in turn determine the accuracy of the resulting MSM, and so a method of evaluating the performance of these hyperparameters  is needed. While the ground truth of the kinetic processes is not available, the initial way forward came through cross-validation and the GMRQ \cite{mcgibbonVariationalCrossvalidationSlow2015}. 

The innovation in reference \cite{mcgibbonVariationalCrossvalidationSlow2015} was to create a model score, the GMRQ (the Rayleigh trace from quantum mechanics), which could be used to judge the quality of the model choices while accounting for the tendency of models to fit to noisy signals in the data (over-fitting). This was achieved through cross-validation \cite{arlotSurveyCrossvalidationProcedures2009}: a model is estimated using a portion of the data and scored on the remaining data. Maximizing the cross-validated GMRQ by varying the hyperparameters increases the accuracy of the eigenvectors \cite{mcgibbonVariationalCrossvalidationSlow2015}. The GMRQ is a special case of the first VAMP score, VAMP-1, while maximizing the total kinetic variance is the same as maximizing the VAMP-2 score. These VAMP metrics completed the analysis pipeline \cite{schererVariationalSelectionFeatures2019} which now can be summarised as: i) transform MD trajectories into features, $\chi$, ii) select reasonable choices of  hyperparameters (features, TICA parameters, number of discrete states) and calculate the cross-validated VAMP-2 score, iii) change the hyperparameters and repeat analysis, iv) stop when the VAMP-2 score stops increasing. 

\section{Hyperparameter optimisation}\label{sec:intro_hyper_opt}
Choosing the hyperparameters which maximize the VAMP-2 score is a `black-box' optimisation problem \cite{jonesEfficientGlobalOptimization1998a}, so called because no gradient information on the response of the VAMP to the hyperparameters is available. This is a common problem in the machine learning community where models have many parameters and may take days to train \cite{feurer2019hyperparameter}. In this case it is not feasible to exhaustively search through combinations of hyperparameters. A popular method for optimising large sets of hyperparameters is Bayesian optimisation (also known as sequential model based optimisation, SMBO) \cite{hutterSequentialModelbasedOptimization2011,NIPS2012_4522,bergstraAlgorithmsHyperParameterOptimizationa,bergstraMakingScienceModel2013}. The idea behind Bayesian optimisation is that there is an objective function which is costly to optimise \cite{brochuTutorialBayesianOptimization2010,shahriariTakingHumanOut2016} (in this case the VAMP-2 score). So instead of optimising this directly, the BO procedure builds an statistical model of  objective function known as a \emph{surrogate function} or \emph{response surface}, using randomly sampled  values of the objective function. Having built an initial response surface, searching for the next hyperparameter to evaluate is guided by an \emph{acquisition function}. These can be selected or adjusted to trade off high-uncertainty regions (the `explore' regime) of the response surface with the high-value, low-uncertainty  regions (the `exploit' regime) \cite{shahriariTakingHumanOut2016}. A suggestion is evaluated, the response surface updated and the process repeats. Bayesian optimisation for hyperparameter optimisation is popular, as the number of packages designed for this purpose will attest (this list is non-exhaustive): Hyperopt \cite{bergstraHyperoptPythonLibrary2013}; sequential model-based algorithm configuration \cite{hutterSequentialModelbasedOptimization2011}, SMAC; BayesOpt \cite{martinez-cantinBayesOptBayesianOptimization2014}; Spearmint \cite{DBLP:conf/uai/GelbartSA14,snoekAbstractBayesianOptimization2013,snoekInputWarpingBayesian2014a,NIPS2013_5086,NIPS2012_4522}, GPyOpt \cite{gpyopt2016}, DragonFly \cite{JMLR:v21:18-223}; Auptimiser \cite{liuAuptimizerExtensibleOpenSource2019}; and Osprey \cite{mcgibbonOspreyHyperparameterOptimization2016}. A popular choice of response surface model is a Gaussian process (GP) \cite{rasmussenGaussianProcessesMachine2006}, a highly flexible type of model which fits naturally within the Bayesian optimisation paradigm \cite{shahriariTakingHumanOut2016}. Indeed, six of the eight packages listed here all implement some kind of Gaussian process  as their response surface model.

The aim of \textbf{chapter \ref{chap:msm}} is to demonstrate the use Bayesian optimisation to optimise the MSM hyperparameters using cross-validated VAMP-2 scores of the model system alanine dipeptide. In addition, the parameters of GPs are explored as a way to describe the relevance of hyperparameters in determining the VAMP-2 score. This chapter lays the ground-work for performing a similar analysis on AADH in chapter \ref{chap:aadh}, in particular: how to fit and interpret GPs and how to use GPs with Bayesian optimisation to optimise hyperparameters. 

\section{Coarse-graining}\label{sec:intro_coarse}

Having arrived at $n$ microstates via an optimal set of MSM hyperparameters by maximising the kinetic variance, the second  stage in the MM pipeline is to coarse grain potentially thousands of microstates into a handful of macrostates to create a more interpretable model. 

However, coarse graining an existing MSM is not the only approach to gaining insight into the conformational landscape of biomolecules.  There are other statistical clustering techniques that have been used for this purpose. The authors of \cite{troyerProteinConformationalLandscapes1995} used hierarchical clustering \cite[chapter 10 of]{friedman2001elements} to group MD frames into groups with mutual root mean square deviation (RMSD) in their alpha-carbon positions below some small threshold value. Hierarchical clustering shows how conformations cluster together as the threshold RMSD is increased.  In this way the conformational landscape at different levels of spatial resolution can be determined and the number of clusters determined by other criteria. In reference \cite{troyerProteinConformationalLandscapes1995} the number of clusters was chosen so that members of each cluster were in the same potential energy minima (albeit this was imperfect as the clustering was still based on geometric similarity, see discussion in section \ref{sec:mm_applications}).  In reference \cite{karpen1993statistical} the authors used a neural network clustering algorithm, ART-2$^{\prime}$ \cite{carpenterARTSelforganizationStable1987}, to investigate the folding mechanism of a pentapeptide. Folding events were described by up to six different clusters where the clustering took place in the space of residue dihedral angles. The number of clusters was determined by considering the size of the clusters in the dihedral space, as opposed to considerations arising from observed conformational changes. 

The main drawback of clustering based on geometric measures of similarity are that metastable macrostates are actually defined by their kinetic properties (i.e., conformations in a metastable macrostate undergo rapid, mutual, inter-conversion over a given timescale) which are not necessarily the same \cite{schutteDirectApproachConformational1999} as configurational similarity, as already discussed in section \ref{sec:mm_applications}. Kinetic clustering dates back to at least 1969 when Kuo and Wei \cite{weiLumpingAnalysisMonomolecular1969, kuoLumpingAnalysisMonomolecular} investigated the conditions under which both exact and approximate coarse graining of systems of coupled first order reactions could occur. The term \emph{exact} implying that the coarse-grained description gave rise to kinetic description consistent with the underlying microscopic kinetics.  Hummer and Szabo \cite{hummerOptimalDimensionalityReduction2015a} tackled the problem of how to define an appropriate coarse-grained rate matrix for a given coarse-graining scheme. i.e., given a mapping of micro- to macrostates, what is the most appropriate way of defining the rate matrix?  They derived expressions for rate matrices which are exact for non-Markovian dynamics  for a given coarse graining scheme (i.e., for systems where transition probabilities are dependent on the history of states visited).  They also derived expressions for the case of Markovian dynamics which, while approximate, still ensured that the macrostate relaxation timescales were exact. In addition, they proposed an algorithm for optimally determining this coarse-graining by iteratively partitioning microstates, such that the timescales implied in their rate matrices are maximized at each step. The authors of reference \cite{kellsCorrelationFunctionsMean2020} went on to show that this scheme was equivalent to constructing coarse-grained rate matrices which preserve the mean-first passage times of the system. This work has been developed to identify not only metastable macrostates but also the comparatively short lived transition states \cite{martiniVariationalIdentificationMarkovian2017}. 

Other methods, solely based on identifying metastable macrostates have been developed. Perron Cluster Cluster Analysis (PCCA) \cite{deuflhardIdentificationAlmostInvariant2000a} and its subsequent `robust' alternative PCCA+ \cite{deuflhardRobustPerronCluster2005b} use the sign structure of a given number of slow eigenvectors of $\mathbf{T}$ to determine the boundaries between macrostates: two microstates are in the same macrostate if they have the same sign for a given eigenvector.  The hierarchical Nystr{\"o}m exstension graph method is a hierarchical clustering method based on the Nystr{\"o}m approximation for approximating $\mathbf{T}$ with a submatrix of well sampled states \cite{yaoHierarchicalNystromMethods2013a}. This submatrix retains the same sign structure of the full matrix and uses this sign structure with PCCA to cluster microstates. The Bayesian aglomerative cluster engine (BACE) \cite{bowmanImprovedCoarsegrainingMarkov2012a} uses Bayesian statistics to assess whether two microstates have the same transition rates to each putative macrostate - if they do then they are lumped into the same state. Other methods include the most probable path \cite{jainIdentifyingMetastableStates2012a} which assigns microstates to the same macrostate if they occur on the sequence of states with the highest transition probabilities; methods based on the renormalisation group \cite{orioliDimensionalReductionMarkov2016c};  minimum variance cluster analysis, MVCA \cite{husicMinimumVarianceClustering2018} which uses the hierachical clustering to merge rows of the transition matrix; and projected Markov models, PMMs \cite{noeProjectedHiddenMarkov2013a}. PMMs include observer operator models \cite{wuProjectedMetastableMarkov2015} and hidden Markov models \cite{noeProjectedHiddenMarkov2013a} (HMM) which are dynamical models in which the microstate/macrostate coarse-grained structure is directly incorporated into the model definition.  These coarse-graining methods were all developed with biomolecular systems in mind and have been applied to various model systems such as alanine dipeptide, $\beta$-lactamase, the fast-folding villin headpiece (see their accompanying references and an overview of many of these methods in reference \cite{bowmanQuantitativeComparisonAlternative2013}). However, it is hidden Markov models which have proved one of the most popular coarse-graining methods \cite{mondalAtomicResolutionMechanism2018a, plattnerCompleteProteinProtein2017, panConformationalHeterogeneityMichaelis2016, juarez-jimenezDynamicDesignManipulation2020, wangDynamicalBehaviorVLactamases2019,FastFoldingPathwaysThrombinBinding2018,remingtonFluorescenceQuenching2aminopurinelabeled2019,curado-carballadaHiddenConformationsAspergillus2019,furiniIontriggeredSelectivityBacterial2018,yangMappingPathwayDynamics2018,ahalawatMappingSubstrateRecognition2018,olaposiMembraneBoundTranscriptionFactor2019, xiaoNaBindingModes2019, hansonWhatMakesKinase2019}. 

\section{Number of metastable states}\label{sec:num_metastable}
When coarse-graining MSMs (or performing any type of cluster analysis) a key parameter is the number of clusters $g$, e.g., does the data support the hypothesis of $g=2$ or $g=3$ (say) clusters \cite{milliganExaminationProceduresDetermining1985}. Choosing the value of $g$ (or any parameter not estimated from the data) is known as model selection \cite[chapter 7]{friedman2001elements}. Choosing the optimum value of $g$ is important as each macrostate pertains to conformations important to the dynamical process being studied \cite{frauenfelderEnergyLandscapesMotions1991}. If the number of macrostates modelled are too few, then important conformations will be lost, whereas with too many macrostates, the model loses its interpretability and can potentially  create macrostates which are artifacts of noise in the data, a processes called `over-fitting' \cite[chapter 7]{friedman2001elements}. The dynamics of proteins is hierarchical \cite{frauenfelderEnergyLandscapesMotions1991} with short lived states aggregating to longer lived states, and as such kinetic clustering must always be in relation to some timescale. However, even given this timescale, the coarse graining and clustering methods so far described do not automatically select the number of macrostates - although some methods provide more information than others to help guide selection. A general approach to determining the appropriate number of macrostates is to look for gaps in the eigenvalues of the transition matrix or its implied timescales \cite{prinzMarkovModelsMolecular2011, mcgibbonVariationalCrossvalidationSlow2015, deuflhardIdentificationAlmostInvariant2000a}. However, due to poor microstate construction or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}, additionally, this also does not allow for transition state macrostates \cite{martiniVariationalIdentificationMarkovian2017}. The variationally optimized coarse-graining method of reference \cite{martiniVariationalIdentificationMarkovian2017} allows the user to test whether a transition macrostate has been found but leaves the decision to continue finding more macrostates up to the user. Hierarchical approaches such as BACE,  HNEG and MVAC, automatically show how the microstates group together for different values of the clustering criteria, allowing the user to judge the most useful clustering.  A more general method for MSMs using Bayesian statistics has been developed, which takes as its data the mapping between the micro- and macrostates \cite{bacalladoBayesianComparisonMarkov2009a} and so is independent of clustering method. To decide on an appropriate number of macrostates, the Bayes factor (the Bayesian weight of evidence for a particular hypothesis \cite{kassBayesFactors1995}), for different numbers of macrostates is calculated and used to select $g$. The evidence is proportional to the probability of observing the microstates \emph{given} the particular coarse-graining and data \cite{bacalladoBayesianComparisonMarkov2009a}. This method is versatile and naturally takes into account model over-fitting \cite{bacalladoBayesianComparisonMarkov2009a} but it is computational intensive.  

Projected Markov models are distinct from the other techniques described above in that they are estimated by maximizing a likelihood function \cite{wuProjectedMetastableMarkov2015, noeProjectedHiddenMarkov2013a} i.e, the probability of observing model parameters given a set of data.  This thesis will concentrate on HMMs which are model of a Markovian process between $g$ \emph{hidden} macrostates i.e., states which are not directly observed in the data. While in a macrostate the system emits randomly, according to a probability distribution, to one of a set of \emph{observed} microstates, which \emph{are} seen in the data (the emission distributions thus define the coarse-graining). 

Maximum likelihood models have a wide range of model selection techniques available to them which are not explicitly related to Markov processes but are nevertheless applicable because the Markov property is subsumed into the likelihood function \cite[chapter 7]{friedman2001elements}\cite{milliganExaminationProceduresDetermining1985, mclachlanFiniteMixtureModels2000}.  Some popular techniques include cross-validation \cite{arlotSurveyCrossvalidationProcedures2009}, the Akaike information criterion (AIC) \cite{akaikeInformationTheoryExtension1998}, the  Bayesian information criterion (BIC) \cite{schwarzEstimatingDimensionModel1978a}, and cross-validation of the log-likelihood (CVLL) which have all  been used to estimate the number of macrostates in HMMs \cite{celeuxSelectingHiddenMarkov2008}. The AIC uses the likelihood to approximate the out-of-sample predictive accuracy of the model, whereas the difference in BICs for two models is approximately equal to the Bayes factor for those models (this is directly related to the Bayes factor approach of \cite{bacalladoBayesianComparisonMarkov2009a} described previous). Both the AIC and BIC benefit from requiring negligible extra calculation once a model has been estimated and have additionally been used to select the number of microstates in MSMs of conformational dynamics \cite{mcgibbonStatisticalModelSelection2014a} as well as being ubiquitous for general model selection \cite[chapter 7]{friedman2001elements}.  A BIC-like criterion called the integrated complete data likelihood (ICL) \cite{biernackiAssessingMixtureModel2000a} has been derived specifically for clustering methods such as HMMs and mixture models (which  group observations into macrostates, albeit without Markov dynamics) \cite{mclachlanFiniteMixtureModels2000}. The ICL differs from the BIC and Bayes factor approaches in that the evidence it considers is proportional to the probability of observing the microstates \emph{and} the coarse-graining \emph{given} the data \cite{biernackiAssessingMixtureModel2000a,mclachlanFiniteMixtureModels2000}.  The ICL has been used extensively \cite{mclachlanFiniteMixtureModels2000} for mixture models and a recent assessment \cite{brochadoDeterminingNumberComponents2020} finds it performs well across a range of types of mixtures. The CVLL, BIC, AIC and ICL, have been utilised to determine the number of macrostates in HMMs, but yet not within biomolecular dynamics context.  


\textbf{Chapter \ref{chap:hmm}} explores the utility of approximations to the Bayes factor and similar criteria for determining the optimal value of $g$: the Bayesian information criterion, BIC \cite{schwarzEstimatingDimensionModel1978a}, the integrated complete data likelihood criterion, ICL \cite{biernackiAssessingMixtureModel2000a}, the Akaikie information criteria, AIC \cite{akaikeInformationTheoryExtension1998}, and cross-validated log-likelihood, CVLL \cite{celeuxSelectingHiddenMarkov2008}. The aim of this chapter is to determine which of these statistical model selection criteria can determine the correct number of metastable states from simulations of  model system. The main benefit of the these criteria (except CVLL) are that they require little additional calculation after estimating a model, in contrast to the full Bayes-factor method of reference \cite{bacalladoBayesianComparisonMarkov2009a}.  This chapter lays the ground-work for application to determining the optimal coarse-grained description of the dynamics of AADH in chapter \ref{chap:aadh}. 

% Chapter \ref{chap:msm} demonstrated using response surface methods and Bayesian optimisation to arrive at an optimal MSM. In order to make sense of the potentially thousands of MSM microstates, it is common practice to kinetically lump them into a smaller number of macrostates. There have been a large number of different methods proposed and coarse-graining has been studied since at least 1969 \cite{kuoLumpingAnalysisMonomolecular, weiLumpingAnalysisMonomolecular1969}. The first among the most recent applications of coarse-graining MSMs was Perron cluster cluster analysis, PCCA \cite{deuflhardIdentificationAlmostInvariant2000a}) and its successor, robust PCCA or PCCA+ \cite{deuflhardRobustPerronCluster2005b}. Other methods include the hierarchical Nystr{\"o}m exstension graph, HNEG \cite{yaoHierarchicalNystromMethods2013a}, the Bayesian agglomerative clustering engine, BACE \cite{bowmanImprovedCoarsegrainingMarkov2012a}, methods based on the renormalisation group \cite{orioliDimensionalReductionMarkov2016c, hummerOptimalDimensionalityReduction2015a}, the most probable path algorithm \cite{jainIdentifyingMetastableStates2012a}, a method based on variationally optimising the coarse grained transition matrix \cite{martiniVariationalIdentificationMarkovian2017a}, minimum variance cluster analysis, MVCA \cite{husicMinimumVarianceClustering2018}; and projected Markov models, PMMs \cite{noeProjectedHiddenMarkov2013a}. PMMs are Markov processes projected onto a coarse grained set of states and are exactly described by observable operator models, OOMs \cite{wuProjectedMetastableMarkov2015}, and approximated by hidden Markov models, HMMs \cite{noeProjectedHiddenMarkov2013a} under certain conditions (HMMs have also been proposed to describe protein dynamics, from other, non-coarse-graining perspectives \cite{mcgibbonUnderstandingProteinDynamics}). OOMs are a generalisation of HMMs \cite{jaegerDiscretetimeDiscretevaluedObservable} and despite their theoretical advantages it is HMMs that have been more widely used to coarse grain molecular dynamics simulations \cite{mondalAtomicResolutionMechanism2018a, plattnerCompleteProteinProtein2017, panConformationalHeterogeneityMichaelis2016, juarez-jimenezDynamicDesignManipulation2020, wangDynamicalBehaviorVLactamases2019,FastFoldingPathwaysThrombinBinding2018,remingtonFluorescenceQuenching2aminopurinelabeled2019,curado-carballadaHiddenConformationsAspergillus2019,furiniIontriggeredSelectivityBacterial2018,yangMappingPathwayDynamics2018,ahalawatMappingSubstrateRecognition2018,olaposiMembraneBoundTranscriptionFactor2019, xiaoNaBindingModes2019, hansonWhatMakesKinase2019}. 


% This is often done by fitting a hidden Markov model (HMM) to the same microstates used to estimate the MSM \cite{noeMarkovModelsMolecular2019b}, although many other techniques exist (e.g., observer operator models \cite{wuProjectedMetastableMarkov2015}, see chapter \ref{chap:hmm} for a full discussion). A HMM is a model of a Markovian process between $g$ \emph{hidden} states i.e., states which are not directly observed in the data. While in a hidden state the system emits randomly, according to a probability distribution, to one of a set of \emph{observed} states, which are seen in the data. The transition rates and emission probabilities are estimated from the data.  The only hyperparameter stipulated by the user (at least for the discrete HMMs considered here) is the number of hidden states, $g$. To model the metastable dynamics, the number of hidden states of the HMM is set equal to the number of metastable states of the system, $r$ \cite{noeProjectedHiddenMarkov2013a}. The number of metastable states is determined by looking for gaps in the eigenvalue spectrum of the MSM \cite{prinzMarkovModelsMolecular2011,noeProjectedHiddenMarkov2013a}. However, due to poor discretisation or insufficient sampling, a clear cut gap is not always possible \cite{bowmanQuantitativeComparisonAlternative2013}. This observation, along with the plethora of other coarse-graining schemes motivated the introduction of Bayes factors \cite{kassBayesFactors1995} for both judging the quality of the coarse graining scheme and the number of metastable states \cite{bacalladoBayesianComparisonMarkov2009a}. The Bayes factor is the ratio of the integrated likelihood of the data given the model parameters \cite{kassBayesFactors1995}, for two competing models. Bayes factors measure the relative evidence of a model (its parameters and hyperparameters) given a trajectory of \emph{observed states} (the microstates). For example, if the Bayes factor between a HMM with $g=2$ and  $3$ hidden states is $100$ then the $g=2$ model is more strongly favoured. 

% The integrated likelihood only takes into account how the model explains the observed states \cite{biernackiAssessingMixtureModel2000a,mclachlanFiniteMixtureModels2000}. It does not factor in directly the hidden state structure of the model. Rather than view the HMM as a coarse graining technique, another approach is to view an HMM and other hidden mixture models as a clustering algorithm \cite{mclachlanFiniteMixtureModels2000} where observed states are classified as belonging to hidden states. An important analogue of the integrated likelihood in mixture model community is the integrated classification likelihood \cite{mclachlanFiniteMixtureModels2000}. This measures the evidence for  both the observed states \emph{and the hidden states} given the model parameters and hyperparameters. 

% \textbf{Chapter \ref{chap:hmm}} explores the utility of approximations to the Bayes factor and similar criteria for determining the optimal value of $g$: the Bayesian information criterion, BIC \cite{schwarzEstimatingDimensionModel1978a}, the integrated complete data likelihood criterion, ICL \cite{biernackiAssessingMixtureModel2000a}, the Akaikie information crteria, AIC \cite{akaikeInformationTheoryExtension1998}, and cross-validated log-likelihood, CVLL \cite{celeuxSelectingHiddenMarkov2008}. This extends previous work \cite{mcgibbonStatisticalModelSelection2014a} which looked at the AIC and BIC for selecting parameters of MSMs. Like chapter \ref{chap:msm} this lays the ground-work for application to the case of AADH in chapter \ref{chap:aadh}. The example used is a four well potential, the Prinz potential \cite{prinzMarkovModelsMolecular2011}, with a well defined number of metastable states against which the BIC, ICL, AIC and CVLL can be judged. The benefits of the these information criteria (except CVLL) are that they do not required extra calculation after a HMM has been estimated and so lend themselves to situations in which a large number of models need to be tested, a situation that will be encountered in this thesis. 

\section{Aromatic amine dehydrogenase}

Aromatic amine dehydrogenase (AADH) oxidizes primary aromatic amines, such as tryptamine, into the corresponding aldehyde and ammonia. The rate limiting step is the proton transfer from a covalently bound Schiff base intermediate to an acceptor aspartate oxygen atom \cite{masgrauAtomicDescriptionEnzyme2006}.  AADH is notable because it exhibits a large primary kinetic isotope effect: substituting deuterium for the hydrogen being cleaved in tryptamine causes the rate to drop by up to a factor of $55$ \cite{masgrauAtomicDescriptionEnzyme2006}.  A drop in the rate is expected when considering the cleavage of the heavier deuterium atoms as the C---H bond has a larger zero-point energy than the C---D bond, effectively decreasing the height of potential barrier the reaction has to cross. However, if the zero-point energy were the only difference contributing to the difference in rates, a KIE of approximately \num{8} would be expected \cite{antoniouLargeKineticIsotope1997}.  The fact that the observed KIE is almost \num{7} times as large implies significant quantum mechanical tunneling \cite{masgrauAtomicDescriptionEnzyme2006, klinmanbeyond2009, basranImportanceBarrierShape2001a}, i.e., at C---H bond distances below the top of the potential energy barrier for the reaction, the proton can transfer to the product state without the need for the kinetic energy required by classical mechanics.  In addition to the presence of tunneling indicated by the inflated KIE, the KIE of AADH is independent of temperature, despite the fact that the underlying reaction is dependent on temperature \cite{masgrauAtomicDescriptionEnzyme2006}. 


The motivation for studying AADH and other enzymes such as monoamine dehydrogenase (MADH) \cite{brooksDeuteriumKineticIsotope1993, basranEnzymaticHTransferRequires1999},  soyabeen lipoxygenase (SLO) \cite{glickmanExtremelyLargeIsotope1994, knappTemperatureDependentIsotopeEffects2002} and  DHFR \cite{sikorskiTunnelingCoupledMotion2004, loveridgeSolventEffectsCatalysis2010a} (as well as many others \cite{puMultidimensionalTunnelingRecrossing2006}) is that the KIEs are temperature independent and often large in absolute value, which cannot be explained \cite{klinmanHydrogenTunnelingLinks2013} by the dominant explanation of thermally activated reaction rates, namely transition state theory (TST) \cite{garciavilocaHowEnzymesWork2004} . These observations have prompted a range of explanations and models going beyond TST \cite{masgrau2004hydrogen, brunoVibrationallyEnhancedTunneling1992, borgisCurveCrossingFormulation1996, antoniouLargeKineticIsotope1997, klinmanbeyond2009}.  Importantly for this thesis these models  link the conformational dynamics of the to observable properties (reaction rates and KIEs) of the enzyme. \emph{Understanding the conformational dynamics of AADH will be important evidence in future debates over the validity of these models.}  

In the TST picture the enzyme-substrate complex (for AADH this is the Schiff-base after reaction with tryptamine) undergoes thermal fluctuation from the reactant state,  along the reaction coordinate through a transition state, and on to the product state (the oxidized Shiff-base in AADH). The rate at which this happens is proportional to $\exp{(-\Delta G^{\mathrm{TS}}/RT)}$, where  $\Delta G^{\mathrm{TS}}$ is the free energy difference between the reactant and transition states (the activation free energy), $R$ is the gas constant and $T$ is the temperature. The zero-point energy difference in isotopes changes the value $\Delta G^{\mathrm{TS}}$. Tunnelling occurs when the thermal fluctuation along the reaction coordinate brings hydrogen atom close enough to the acceptor atom so that wave-functions of the reactant and product state overlap, effectively lowering the value of $\Delta G^{\mathrm{TS}}$ \cite{puMultidimensionalTunnelingRecrossing2006}.  However, as Klinman and Kohen point out \cite{klinmanHydrogenTunnelingLinks2013}, this model predicts both temperature dependent rates and KIEs, while for AADH the rate determining step is temperature \emph{dependent} while the KIE is temperature \emph{independent}. 

The main alternative to TST used to explain enzymatic reactions involving tunneling are `Marcus-like' (which take their name from their similarity to the Marcus theory of electron transfer \cite{marcusElectronTransfersChemistry1985}) or full-tunneling models, which were originally adapted for hydrogen transfer reactions by Kuznetsov and Ulstrop \cite{kuznetsovProtonHydrogenAtom1999a}. These models decouple tunneling from other processes by factorizing the rate into two terms \cite{kuznetsovProtonHydrogenAtom1999a, antoniouLargeKineticIsotope1997, knappTemperatureDependentIsotopeEffects2002} (for an extensive review of the different types of models and their applications see reference \cite{puMultidimensionalTunnelingRecrossing2006} and \cite[chapters 4, 5 and 6 ]{allemannQuantumTunnellingEnzymeCatalysed2004}). The first term describes the process of rearranging the heavy atoms into an state ready for tunneling and is an activated process - i.e., determined by an activation energy. The second term describes the tunneling process and is therefore  largely determined by the properties of the atom being transferred (the hydrogen or deuterium atom or ion). This second term describes  the probability of of tunneling occurring  in terms of fluctuations in the donor-acceptor distance (DAD - the distance moved by the hydrogen atom in the course of the reaction). This may or may not depend on temperature, depending on whether fluctuations in the DAD are necessary for tunneling to occur. This separation allows the model to accommodate both temperature dependent rates of reaction and KIEs which are either temperature dependent or independent \cite{klinmanHydrogenTunnelingLinks2013}. Full tunneling models been applied to experimental results of AADH, MADH and others \cite{johannissenProtonTunnelingAromatic2007, johannissenEnzymeAromaticAmine2008, masgrau2004hydrogen, sutcliffeHydrogenTunnellingEnzymecatalysed2006, stojkovic2012effects, klinmanHydrogenTunnelingLinks2013, puMultidimensionalTunnelingRecrossing2006} which explain the temperature dependent rates as being largely due to the rearrangement of the enzyme prior to the tunneling process.  Once in the necessary configuration tunneling occurs without the need for further thermal fluctuations, meaning the tunneling rate does not depend on temperature and only on the mass of the transferring hydrogen isotope, giving rise to temperature independent KIEs.

Full-tunneling models are not the only explanation for reaction rates and temperature independent KIEs. In reference \cite{glowackiProteinDynamicsEnzyme2012, glowackiTakingOckhamRazor2012b} the authors argue that by extending transition state theory to include the effect of conformational dynamics, the temperature dependencies of KIEs can be explained without the need for full-tunneling models. Their model posits two conformational sub-states, rapidly inter-converting (relative to the reaction timescale) which both react  via different pathways and therefore with different activation energies and different degrees of tunneling. They showed that fitting this model to kinetic data from AADH, MADH, SLO and DHFR, reproduced the temperature dependence of the KIE in all four enzymes, however the fitted parameters have been criticized as being unrealistic \cite{klinmanHydrogenTunnelingLinks2013}. 

Other models of enzymatic reaction rates have been put forward which incorporate non-equilibrium dynamic motions such as networks of promoting vibrations which couple to the reaction coordinate across the enzyme  (see reference \cite{antoniouProteinDynamicsEnzymatic2011} for a supportive review of these proposals). In fact, the role of DAD fluctuations in the rate of tunneling driven reactions have prompted some \cite{adamczykCatalysisDihydrofolateReductase2011, pisliakovEnzymeMillisecondConformational2009} to assume that this implies non-equilibrium effects are needed to explain both the rates of reactions and the origin of catalysis (i.e., the rate enhancement due to the enzyme). However this has been refuted \cite{klinmanHydrogenTunnelingLinks2013} on the grounds that these fluctuations are thermally activated (this will be more thorough described  in chapter \ref{chap:aadh}). This critique is part of a larger controversy  \cite{klinmanHydrogenTunnelingLinks2013, puMultidimensionalTunnelingRecrossing2006, mcgeaghProteinDynamicsEnzyme2011,glowackiTakingOckhamRazor2012b,glowackiProteinDynamicsEnzyme2012} surrounding proposals for models incorporating non-equilibrium effects in enzyme catalysis. One particular proposal is that certain fast conformational transitions provide the inertia needed for the enzyme to take the reaction to completion \cite{eisenmesserIntrinsicDynamicsEnzyme2005, henzler-wildmanIntrinsicMotionsEnzymatic2007, olssonSoluteSolventDynamics2004}. While this has been criticized in light of evidence from simulations \cite{kamerlinMultiscaleModelingBiological2011}, it provides yet another example of the need to consider the role of conformational dynamics in explaining enzymatic reaction rates.  


It is clear from the preceding discussion that the conformational dynamics of an enzyme such as AADH, with its large and temperature independent KIE, will be an important contribution to the debate over the validity of the number of different models of enzyme reactivity. Despite being well studied in other areas, no simulation study has so-far described the conformational dynamics of AADH in the reactant state of its rate determining step.  

The aims of \textbf{Chapter \ref{chap:aadh}} are thus two-fold.  First, this chapter describes molecular dynamics simulations of AADH in its reactant state and uses this data to create a Markov state model description of its conformational dynamics. To do this an  set of MSM hyperparameters is optimised and understood in terms of the response surface, utilising the work of chapter \ref{chap:msm}, in addition a set of sensitivity models are proposed. The model selection criteria of chapter \ref{chap:hmm} are used to select the appropriate number of hidden states for a coarse-grained description using HMMs. The second aim is to critically assess the MSM optimisation and model selection criteria  using the AADH system as a `real-world' test.

\textbf{Chapter \ref{chap:conclusions}}, discusses the conclusions of this thesis and sets out concrete steps for further work in this area. 


% using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 


% Gaussian Processes (GPs) have been


% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 


% with a concomitant increase in the number of software packages 


%  - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% % Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}




% \textbf{From art to science:} 
% MSMs are master equations (ME review [6])
% Model is n x n matrix, spans the conformational space, conditional probabilities as elements. 
% State populations are give thermodynamics, off-diagonal elements give kinetics. 
% If we assume thermodynamic equilibrium then eigendecomposition is useful [description of eigenvectors]
% the n states should capture the dyanmics. 

% Memoryless transition networks come from Zwanzig [1] then Van Kampen [2] key MSM papers [7-9]. 

% creating meaningful states is difficult [33,34] Karpen did dihedrals [10] de Groot [20] did PCA and k-medioids.

% can stitch together different trajectories: McCammon [11] now Folding@home [35], Luty [12] suggested stitching together different trajs for ligand binding. 

% Hardware devs: FoldingAtHome, BlueGened [39-41], GPUGRID [43,44]

% ITS plots [9] CKtest [46]

% Different features different dynamics [45]

% Errors are (1) state decomposition and (2) finite sampling [47]

% Global descriptors worse than internal degrees of freedom [49]

% Sarich [73, 92]: discretization error decreases and partitioning become finer and the lag time increases. 

% Djurdjevac [93]: upper bounds for error between MSM and trajectories decreases with lag time. 

% TICA\c variance explained [115]:
% This MSM score was termed the GMRQ, which stands for
% generalized matrix Rayleigh quotient, the form of the approx-imator (also referred to as the Rayleigh trace).124
% The GMRQ on the validation set will be poor if the model was overfit on the
% training set but better if the model identifies the underlying
% dynamics common to both sets. In 2016, No and Clementi115 demonstrated that kinetic variance in a data set can be explained area.
% by summing the squared tICA eigenvalues. Since the variational principle derived in No and Nske95 holds for any strictly nonincreasing weights applied to the scored eigenvalues,96 the kinetic variance can also be used to score models, or to deter- mine how many tICs are needed to explain a given amount of kinetic variance in the data.

% \textbf{Simple MSM}
% Analysis of three water molecules \cite{schulzCollectiveHydrogenbondRearrangement2018} to understand the collective hydrogen bond rearrangement, uses both Euler angles and spherical coordinates for dofs. 








% \textbf{Coarse graining and HMMs}


% \textbf{Hyperparameter search}
% Feature selection: \cite{schererVariationalSelectionFeatures2019}


% \cite{bergstraHyperoptPythonLibrary2013} Hyperopt - Bayesian optimisation using TPEs (has conditional variables). 
% \cite{mcgibbonOspreyHyperparameterOptimization2016a} Osprey
% \cite{hutterSequentialModelbasedOptimization2011} SMAC

% Spearmint: \cite{DBLP:conf/uai/GelbartSA14}\cite{snoekAbstractBayesianOptimization2013}\cite{snoekInputWarpingBayesian2014a}\cite{NIPS2013_5086}\cite{NIPS2012_4522}

% BayesOpt: \cite{martinez-cantinBayesOptBayesianOptimization2014}

% GPyOpt: \cite{gpyopt2016}

% DragonFly: \cite{JMLR:v21:18-223}

% Auptimiser: \cite{liuAuptimizerExtensibleOpenSource2019}
% Ecabc: \cite{Sharma2019}

% Particle swarm: \cite{lorenzo2017particle}

% Optunity: \cite{claesenEasyHyperparameterSearch2014}

% \cite{pmlr-v32-hutter14} use random forest and ANOVA to assess parameter importance. 
% \cite{gramacyVariableSelectionSensitivity2013} using tree models to optimize and explore relevance of options for compiling and optmizing computer code. 

% \cite{falknerBOHBRobustEfficient2018a} goes beyond BO and random bandit. 

% \cite{di2018genetic} genetic algorithm for hyperparameter search. 

% SMBO: \cite{hutterSequentialModelbasedOptimization2011}

% Practical Bayesian optimisation of machine learning algorithms \cite{NIPS2012_4522} (GPs only)

% Algorithms for hyper-parameters optimisation \cite{bergstraAlgorithmsHyperParameterOptimizationa} (GP and TPE)

% Making a science out of hyperparameter search \cite{bergstraMakingScienceModel2013}. 




















