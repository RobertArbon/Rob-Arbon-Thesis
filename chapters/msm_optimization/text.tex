\let\textcircled=\pgftextcircled
\chapter{Markov State Model Optimization}\label{chap:msm}


\section{Antonia Mey comments}
\emph{Structure:}
I think the main thing I would change,  is altering the structure a little bit to make it easier to follow (below a rough suggestion). 

\begin{enumerate}
    \item MSMs are great, but has too many modelling choices, here we introduce a method to alleviate this
    \item We need MD data to create MSMs, here are the details on the MD systems
    \item Here is how you build an MSM and based on this I have chosen a fixed MSM lag time (this may be done in a different chapter)
    \item Now we are left with all of these other parameters how can we make sure this is the best choice of MSM?
    \item Lets turn to ML and see if we can use GP and BO to figure out the best parameter combination
    \item Look I came up with this amazing algorithm to do this
    \item Let’s apply this to a) di-ala b) AADH.
\end{enumerate}

Maybe  a pictorial image of what your optimisation actually does, and how you assess it, much in the same way as you describe the steps of markov modelling would be useful (I am far too big a fan of diagrams and it would be a great TOC graphic for the paper). For example to be put into the introduction to the chapter.

\emph{Core concepts}
If you can make sure core concepts are well explained and highlighted in the text. With this I mean things such as:
\begin{itemize}
    \item Response surface (I didn’t quite understand)
    \item How you make kernel choices
    \item how you select l
    \item Trial data 50:50 shuffle split etc. (essentially at the moment I don’t feel like I could reproduce exactly what you did to the data and how to run the optimisation. for the MSM part it seems a bit clearer, but that may just be because I have done these steps many times)
\end{itemize}

\emph{Nomenclature/Maths typesetting}
I think the main confusions for me were the nomenclature of variables. Even when you redefine the nomenclature, there are many differe phi, psi and ks and xs and other. I am a fan of specific examples from time to time in the text after something is definite of what the variables are in terms of your MD data for example, or what exact optimisation variable you are referring to, even if repetitive makes it easier for the reader (and this is still a Chemistry PhD I suppose). This may be a ‘spelling thing’, but there is a lot of vectors/matrices that should be bold missing. 

% Conclusions: where you state "something described in the MSM literature”, cite some relevant references. 

It will be important to discuss the relevance of your findings for future MSM investigations - summarise the implications and make some clear recommendations for how others should approach simulations and MSM construction in future. Also: do your findings reveal anything about other MSM studies in the recent literature? Important to set the work in context. This may come in Chapter 6?

\section{Introduction}

\begin{table}
    \centering
    \mycaption{Table of symbols used throughout this chapter. }
    \begin{tabularx}{0.9\textwidth}{ |l| >{\raggedright\arraybackslash}X | } 
    \hline
        \textbf{Symbol}  &  \textbf{Definition} \\
        \hline\hline
        $\mathbf{x}$ & vector of MSM hyperparameters \\
        $\chi$ & MSM hyperparameter: a protein/peptide feature e.g. backbone torsion \\
        $\tau$ & MSM hyperparameter: the TICA lag time \\
        $m$ & MSM hyperparameter: the number of TICA components retained \\
        $n$ & MSM hyperparameter: the number cluster centers. \\
        $y$ & the response of an MSM, $y =\operatorname{VAMP-2}$ \\
        $\Psi_i(\mathbf{z})$ & the right eigenvectors of an MSM \\
        $\delta_i$ & the discretisation error of the MSM eigenfunction $\Psi_{i}$ \\
        $g(\mathbf{x})$ & the objective function. In the case of MSMs this is the unknown function 
                            which maps hyperparameters to the response. \\
        $f(\mathbf{x})$ & response surface function, a statistical estimation the objective function 
                          which is easy to optimise. In the context of Bayesia optimisation called the 
                          surrogate function. \\
        $\mathcal{D}$ & a hyperparameter trial data set. A set of hyperparameter/response pairs: $(\mathbf{x}_{i}, y_{i})$. Used to estimate the response surface. \\
        $\tau(\mathrm{MSM})$ & the MSM lag time.  \\
        $\mu(\mathbf{x}$ & the mean function of a Gaussian Process. \\
        $k(\mathbf{x}, \mathbf{x}^{\prime})$ & the covariance kernel of a Gaussian Process. Defines the covariance between the response at $\mathbf{x}$ and $\mathbf{x}^{\prime}$. \\
        $\mathbf{K}$ & the covariance matrix of the Gaussian Process. $[\mathbf{K}]_{ij} =k(\mathbf{x}_{i}, \mathbf{x}_{j})$. \\
        $\theta$ & the collection of kernel hyperparameters. \\
        $\eta$ & Kernel hyperparameter: determines the scale of fluctuations of the response. \\
        $\sigma_{n}$ & Kernel hyperparameter: determines the noise associated with a single trial. \\
        $l_{i}$ & Kernel hyperparameter: the characteristic length-scale of the Gaussian Process 
                along the $i$th predictor (MSM hyper-hyperparameter).\\
        $R_{i}$ & Kernel hyperparameter: the relevance of the $i$th predictor, $R=\sfrac{1}{l}$. \\ 
        $\sigma(\mathbf{x})$ & the width of the Gaussian process at the point $\mathbf{x}$. These values  are the diagonal elements of $\mathbf{K}$. Depending on context this may or may not include the contribution from $\sigma_{n}$. \\
        $\alpha_{EI}(\mathbf{x})$ & the acquisition function used to determine the next hyperparameter trial. \\
        $N$ & the number of hyperparameter trials, or observations,  in a trial data set.\\
        $(\phi, \psi)$ & peptide feature: the backbone torsional angles of an amino acid.  \\
        $(\phi, \psi, \chi)$ & peptide feature: the backbone and residue torsional angles of an amino acid. \\
        $|\mathbf{r}_{1}-\mathbf{r}_{2}|$ & peptide feature: all heavy atom interatomic distances.  \\
        $(x, y, z)$ & peptide feature: Cartesian coordinates \\
        $C_{\alpha}-C_{\alpha}$ & peptide feature: the alpha carbon contact distances \\
        $X-X$ & peptide feature: the heavy atom contact distances \\
     \hline
     \end{tabularx}
    \label{tab:msm_symbols}
\end{table}

In chapter \ref{chap:theory} the theory of modelling biomolecular dynamics as a Markov process using a Markov state models (MSMs) was laid out. Central to the method is defining  a series of basis sets, $s^i, i=1, ..., n$, which  map atomic coordinates to $n$ discrete microstates. These basis sets allow the molecular dynamics trajectories to be represented as an approximate one-dimensional Markov chains from which the MSM can be estimated. The choice of basis set are crucial for accurately capturing the dynamics of system \cite{husicOptimizedParameterSelection2016} and must therefore be chosen with care. In section \ref{sec:theory_msm} the process of creating basis sets was described using as making four (although many more are possible) modelling choices, or  \emph{hyperparameters}, collectively denoted by $\mathbf{x} = (\chi, \tau, m, n)$. Even with reasonable ranges taken of these hyperparameters taken from \cite{husicOptimizedParameterSelection2016} the number of distinct values of $\mathbf{x}$ is computationally intractable. A systematic and efficient method is needed to choose appropriate hyperparameters. This chapter applies ideas and techniques from the machine learning literature to choose efficiently, understand and visualise the impact of hyperparameters on MSMs. 

Choosing hyperparameters can be thought of as an optimisation problem\cite{feurer2019hyperparameter}\cite{jonesEfficientGlobalOptimization1998} , where the task is to find  vectors from hyperparameter configuration space, $X=\chi \times \tau \times m \times n$  which maximise an objective function, $f$, for judging their quality, which for MSMs is VAMP score:  
\begin{equation}
    f: X \rightarrow \operatorname{VAMP}
\end{equation}
Within the MM community, efficiently finding the optimum set has recently gained attention \cite{schererVariationalSelectionFeatures2019}, however nine out of ten recent studies from a non-random sample\footnote{10 papers published in 2020 which cite PyEMMA \cite{schererPyEMMASoftwarePackage2015a} and apply MMs to understand a simulated data set.} sample performed no hyperparameter optimisation at all.  

Within the larger machine learning community however, finding the optimum set of hyperparameters for a given model and data set is a common task and has received a lot of attention, as discussed in section \ref{sec:intro_hyper_opt} of the introduction. Much of the focus has been on creating algorithms which automatically optimise hyperparameters rather than a domain expert choosing the values ``by hand''.\cite{feurer2019hyperparameter}  Automated methods of selecting hyperparameters are beneficial for a number of reasons: \cite{feurer2019hyperparameter}
\begin{enumerate}
    \item reduce the human and energy resources needed for creating an accurate MSM, 
    \item improve the performance of MSMs in general,
    \item improve reproducibility and transparency in the MSM estimation process. 
\end{enumerate}

There are two general approaches to hyperparameter optimisation: i) model-free and ii) model-based optimisation.\cite{feurer2019hyperparameter} \emph{Model-free} optimisation techniques include \cite{feurer2019hyperparameter}: grid search (or full-factorial design \cite{c1997montgomery}, i.e., placing a regular grid over the hyperparameter search space and evaluating each point), random search (i.e., randomly sampling hyperparameters from the search space)  and population techniques. The latter include evolutionary algorithms \cite{simon2013evolutionary},  particle swarm optimisation \cite{kennedyParticleSwarmOptimization1995}\cite{eberhart1998comparison} and covariance matrix adaption \cite{hansenCMAEvolutionStrategy2016}. The latter method and related techniques all perform well in benchmarking exercises \cite{dufosse2019benchmarking}\cite{faury2019benchmarking}\cite{bodner2019benchmarking} but will not be considered further. When hyperparameter optimisation is performed within the MM community, the former two methods are popular. For example, in \cite{husicOptimizedParameterSelection2016} the authors use random search to determine trends and heuristics for creating MSMs of fast folding proteins, while the authors of \cite{chenDynamicConformationalLandscape2019} used grid search over different protein features, various TICA hyperparameters, and number of microstates to optimise a MSM to describe the conformational landscape of the methyltransferase, SETD8. Random search has a number of advantages over grid search. First, when only a small proportion of the hyperparameters are relevant for determining the the model score random search has been shown \cite{bergstrajamesbergstraRandomSearchHyperParameter2012} to be more efficient than  than grid search. The reason is that grid search places equal importance on each hyperparameter and effectively wastes the computational budget on combinations of hyperparameters which will score similarly. Second, it is easily adapted to parallel computer architectures and third, increasing the optimisation `budget' (the number of optimisation steps available) or the size of the search space is easily incorporated into the workflow. \cite{feurer2019hyperparameter} 

\begin{figure}
    \centering
    \mycaption{\textbf{Response surface and acquisition functions}. Panel (a) shows the hyperparameter trial data set, $\mathcal{D}$, as black crosses, the objective function $f(x)$ (black dashed line), and the estimated response surface $\hat{f}(x)$ (solid blue line) with uncertainty (shaded blue area). The dotted line shows the incumbent, $\max(f(x))\ x\in \mathcal{D}$. Panel (b) shows corresponding acquisition functions: the probability of improvement $\mathbb{P}(I>0)$ (orange line) and the expected improvement $\mathbb{E}[I]$ (green line), for reference the improvement function, $I(x)$ (blue line) is also shown. }
    \includegraphics[width=0.8\textwidth]{chapters/msm_optimization/figures/response_surface_explainer.png}
    \label{fig:msm_rsm_explainer}
\end{figure}

\emph{Model-based} search techniques involve estimating a statistical approximation to the objective function, known  as the \emph{response surface} (or surrogate function) and using the response function to choose the next hyperparameter to evaluate \cite{hutterSequentialModelbasedOptimization2011}. The evaluated hyperparameter is then used augment the data used to estimate the response surface.\cite{hutterSequentialModelbasedOptimization2011} The alternating sequence of response surface estimation and hyperparameter evaluation is continued until a satisfactory convergence in the maximum of the response surface.\cite{hutterSequentialModelbasedOptimization2011}  A example response surface for a model with a single hyperparameter is shown in figure \ref{fig:msm_rsm_explainer}. Evaluating the fictitious model with a hyperparameter value of $x$ leads to a model score of $y$. The pair $(x, y)$ will be referred to as a \emph{hyperparameter trial}. Repeating trials with different values of $x$ gives a (hyperparameter) \emph{trial data set}  $\mathcal{D}=\{(\mathbf{x}_{i}, y_{i})\}$. In the figure $\mathcal{D}$ is shown as black crosses. The score is a random variable, $Y$, which can be modelled by: $Y \sim f(x) + \epsilon$ where $\epsilon$ is an error term. The estimated function, $\hat{f}(x)$, is the response function and is shown as a blue line, while the uncertainty (as captured by $\epsilon$) is shown as a blue shaded area. The uncertainty arises from any random element in evaluating the score (e.g., from cross-validation) or from the model itself.  

Bayesian optimisation is a popular model-based technique for optimising hyperparameters \cite{bergstraAlgorithmsHyperParameterOptimizationa}\cite{hutterSequentialModelbasedOptimization2011}\cite{NIPS2012_4522}\cite{bergstraMakingScienceModel2013}\cite{feurer2019hyperparameter}. The key components of Bayesian optimisation are: i) the \emph{acquisition function}, $\alpha$, which determines the utility of choosing a particular hyperparameter value, and ii) the response function, which encapsulates all current knowledge of the objective function.  The next \emph{candidate} hyperparameter in the optimisation sequence is chosen by maximizing the acquisition function.\cite{shahriariTakingHumanOut2016} Acquisition functions trade-off exploration of the search space with exploitation of areas more likely to optimise the objective function. Each does this in their own way, with their own particular strengths and weaknesses, but they come in three main categories i) improvement-based policies, ii) optimistic policies, and iii) information-based policies. \cite{shahriariTakingHumanOut2016}

Improvement based policies use the improvement function, $I(\mathbf{x})$, (shown in figure \ref{fig:msm_rsm_explainer} panel (b) as the blue line), which is defined as the  difference between the value of the response surface, $f(\mathbf{x})$, and incumbent, $\mu^{*}$:\cite{shahriariTakingHumanOut2016}
\begin{equation}
    I(\mathbf{x}):=\left(f(\mathbf{x}) - \mu^{*}\right) \mathbb{1}\left(f(\mathbf{x}) > \mu^{*}\right). 
\end{equation}
The incumbent\cite{brochuTutorialBayesianOptimization2010} is defined as $\mu^{*}=\max{(f(\mathbf{x}))}, \mathrm{s.t.}\ \mathbf{x} \in \mathcal{D}$, and $\mathbb{1}$ is an indicator function which ensures that $I \ge 0$. In other words, the incumbent is optimum of the response surface but restricted to values of $\mathbf{x}$ which occur in the data used to estimate the response surface. $\mu^{*}$ is shown as a dotted line in the example in figure \ref{fig:msm_rsm_explainer} panel (a). Examples of such policies include probability of improvement \cite{Kushner1963} $\alpha_{\mathrm{PI}}(\mathbf{x}) = \mathbb{P}\left[I(\mathbf{x}>0)\right]$  (orange line figure \ref{fig:msm_rsm_explainer} panel (b)) and the expected improvement \cite{mockus1978application} $\alpha_{\mathrm{EI}}(\mathbf{x}) = \mathbb{E}\left[I(\mathbf{x})\right]$ (green line in figure \ref{fig:msm_rsm_explainer} panel (b)). The probability of improvement tends to exploit known regions of the response surface and so can fail to explore sufficiently to find the global optimum \cite{jones2001taxonomy}. This is because, as can be seen from the definition, it treats all improvements the same no matter how small (although the incumbent can be altered to modify the explore/exploit trade-off).\cite{jones2001taxonomy}  The expected improvement corrects this by taking into account both the probability of improvement \emph{and} the size of improvement. The expected improvement is a popular choice \cite{feurer2019hyperparameter} and is the default option in some well-known packages such as Spearmint \cite{NIPS2012_4522} and BayesOpt \cite{martinez-cantinBayesOptBayesianOptimization2014}. 

Optimistic policies, such as the upper confidence bound \cite{icml2010_129}, choose the candidate to maximize a particular quantile of the uncertainty in the response surface. They have been shown to minimize the `regret' accumulated over all iterations of the optimisation procedure.\cite{icml2010_129} The `regret' being the difference between the true maximum of the objective function and the objective function measured on the candidate hyperparameter trial \cite{berger2013statistical}. More detail on the performance of improvement and optimistic acquisition functions can be found in \cite{jones2001taxonomy} Information based policies are based on the distribution over the potential hyperparameters, $p(\mathbf{x}|\mathcal{D})$, which describe the probability of optimising the objective function and are induced by the uncertainty in the response surface.\cite{shahriariTakingHumanOut2016} Examples include entropy search \cite{hennig2012entropy}, predictive entropy search \cite{hernandez2014predictive}, and entropy search portfolio \cite{shahriariEntropySearchPortfolio2015}. 

The second component of Bayesian optimisation is the functional form of the response function. Stochastic processes such as Gaussian processes (GPs) or T-student processes (TPs) \cite{rasmussenGaussianProcessesMachine2006} are common and are implemented in a number of packages \cite{martinez-cantinBayesOptBayesianOptimization2014}\cite{NIPS2012_4522}\cite{gpyopt2016}\cite{JMLR:v21:18-223}\cite{mcgibbonOspreyHyperparameterOptimization2016a}\cite{liuAuptimizerExtensibleOpenSource2019}. Gaussian processes in particular have many useful properties for Bayesian optimisation. \cite{feurer2019hyperparameter}\cite{brochuTutorialBayesianOptimization2010}\cite{jonesEfficientGlobalOptimization1998}  First, the improvement and optimistic acquisition functions discusses previously have simple analytic forms.\cite{brochuTutorialBayesianOptimization2010} Second, they do not specify a particular form of mean response (unlike for example, general linear models which are linear functions of their predictors \cite{nelder1972generalized}). Rather, they specify the structure of the covariance between values of the response as a function of predictors through a kernel function $k(\mathbf{x}, \mathbf{x}^{\prime})$. \cite{rasmussenGaussianProcessesMachine2006} This allows easy fitting of arbitrarily shaped response functions.  Third, with recent work in sparse estimation methods they are also able to handle large data sets.\cite{quinonero-candelaUnifyingViewSparse2005} Despite their many advantages, GPs do suffer some drawbacks and technical hurdles for hyperparameter optimisation. First, they perform poorly with large numbers of categorical hyperparameters \cite{eggensperger2013towards} compared to tree based response surface models such as tree Parzen estimators (TPEs) \cite{bergstraAlgorithmsHyperParameterOptimizationa} and random forests (RFs) \cite{hutterSequentialModelbasedOptimization2011}\cite{breiman2001}.  Second, they come with their own modeling choices which determined for the particular type of response surface.\cite{rasmussenGaussianProcessesMachine2006}  These are the choice covariance kernel, $k(\mathbf{x}, \mathbf{x}^{\prime})$ and transformations of the predictors, or input warping \cite{snoekInputWarpingBayesian2014a}. 

Understanding the effect of hyperparameters, as well as efficiently optimising them is also important and the learned parameters of GPs are particularly suited to this task.\cite{rasmussenGaussianProcessesMachine2006} For example, the authors of \cite{bergstrajamesbergstraRandomSearchHyperParameter2012} used GPs to demonstrate the important result discussed earlier, that random search is more efficient than grid search under specific but common circumstances. However, similar ideas can be used with other types of response surface. For example in  \cite{gramacyVariableSelectionSensitivity2013} and \cite{pmlr-v32-hutter14} the authors used  RFs to assess the importance of variables for optimising compiler options and machine learning models respectively. 

This chapter uses GPs to model the response surface of an MSM  and Bayesian optimisation (BO) to optimise its hyperparameters, using the benchmark system alanine dipeptide (see for example any number of MSM method papers e.g.,  \cite{wehmeyerTimelaggedAutoencodersDeep2018a}\cite{nuskeVariationalApproachMolecular2014}\cite{bowmanQuantitativeComparisonAlternative2013}). This system is well known but the free energy surface is relatively uncomplicated (relative to, say, large peptides and proteins) and the hyperparameter configuration space relatively small (only two hyperparameters are considered) which therefore limits the conclusions of this chapter. However, the main purpose of this chapter is to i) practically demonstrate and explain how to use GPs to model response surfaces, ii) comment on the interpretation of GPs in understanding the effect of MSM hyperparameters, and iii) demonstrate and comment on a simple BO method for optimising hyperparameters. This will lay the ground work necessary for tackling the more complex (and representative) system of aromatic amine dehydrogenase (AADH) in chapter \ref{chap:aadh}. The chapter is structured as follows: section \ref{sec:msm_methods} discusses the methods of GP regression modelling and Bayesian optimisation in detail, section \ref{sec:msm_results} presents and discusses the results and section \ref{sec:msm_conc} discusses conclusions and limitations of this work. 

\section{Methods}\label{sec:msm_methods}
\subsection{Molecular dynamics}

 A molecular dynamics data set  of alanine dipeptide were taken from \cite{wehmeyerTimelaggedAutoencodersDeep2018a} and are described in detail in \cite{nuskeMarkovStateModels2017b}.  It consists of $3\times \SI{250}{\nano\second}$ trajectories sampled from a constant volume, constant temperature ensemble at $T=\SI{300}{\kelvin}$ controlled using a Langevin thermostat in explicit (TIP3P \cite{jorgensen1983comparison}) water.\cite{nuskeMarkovStateModels2017b} The sampling was performed using the ACEMD \cite{harveyACEMDAcceleratingBiomolecular2009} package, with the AMBER ff-99SB-ILDN \cite{lindorff-larsenImprovedSidechainTorsion2010} force field and a \SI{2}{\femto\second} time-step.\cite{nuskeMarkovStateModels2017b} Electrostatic forces were computed using the particle-mesh Ewald (PME) \cite{dardenParticleMeshEwald1993} summation method every two time-steps with  real-space cutoff \SI{9}{\angstrom} and grid spacing \SI{9}{\angstrom}, and all bonds to hydrogen atoms were constrained. \cite{nuskeMarkovStateModels2017b} The atomic coordinates were saved every \SI{1}{\pico\second} and the three trajectories were split into $750\times\SI{1}{\nano\second}$ smaller trajectories of \num{1000} frames each. 


\subsection{MSM fitting and scoring}\label{subsec:msm_fitting}
In order to  model the response surface of an MSM, a hyperparameter trial data set  $\mathcal{D} = \left\{ (\mathbf{x}_{i}, y_{i}) \right\}$ was created. This was created by  randomly sampling hyperparameters, $\mathbf{x}$, building an MSM using $\mathbf{x}$, and then measuring the MSM response, $y$, using the VAMP-2 score. 

The hyperparameter search space of alanine dipeptide is shown in table \ref{tab:ala2searchspace}. It consists of only two hyperparameters, the peptide feature, $\chi$, and the number of cluster centres, $n$.  For each value of $\chi$ \num{100} values of $n$ were randomly sampled and scored. This number was chosen to ensure variation in the response with respect to $n$ was captured. A similar study \cite{husicOptimizedParameterSelection2016} used between \num{33} and \num{100} randomly sampled values per hyperparameter. This meant $\mathcal{D}$ contained $N=500$ hyperparameter trials.  

The response of each trial was measured by building an MSM with a lag time of $\tau(\mathrm{MSM}) = \SI{9}{\pico\second}$ and evaluated using VAMP-2 scored with the first $r=5$ eigenvalues in line with \cite{bowmanQuantitativeComparisonAlternative2013}. \num{20} iterations of 50:50 shuffle-split cross-validation, described in algorithm \ref{alg:shuffle_split}, was used when estimating the VAMP-2 score. 

The initial creation of the hyperparameter trial data set was performed separately to the Bayesian optimisation and analysis. \emph{Creation of hyperparameter trial data set}: The sampling of hyperparameters and fitting of MSMs was managed by a development version of Osprey (version 1.2.0dev) \cite{mcgibbonOspreyHyperparameterOptimization2016} adapted by the author of this thesis. The fitting and scoring of the MSMs was performed with a development version of PyEMMA (version 2.4) \cite{schererPyEMMASoftwarePackage2015a} adapted by the author of this thesis to be compatible with Osprey. Python version 3.5 was used throughout. \emph{Bayesian optimisation and analysis}:
All MSM fitting and MD trajectory analysis was performed in Python 3.7 using the packages PyEMMA (version 2.5) \cite{schererPyEMMASoftwarePackage2015a}, MDTraj (version 1.9) \cite{mcgibbonMDTrajModernOpen2015}, NumPy (version 1.19) \cite{waltNumPyArrayStructure2011}, Pandas (version 0.23) \cite{mckinneyPandasFoundationalPython2011}, Matplotlib (version 3.3) \cite{hunterMatplotlib2DGraphics2007},  Seaborn (version 0.10) \cite{michaelwaskomMwaskomSeabornV02020} and the Jupyter Project (version 4.6) \cite{kluyverJupyterNotebooksPublishing2016}.

\begin{table}
    \mycaption{\textbf{The hyperparameter search space of alanine dipeptide}. Prior to feature selection the Cartesian coordinates of the MD trajectories were first aligned to a single, randomly chosen, trajectory frame so that features (2) and (5) did not include spurious rotational or translational motion.  The number of dimensions, `Dim.', refers to the number of individual feature variables created by $\chi$.}
    \centering
    \begin{tabularx}{0.9\textwidth}{ |>{\raggedright\arraybackslash}X|l|l|c| >{\raggedright\arraybackslash}X | } 
    \hline
    \textbf{Hyper-parameter} & \textbf{Type} & \textbf{Range} &\textbf{Dim.} & \textbf{Details} \\
     \hline\hline
    Feature, $\chi$ & Categorical & (1) $(\phi, \psi)$ & $2$ & Torsions \\
    & & (2) (x, y, z) & $30$ & Heavy atoms only  \\
    & & (3) $\phi$ & $1$ & Torsion \\ 
    & & (4) $\psi$ & $1$ & Torsion \\ 
    & & (5) RMSD & $1$ & Heavy atoms only\\ 
    \hline 
    Cluster centres, $n$ & Integer & \numlist[list-final-separator = { ... }]{10;11;1000} & & Clustered using k-means clustering \\
     \hline
    \end{tabularx}
    \label{tab:ala2searchspace}
\end{table}

\subsection{Gaussian process regression}\label{subsec:gp}

% 

% A Gaussian process (GP) is specified by a mean function, $\mu(\mathbf{x})$ and a covariance function, $k(\mathbf{x}, \mathbf{x}^{\prime})$, where $\mathbf{x} = (\chi, \tau, m, n)$ are the hyperparameters and $f=f(\mathbf{x})$ is the MSM response. These are related in the following way: \cite{rasmussenGaussianProcessesMachine2006}

% \begin{equation}
%     \begin{aligned}
%     \mu\left(\mathbf{x}\right) &=\mathbb{E}\left[f\left(\mathbf{x}\right)\right] \\
%     k\left(\mathbf{x}, \mathbf{x}^{\prime}\right) &=\mathbb{E}\left[(f(\mathbf{x})-\mu(\mathbf{x}))\left(f\left(\mathbf{x}^{\prime}\right)-\mu\left(\mathbf{x}^{\prime}\right)\right)\right]
%     \end{aligned}
% \end{equation}
A Gaussian process is a distribution over functions \cite{rasmussenGaussianProcessesMachine2006}.  In other words, drawing a sample from a GP gives a continuous function $f(\mathbf{x})$, where $\mathbf{x}$ is a (potentially multidimensional) input. Considering this function at a set of discrete points, $\mathbf{x}_{1}, \mathbf{x}_{1},\ldots, \mathbf{x}_{N}$, will give a set of random variables, which together form a multivariate normal distribution: 

\begin{equation}
\begin{bmatrix}  f\left(\mathbf{x}_{1}\right) \\ \vdots \\ f\left(\mathbf{x}_{N}\right) \end{bmatrix} 
\sim 
\mathcal{N}\left( 
\begin{bmatrix} \mu\left(\mathbf{x}_{1}\right) \\  \vdots \\ \mu\left(\mathbf{x}_{N}\right) \end{bmatrix}, 
\begin{bmatrix}
k(\mathbf{x}_{1}, \mathbf{x}_{1}) & \cdots & k(\mathbf{x}_{1}, \mathbf{x}_{N}) \\
\vdots & \ddots & \vdots \\
k(\mathbf{x}_{N}, \mathbf{x}_{1}) & \cdots & k(\mathbf{x}_{N}, \mathbf{x}_{N}) \\
\end{bmatrix}
\right)
\end{equation}\label{eqn:msm_mvn}

As can be seen from equation \ref{eqn:msm_mvn} the GP is specified by the mean function, $\mu(\mathbf{x})$, and a covariance function or \emph{kernel} $k(\mathbf{x}, \mathbf{x}^{\prime})$. This can be written succinctly as $\mathbf{f} \sim \mathcal{N}(\bm{\mu}, \mathbf{K})$. \cite{rasmussenGaussianProcessesMachine2006} If the GP ($f$) cannot be observed directly but is instead subject to some random additive noise then the observed quantity $y$ is related to $f$ by $y = f(\mathbf{x}) + \epsilon$, alternatively $\mathbf{y} \sim \mathcal{N}(\mathbf{f}, \sigma^{2}\mathbf{I})$, where $\mathbf{I}$ is the identity matrix and $\sigma^{2}$ is the variance of the noise. \cite{rasmussenGaussianProcessesMachine2006} The observed response of an MSM can be equated with $y$ with the noise term due the randomness in the cross-validation procedure used to estimate the score. 

So far a GP is just a distribution of functions, $p(\mathbf{f})$, independent of data. In order to make draws from this distribution the mean function and kernel must be specified. To center this discussion consider the Gaussian kernel: 
\begin{equation}
    k(\mathbf{x}, \mathbf{x}^{\prime}) = \exp{\left(-\frac{\left|\mathbf{x}-\mathbf{x}^{\prime}\right|^{2}}{l^{2}}\right)}
\end{equation}\label{eqn:msm_rbf}
This states that values of the GP will be highly correlated between values of $\mathbf{x}$ that are `similar' and less so when they are separated. The value of $l$ is the characteristic length-scale of the GP and determines how close values of $\mathbf{x}$ must be to be considered `similar'.\cite{rasmussenGaussianProcessesMachine2006} In other words, it determines how rapidly the function changes for changes in $\mathbf{x}$. An example GP prior might be $\mu(\mathbf{x})=0$ for all $\mathbf{x}$ and $k(\mathbf{x}, \mathbf{x}^{\prime})$ given by equation \ref{eqn:msm_rbf} with $l=2$.  

To incorporate the training data, in this case the observed hyperparameter trials $\mathcal{D}=\left\{(\mathbf{x}_{i}, y_{i})\right\} = (\mathbf{y}, \mathbf{X})$, Bayes' rule is used: \cite{rasmussenGaussianProcessesMachine2006}
\begin{equation}\label{eqn:ya_boy_bayes}
    p(\mathbf{f}|\mathbf{y}, \mathbf{X})  = \frac{p(\mathbf{y}|\mathbf{f}, \mathbf{X})p(\mathbf{f})}{p(\mathbf{y}|\mathbf{x})}. 
\end{equation}
The posterior distribution $p(\mathbf{f}|\mathbf{y}, \mathbf{X})$ is the distribution of functions \emph{given the training data}.\cite{gelmanBayesianDataAnalysis2014} Draws from this distribution will now (hopefully) resemble the data. The posterior takes into account the training data  through the likelihood function $p(\mathbf{y}|\mathbf{f}, \mathbf{X}) = \mathcal{N}(\mathbf{f}, \sigma^{2}\mathbf{I})$ which is the probability of observing $\mathbf{y}$ given a specific function $\mathbf{f}$ and the predictors $\mathbf{X}$. \cite{gelmanBayesianDataAnalysis2014} For GPs the posterior distribution is also a GP, with mean and covariance functions related to the prior mean and covariance functions and the training data.\cite{rasmussenGaussianProcessesMachine2006} Let $\bar{f}_{*}$ be the mean of the posterior GP at some arbitrary point $\mathbf{x}_{*}$ (which may or may not have been in the training data), and let $\mathbb{V}\left[f\right]$ be the covariance between value of the GP at this point and all other points in the training data then: \cite{rasmussenGaussianProcessesMachine2006} 
\begin{equation}\label{eqn:gp_pred_dist}
\begin{aligned}
\bar{f}_{*} &=\mathbf{k}_{*}^{\top}\left(\mathbf{K}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{y} \\
\mathbb{V}\left[f_{*}\right] &=k\left(\mathbf{x}_{*}, \mathbf{x}_{*}\right)-\mathbf{k}_{*}^{\top}\left(\mathbf{K}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{k}_{*}.
\end{aligned}
\end{equation}
Here $\mathbf{k}_{*}$ is a vector of covariances between $\mathbf{x}_{*}$ and the training observations, $\mathbf{X}$. As the point $\mathbf{x}_{*}$ is arbitrary, these equations define the posterior GP over the entire domain of $f$. 

Equations \ref{gp_pred_dist} only determine how the posterior GP should be defined in terms of the training data and the hyperparameters\footnote{not to be confused with the hyperparameters of the MSM which are the predictors of the GP} of the GP prior, i.e., $l$ in equation \ref{eqn:msm_rbf}.  In general kernels may be complicated with many hyperparameters and so deciding what the particular values of $l$ may be prone to error. Instead they may also be estimated from the data  by maximizing the \emph{log marginal likelihood}:\cite{rasmussenGaussianProcessesMachine2006}

\begin{equation}\label{eqn:marg_llike}
\log p(\mathbf{y} | \mathbf{X})=-\frac{1}{2} \mathbf{y}^{\top}\left(\mathbf{K}+\sigma^{2} \mathbf{I}\right)^{-1} \mathbf{y}-\frac{1}{2} \log \left|\mathbf{K}+\sigma^{2} \mathbf{I}\right|-\frac{N}{2} \log 2 \pi
\end{equation}

The values of the kernel hyperparameters estimated this way are known as Maximum a posteriori (MAP) estimates.\cite{rasmussenGaussianProcessesMachine2006} When estimates of the variability of $l$ are required Bayesian estimation can be used.\cite{gelmanBayesianDataAnalysis2014} i.e., a prior is placed over $l$ and Markov Chain Monte Carlo (MCMC) used to sample the posterior distribution of $l$. The same considerations for Bayesian MSMs in section \ref{sec:theory_bayes} apply in this case. In fact, priors can also be place over $l$ when maximizing \ref{eqn:marg_llike} as well.\cite{rasmussenGaussianProcessesMachine2006}

The process of fitting a GP to data can be summarised as follows. Specify i) a prior mean function, ii) a functional form of covariance kernel $k(\mathbf{x}, \mathbf{x}^{\prime})$, e.g. equation \ref{eqn:msm_rbf}, iii) priors over the covariance hyperparameters, $l$, and then iv) collect training data $\mathcal{D}$. Using these four components maximize the marginal likelihood, equation \ref{eqn:marg_llike}, to determine the optimal values of $l$, and use equations \ref{eqn:gp_pred_dist} to specify the GP for any value of $\mathbf{x}$. 

Having fitted a GP to the training data a method for evaluating that fit is needed. For models fit by maximizing the marginal likelihood the predictive value of the GP can be measured through the standardized mean square error (SMSE) and the mean standardized log loss (MSLL).\cite{rasmussenGaussianProcessesMachine2006} These play the same r\hat{o}le as, for example, the $R^2$ or deviance play in generalized linear models.\cite{nelder1972generalized} The SMSE is defined by:\cite{rasmussenGaussianProcessesMachine2006} 
\begin{equation}\label{eqn:smse}
SMSE =\left(\frac{1}{N}\right) \sum_{i=1}^{N} \frac{\left(f(x)-y_{i}\right)^{2}}{\sigma_{obs}^{2}},
\end{equation}
and the mean standardized log loss (MSLL) by:\cite{rasmussenGaussianProcessesMachine2006}
\begin{equation}\label{eqn:msll}
MSLL=\left(\frac{1}{N}\right) \sum_{i=1}^{N}\left[\left(\frac{1}{2} \log \left(2 \pi \sigma_{i}\right)+\frac{\left(f(x)-y_{i}\right)^{2}}{\sigma_{i}^{2}}\right)-\left(\frac{1}{2} \log \left(2 \pi \sigma_{obs}\right)+\frac{\left(\bar{y}-y_{i}\right)^{2}}{\sigma_{obs}^{2}}\right)\right].
\end{equation}  

Here $\sigma_{obs}^{2}$ is the observed variance of the training data response, $\mathbb{E}\left[(y_{i}-\bar{y})^{2}\right]$, and $\sigma_{i}^{2}$ refers to the GP predicted variance at the observation $i$ (including the noise term). The standardization puts the log loss and mean square error on the same scale as the observations. So a $SMSE < 1$ indicates the predictions have smaller mean square error than replacing all the predictions with the mean of the observations. Similarly, $MSLL<0$ indicates the predictions have a larger log predictive density than replacing all of the predictions with a single Gaussian with mean and variance equal to the mean and variance of the observations.   

\subsection{Response surface modelling}\label{subsec:rsm}
The response surface of both systems was modelled using Gaussian process (GP) regression model with the $\operatorname{VAMP-2}$ score measured on the test data as a response ($y$) and the MSM hyperparameters as predictors ($\mathbf{x})$. A variety of combinations of predictor pre-processing and covariance structures were tried and the best combinations for each response surface determined using the SMSE and MSLL. 

To reduce the computational effort required to fit the GPR models, a sparse approximation to the full covariance of the GP, called the Fully Independent Training Conditional (FITC)\cite{quinonero-candelaUnifyingViewSparse2005}, was used. In this approximation a number of observations must be designated as inducing points. This number was set to $\SI{10}{\percent}$ of the total number of observations and their location determined by k-means clustering. This fraction was chosen by fitting a GPR with the number of inducing points ranging from $\SI{10}{\percent}$ to $\SI{100}{\percent}$ of the total observations. The quartiles of the posterior for each variable of the model were found to be independent of the number of inducing points in this range.  

In order to fit a GP model a number of modelling choices need to specified, these are: 
\begin{enumerate}
    \item the prior of the mean function, $\mu(\mathbf{x})$;
    \item the kernel function, $k(\mathbf{x}, \mathbf{x}^{\prime})$;
    \item the prior distributions of kernel hyperparameters;
    \item the transformations of the predictors.
\end{enumerate}

The prior of mean function  was set to zero everywhere: $\mu(\mathbf{x})=\mathbf{0}$. The kernel function was restricted to stationary kernels, i.e. where $k(\mathbf{x}, \mathbf{x}^{\prime}) = k(|\mathbf{x} - \mathbf{x}^{\prime}|)$, with the form: 

\begin{equation}\label{eqn:kernel_form}
    k(|\mathbf{x}-\mathbf{x}^{\prime}|; \theta) = 
    \eta^{2}\prod_i k_{M}(|x_{i}-x_{i}^{\prime}|; \nu, l_i)
    +\sigma_{n}^{2}\delta_{\mathbf{x}, \mathbf{x}^{\prime}}
\end{equation}

The index of the product runs over the predictors of the GP with separate kernel functions ($k_{M}$) over each predictor, however, for a given model, the value of $\nu$ for each predictor was the same. The values of $x$ of each product term refers to a single component of $\mathbf{x}$ and so the bold font has been dropped. The multiplicative form of this kernel means that the response is correlated when the values of the predictors are simultaneously similar, with the degree of similarity required set by the length-scale parameters $l_{i}$.  The $\eta$ terms controls the total variation in the response function, the larger the value of $\eta$ the more the response is able vary over the whole predictor space. The $\sigma_{n}^{2}$ term is the noise term (the variance of the response with identical predictor values) which allows the GP to account for variation in the response due to the cross validation procedure and the finite amount of MD data used to fit the MSM. 

The kernels over the individual predictors, $k_{M}(\cdot; \nu, l)$, were kernels in the Mat\'{e}rn class with values of $\nu=\sfrac{1}{2},\sfrac{3}{2},\sfrac{5}{2},\infty$. These are alternatively known as an exponential,Mat\'{e}rn 3-2, Mat\'{e}rn 5-2 and  Gaussian (Radial Basis Function, RBF) kernels respectively and are defined as follows: 

\begin{align}
k_{\text{Exp}}\left(x, x^{\prime}; \sfrac{1}{2}\right) &=\exp (-r) \label{eqn:kern_exp}\\
k_{\text{M-\sfrac{3}{2}}}\left(x, x^{\prime}; \sfrac{3}{2}\right) &= \exp (-\sqrt{3} r)(1+\sqrt{3} r) \label{eqn:kern_m32} \\
k_{\text{M-\sfrac{5}{2}}}\left(x, x^{\prime}; \sfrac{5}{2}\right) &= \exp (-\sqrt{5} r)\left(1+\sqrt{5} r+\frac{5}{3} r^{2}\right) \label{eqn:kern_m52}\\
k_{\text{RBF}}\left(x, x^{\prime}; \infty\right) &= \exp \left(-\frac{1}{2} r^{2}\right) \label{eqn:kern_gauss}
\end{align}

where $r = |x-x^{\prime}|/l$. These kernels were chosen based on their common usage \cite{shahriariTakingHumanOut2016} and  range from `rough' exponential kernel, where correlation in the response drops off rapidly with changes in the predictors, to smooth processes with the Gaussian kernel. See chapter 5 of \cite{rasmussenGaussianProcessesMachine2006} for a full description of the Mat\'{e}rn kernels and their properties.  

\begin{figure}
    \centering
    \mycaption{[TM: explain (c) more] Prior functions used specification of the GP models of the response surface for both alanine dipeptide and AADH. The variance parameters, $\eta$, $\sigma_n$ use the half-Cauchy with $\beta=2$, panel (a). The length-scale parameters $l_{i}$ use the Gamma distribution with $\alpha=1, \beta=0.05$, panel (b). 10 draws from a Gaussian process using an RBF kernel with kernel hyperparameters $l$ and $\eta$ drawn from these prior distributions is shown in panel (c).}
    \includegraphics[width=0.8\textwidth]{chapters/msm_optimization/figures/prior_functions.png}
    \label{fig:priors}
\end{figure}

When modelling the response surface for visualisation and Bayesian optimisation, Maximum a posteriori (MAP) estimates  of the kernel hyperparameters, $\theta = (\eta, \sigma_{n}, l_{1}, l_{2}, ...)$, with weakly informative priors were used. The prior distributions for the variance terms, $\eta$ and $\sigma_{n}$, were $\mathrm{half-Cauchy}(\beta=2)$ and the priors for the length-scale parameters, $l_{i}$, were $\mathrm{Gamma}(\alpha=1, \beta=0.05)$, these distributions are shown in Figure \ref{fig:priors} panels (a) and (b) respectively.  Examples of a 1D Gaussian process with a Gaussian covariance kernel with $\eta$ and $l$ drawn from these priors are shown Figure \ref{fig:priors} panel (c).  

The r\^ole of weakly informative priors is to exclude unrealistic or disallowed values of the parameters without imposing strong prior beliefs on the outcome. The half-Cauchy distribution  was used for $\eta$ and $\sigma_n$  based on its recommended use in other settings (\cite{polsonHalfCauchyPriorGlobal2012}). It was  only necessary for the scale of this distribution to give significant density in the range $0-5$ as the $\operatorname{VAMP-2}(k)$ score will lie in the range $[1,5]$ for alanine dipeptide and $[1, 4]$ for AADH which limits the possible values of $\eta$ and $\sigma_{n}$. The prior for $l$ was justified on the basis that, after scaling the predictors to lie in $[0, 1]$, values of $l \gg 1$ imply a flat response, meaning significant density for $l \gg 1$ isn't necessary. 

All predictors were scaled to lie in the range $[0, 1]$ to aid interpretation of the kernel length-scale parameters, $l$. Before scaling the integer predictors ($\tau$, $m$ and $n$) two input warpings, $T(\cdot)$, were considered: the identity $I(\cdot)$, and a logarithmic transformation, $\log(\cdot)$.  Input warpings are used \cite{snoekInputWarpingBayesian2014a} to mitigate the problems of modelling non-stationary functions using stationary GP models [TM: explain]. The categorical predictor, $\chi$, was dummy coded \cite{dalyDummyCodingVs2016} to give a $d$ dimensional vector of $1$s and $0$s. For example, the input vector, $\mathbf{x}$, for the first feature would be $\mathbf{x} = (1, 0, 0, 0, 0)$, for the second feature would be $\mathbf{x} = (0, 1, 0, 0, 0)$ and so on [TM: not clear]. 

In order to find the best GPR model for a given response surface, every combination of kernel function and  predictor warping was estimated using the MAP estimates for the kernel hyperparameters and evaluated using the MSLL and SMSE metrics. This meant that for the response surface of alanine dipeptide eight different models were estimated (four different kernels and two warpings for $n$). For the response surface of AADH, with three integer predictors, $32$ different models were estimated (four different kernels and $2\times2\times2=8$ different predictor warpings). For each model both metrics were calculated used 10-fold cross validation and any model with $MSLL > 0$ or $SMSE > 1$ was discarded. The remaining models were ranked separately according to MSLL and SMSE ($R_{MSLL}$, $R_{SMSE}$) and the ranks combined according to $\sqrt{R_{MSLL}^2 + R_{SMSE}^2}$. This ranking method was used to ensure a balance between the two selection metrics compared to, say, the mean of the two ranks.  

All GPR modelling was performed with the Python package PyMC3 \cite{salvatierProbabilisticProgrammingPython2016} with some visualisation performed using package GPy \cite{gpy2014}. 

\subsection{Hyper-parameter relevance}\label{subsec:meth_rel}
The characteristic length-scales in equation \ref{eqn:kernel_form}, $l$, each correspond to a different predictor, or level of categorical predictor. They determine the covariance of the response between points with different values of that predictor. For example, with $l=1$ in an exponential kernel then inputs separated by $|x-x^{\prime}|= 1$ will on average have a covariance of $\exp^{-0.1}\simeq 0.9$. This means for large values of $l$ the response with respect to changes in $x$ will be flat, or in other words, $x$ is irrelevant to determining the response. This prompts the definition of \emph{relevance}, $R = \sfrac{1}{l}$: when $R$ is large, the small changes in $x$ result in larger changes in the response, meaning it is relevant to determining the response. Hereafter the kernel functions (equations \ref{eqn:kern_exp} - \ref{eqn:kern_m52}) will be parameterized interchangeably with $R$ and $l$ as convenient.  

The relevance of the MSM hyperparameters is important for understanding and visualising the response surface and so to calculate the uncertainty in $R$ a fully Bayesian approach was used. After model selection using the maximum marginal likelihood models described in section \ref{subsec:rsm}, the GP model hyperparameters were re-estimated by sampling the posterior distribution using Markov Chain Monte Carlo. A No U-Turn sampling algorithm, using two independent chains with $500$ tuning steps and $1000$ sampling steps. Convergence was checked using the R-hat statistic \cite{gelmanBayesianDataAnalysis2014}. 


\subsection{Bayesian Optimization}\label{subsec:bayes_opt}
Bayesian optimization is a method for optimizing ``black-box'' objective functions - those which can be sampled but are otherwise unknown \cite{shahriariTakingHumanOut2016}. The response of an MSM to its hyperparameters is a black-box objective function because it is not possible to determine the response without first fitting the MSM. Bayesian optimisation is a type of sequential model based \cite{hutterSequentialModelbasedOptimization2011} optimisation technique. These techniques entail first sampling the response of the objective function over a search space to create a trial data set. A statistical model called the \emph{surrogate function}, which serves as a proxy for the objective function, is built using the sampled responses. An utility function, known as the \emph{acquisition function}, determines the anticipated utility of a set of inputs in optimizing the objective function, based on the predictions and uncertainty in the surrogate function.  The objective function is then sampled using the set of inputs which maximize the acquisition function. This new trial is added to trial data set and process repeated. This algorithm (with modifications discussed below) is summarised in algorithm \ref{alg:bayes_opt}. 

\begin{algorithm}\label{alg:bayes_opt}
\KwData{Trial data: $\mathcal{D}_{N} = \{(y_{1}, \mathbf{x}_{1}), ...,(y_{N}, \mathbf{x}_{N}) \}$}
\KwData{Search space grid: $\mathbf{X} = \{(\chi_1, \tau_1, m_1, n_1), ...,(\chi_{M}, \tau_{M}, m_{M}, n_{M})\}$}
\KwResult{$\mathbf{x}^{*} = \argmax_{\mathbf{x}}{f(\mathbf{x}; \mathcal{D}_{N+p})}$}
\BlankLine
\For{$i\leftarrow N$ \KwTo $N+p$}{
    estimate GP surrogate $f(\mathbf{x}; \mathcal{D}_{i})$\;
    calculate incumbent: $\mu^{*} = \argmax{f(\mathbf{x};\mathcal{D}_{i})}\ \mathrm{s.t.}\ (y, \mathbf{x}) \in \mathcal{D}_{i}$\;
    estimate acquisition function: $\alpha_{\mathrm{EI}}(\mathbf{x}; \mathcal{D}_{i})\ \mathbf{x} \in \mathbf{X}$\;
    select candidate: $\mathbf{x}_{i+1} = \argmax_{\mathbf{x}}\alpha_{\mathrm{EI}}(\mathbf{x}; \mathcal{D}_{i})\ \mathrm{s.t.}\ (\mathbf{x} \in \mathbf{X})\ \&\ (\mathbf{x} \notin \mathcal{D}_{i})$\;
    query objective function to obtain: $y_{i+1}$\;
    augment data: $\mathcal{D}_{i+1} \leftarrow \{\mathcal{D}_{i}, (y_{i+1}, \mathbf{x}_{i+1})\}$
}
\caption{Bayesian Optimisation.}
\end{algorithm}

In principle, any function capable of predicting a response along with its uncertainty can be used in Bayesian optimisation, examples include random forests [] and Tree Parzen estimator models []. However Gaussian process models are commonly used [][][], especially for hyperparameter optimisation problems\cite{bergstraAlgorithmsHyperParameterOptimizationa}\cite{martinez-cantinBayesOptBayesianOptimization2014}, due to their flexibility in modelling non-linearities; their natural incorporation of uncertainty due to data sparsity; the resulting analytical expressions for acquisition functions, and the fact that they are closed under sampling \cite{NIPS2012_4522}\cite{brochuTutorialBayesianOptimization2010} (this last fact essentially asserts that updating a GP with additional observations the model remains a GP). 


This work uses the the expected improvement, $\mathbb{E}[I]$ where the improvement, $I$, is defined :  
\begin{equation}
    I(\mathbf{x}, f, \mu^{*}):=(f(\mathbf{x}) - \mu^{*}) \mathbb{I}(f(\mathbf{x}) > \mu^{*})
\end{equation}

Here the $\mathbb{I}$ is an indicator function which prevents the improvement from being negative and the incumbent is defined as the maximum of the response surface \emph{at the trial values}. The expected improvement is the expectation of $I$, averaged over the distribution of $f(\mathbf{x})$ which can be calculated analytically for maximum marginal likelihood GPs as: 

\begin{align}
        \alpha_{EI}(\mathbf{x}) := &  \mathbb{E}\left[I(\mathbf{x}, f(\mathbf{x}), \mu^{*})\right] \\
         =  &(\mu(\mathbf{x}) - \mu^{*})\Phi\left( \frac{ \mu(\mathbf{x}) - \mu^{*} }{\sigma(\mathbf{x})} \right ) + \sigma(\mathbf{x})\phi\left( \frac{ \mu(\mathbf{x}) - \mu^{*} }{\sigma(\mathbf{x} } \right )
\end{align}

Here $\Phi, \phi$ are the normal cumulative and probability distribution functions respectively, and $\sigma(\mathbf{x})$ is the variance of the GP at the point $\mathbf{x}$. It is possible to take the expectation over both the distribution of $f$ and of the GP hyperparameters $\theta$ and this has been suggested and shown to be effective \cite{NIPS2012_4522}). However this was not done in this work because the extra computational cost involved. The candidate MSM hyperparameters were determined as those which had the highest expected improvement as calculated on a $4 \times 100 \times 20 \times 100$ ($\chi \times \tau \times m \times n$) evenly spaced grid over the search space.

It was observed during these experiments that the same candidate hyperparameters were being proposed by the algorithm. This was deemed due to the granularity of the grid used in the maximisation of the acquisition function. To ensure that each candidate hyperparameter is unique, the Bayesian optimisation algorithm was modified so that only  hyperparameters sets not already in the trial data set were considered as candidates. This is reflected in the conditions on the `select candidate' step of algorithm \ref{alg:bayes_opt}.

Although not explicitly discussed in the theoretical literature, software packages seed the process with randomly selected hyperparameter trials so that the initial response surface contains some information, rather than just a random draw from the prior function distribution. The default number of trials in Bayesian optimisation software packages lie in the range x - y. Conventional advice \cite{harrelRegressionModelingStrategies2015} for parametric models (e.g. multi-variable linear regression) puts the required number of observations for estimating parameters as at least $15$ observations per parameter. An appropriate number was partially explored in this work. 

\subsection{MSM Optimisation}
Bayesian optimisation was used to determine whether the maximum of the response surface estimated from the trial data set could be improved and whether this maximum could be reached with smaller trial data set.

An exploratory optimisation procedure was first performed on the alanine dipeptide system to demonstrate its effectiveness and to determine how many randomly sampled seed trials are needed to initalise the BO procedure, $N_{seed}$. $p=10$ steps of the BO algorithm was performed on five random subsets of the alanine dipeptide trial data set with $N_{seed}=30, 50$ trials (stratified over the features $\chi$), corresponding to $15, 25$ trials per predictor. As discussed further in section \ref{sec:ala_opt}, $25$ trials per predictor was found to result in satisfactory improvement in the incumbent.  

For AADH two optimisation experiments were performed:
\begin{enumerate}
    \item $p=50$ steps of Bayesian optimisation was performed using a response surface seeded with all of the randomly sampled hyperparameter trial data ($N_{seed}=361$);
    \item $p=50$ steps of Bayesian optimisation was performed on five subsets  of the hyperparameter trial data with  $N_{seed}=100$, which corresponded to $25$ trials per predictor.
\end{enumerate} 

In both cases the combination of input warping and kernel-type  were determined by selecting the highest ranked model using the MSLL and SMSE metrics (equations \ref{eqn:msll} and \ref{eqn:smse}). The of the kernel hyperparameters were determined by maximizing the marginal log-likelihood, equation \ref{eqn:marg_llike}. 

\section{Results and discussion}\label{sec:msm_results}
% \subsection{Alanine Dipeptide}\label{subsec:ala1}
\subsection{Response surface}\label{sec:ala_rsm}

\begin{figure}[p]
    \centering
    \mycaption{The $\operatorname{VAMP-2}$ scores of the hyperparameter trials for MSMs of alanine dipeptide. The test response, $f^{test} = f(\chi, n; \mathbf{X}^{test})$ is shown in blue, panels: a, c, e, g, i,  while the degree of over-fitting, $f^{train} - f^{test}$, is shown in orange, panels: b, d, f, h, j. Each row represents a different value of the feature ($\chi$) and the horizontal axis represent the number of clusters, ($n$). Each trial was scored with $20$ iterations of 50:50 shuffle split cross validation. The error bars represent the $25$th and $75$th quantiles of the cross-validation folds. 
    The features are ordered according to the mean of the their test scores.}
    \includegraphics[height=0.8\textheight]{chapters/msm_optimization/figures/ala1_train_test_results.png}
    \label{fig:ala1_train_test}
\end{figure}

The average MSM response for each trial are shown in figure \ref{fig:ala1_train_test}. The test response ($f^{test} = f(\chi, n; X^{test})$, blue points) and the difference between train and test response, ($\Delta f = f^{train} - f^{test}$, orange) are shown as a functions of $n$. The features are ordered according to the  mean of the test response. As expected [][] the  $(\phi, \psi)$ feature has the highest average response, but unexpectedly, the heavy atom $(x,y,z)$ coordinates feature performs just as well. 

The difference between the train and test response, the \emph{over-fitting} reflects the consistency between the eigenvectors estimated on the training data and those implied from the time-lagged covariance and overlap matrices ($C$ and $\Pi$  in \ref{eqn:tran_def}) estimated  on the test data. So a small $\Delta f$ implies that the picture of the relaxation processes are represented equally well, with the given hyperparameters, in the training and test data (even if they're both inaccurate pictures). This will arise, as is likely in this case, with large volumes of data.    

The response surface (figure \ref{fig:ala1_response}) was modelled as a Gaussian process with $\chi$ and $n$ as predictors. A Mat\'{e}rn 5-2 kernel and logarithmic input warping of $n$ were chosen using a combination of the MSLL and SMSE model selection criteria (see table \ref{tab:ala2_fit_results} for the full model selection results). The choice of logarithmic warping of $n$ is unsurprising given that the response for the $(\phi, \psi)$ and $(x,y,z)$ features (panels (a) and (c) in figure \ref{fig:ala1_train_test}) is a clearly non-stationary process: the covariance of the response with respect to changes in $n$ is much lower for $n\leq 100$ than for $n\geq 100$ where the response reaches a plateau. The log transformation smooths the response with respect to $n$ and makes the assumption of a stationarity more plausible.  This response surface fits the observed data well, both in terms of the mean response and its uncertainty. There are a number of features of the response surface which are worth discussing. 

\begin{figure}
    \centering
    \mycaption{The response surface of alanine dipeptide as a function of the feature, $\chi$ (panels a - e) and number of clusters, $n$ (horizontal axis). The features are ordered according to their average response. A Mat\'{e}rn 5-2 kernel and logarithmic warping of the predictor $n$  was used. The blue line is the mean of the surface, the blue shaded bands represent the uncertainty ($\pm2\sigma$ excluding the noise term $\sigma_{n}$, and the black crosses are the observed values (the cross validated mean of VAMP-2). Note the differing vertical axis scales. This was to allow the curvature of the response surface to be visible.}
    \includegraphics[width=0.8\textwidth]{chapters/msm_optimization/figures/ala1_response_surface.png}
    \label{fig:ala1_response}
\end{figure}

First, there is a decrease in response as $n \rightarrow 10$ for the $(\phi, \psi)$ and $(x,y,z)$ coordinate features but not the remaining features (although this is due to the sparse sampling for $n<20$ and no sampling for $n<10$). This is expected from previous studies \cite{wuVariationalApproachLearning2019}\cite{mcgibbonVariationalCrossvalidationSlow2015} and is due to decreasing eigenfunction discretization error, $\delta$  \cite{prinzMarkovModelsMolecular2011} as a $n$ increases. In the language of statistical learning theory \cite{friedman2001elements}, this is the high ``Bias'' regime of the ``Bias-variance'' trade-off. The discretizaton error for the $i$'th normalized eigenfunction of an MSM $\Psi_{i}$ is given by [TM: needs more explanation]: 

\begin{equation}
    \delta_{i} \equiv\left\|\Psi_{i}\left(\mathbf{z})-\hat{\Psi}_{i}(\mathbf{z}\right)\right\|_{\pi, 2}=\left(\int_{\Omega} d \mathbf{z} \pi(\mathbf{z})(\Psi_{i}(\mathbf{z})-\hat{\Psi}_{i}(\mathbf{z}))^{2}\right)^{1 / 2}
\end{equation}
\begin{figure}
    \centering
    \mycaption{The discretization error of the dominant right eigenfunction of alanine dipeptide as a function of the number of cluster centers. The feature used is the $\phi$ backbone torsion. Panel (a) shows the free energy along this feature. Panels (b), (c) and (d) show the normalized MSM right eigenfunction (blue line) estimated $n=2, 10$ and $50$ cluster centers respectively. This is compared with the same eigenfunction estimated with $n=500$ cluster centers (black line). The red shaded area represents the difference between the two eigenfunctions. The discretization error, labelled $\delta$ is the integral of the red area. The $VAMP-2$ score is also labelled for comparison.}
    \label{fig:ala1_evcompare}
    \includegraphics[width=0.8\textwidth]{chapters/msm_optimization/figures/ala1_ev_n_compare.png}
\end{figure}

Here $\mathbf{z}$ are the coordinates of state space (e.g. Cartesian coordinates), $\pi$ is the stationary distribution and the integral runs over all of the state space, $\Omega$. The integrand is the difference between the true normalized eigenfunction $\Psi$ and approximate eigenfunction $\hat{\Psi}$. 

To get a sense of how the response and the discrietisation error are related, approximations to dominant eigenfunction, $\hat{\Psi}_{2}$, for the $\phi$ feature, are plotted in figure \ref{fig:ala1_evcompare}. For reference, panel (a) shows the stationary distribution. The truncation around the values of $\phi \simeq 0, 2$ is due to the temporal resolution of the MD trajectories. The Panels (b), (c) and (d) show the difference between the true second normalised eigenvector ( black line labelled $\Psi_{2}$ (True)) and the same eigenvector estimated with $2, 5, 50$ basis states (blue line, labelled $\Psi_{2}$ (Approx.)). The true eigenvector was taken as $\Psi_{2}$ estimated with $n=500$ basis states (i.e. with negligible discretization error) rather than the eigenvector measured in the $(\psi, \phi)$ space for simplicity. The red shaded area shows the discretization error which is summed to give $\delta$. As $n$ increases from $2$ to $10$ to $50$, $\delta$ decreases (from $67.98$ to $51.54$ to $1.99$) while the response increases from $1.25$ to $1.98$ to $1.99$. For this feature, and likely for the other one-dimensional features ($\psi$, RMSD) the largest decrease in VAMP-2 occurs below $n=10$ which explains why the drop in response with decreasing $n$ is not observed. 

Second, the response for all features for $n > 100$ is constant. This is a result of the volume of MD simulation data. The discretisation error will eventually become negligible for all of the  non-trivial eigenvectors used in the VAMP-2 score as $n$ increases. As already mentioned, this explains the rapid increase in the response for $n<100$. In general, as $n$ increases the statistical uncertainty in the elements of the estimated transition matrix will increase and the model enters the high variance regime of the ``Bias-variance'' trade-off.  However, with the large volume simulation data the number of observations ($750\times(1000-9) = \num{743250}$ pairs of observed transitions) is comparable to the degrees of freedom for a reversible MSM \cite{trendelkamp-schroerEstimationUncertaintyReversible2015b} ($\sfrac{1}{2}n(n-1)+n-1=\num{500499}$ for $n=1000$).   

Third, large uncertainty of response surface for $n \leq 20$ is a result of the logarithmic warping  of $n$ and the comparatively sparse sampling in this region (as all sampling was done without prior logarithmic warping). 

Fourth, it is clear from inspection of figure \ref{fig:ala1_response} that the most relevant hyperparameter for determining the test response is the feature, $\chi$, while the number of cluster centres,  $n$, is almost irrelevant. This information is encoded in the learned hyperparameters of a Gaussian process which will be of benefit in more complicated systems with more than two hyperparameters. 
 
\subsection{Hyper-parameter relevance}\label{subsubsec:ala_relevance}

\begin{figure}
    \centering
    \mycaption{The relevance of the hyperparameters of alanine dipeptide. The distribution of the parameters of the response surface (shown in figure \ref{fig:ala1_response}) were estimated using MCMC. The relevance of the features (levels of $\chi$) are shown in blue, labelled `Feature'. The relevance of the log-transformed number of cluster centres, $n$ is shown in orange (labelled `Other').}
    \includegraphics[width=0.8\textwidth]{chapters/msm_optimization/figures/ala1_relevance.png}
    \label{fig:ala1_relevance}
\end{figure}

\begin{table}
    \centering
    \mycaption{Median and $\SI{95}{\percent}$ credible intervals for the kernel hyperparameters of the alanine dipeptide response surface estimated using MCMC. The length-scale parameters in \ref{eqn:kernel_form} are re-written here as relevances.}
    \begin{tabular}{|l|l|l|}
    \hline
                          Hyper-parameter &    Median & $\SI{95}{\percent}$ C.I. \\
    \hline\hline
     $R_{(\phi, \psi)\ \mathrm{torsion}}$ &  0.321 & 0.020-4.456 \\
        $R_{(x, y, z)\ \mathrm{coords.}}$ &  0.344 & 0.024-5.572 \\
             $R_{\phi\ \mathrm{torsion}}$ &  0.068 & 0.015-1.176 \\
             $R_{\psi\ \mathrm{torsion}}$ &  0.056 & 0.013-1.327 \\
                               $R_{RMSD}$ &  0.081 & 0.016-1.406 \\
                          $R_{\log{(n)}}$ &  0.029 & 0.013-0.063 \\
                                   $\eta$ &  2.518 & 1.141-5.530 \\
                               $\sigma_n$ &  0.006 & 0.006-0.007 \\
    \hline
    \end{tabular}
    \label{tab:ala1_rel_post}
\end{table}

This last observation can be quantified and understood by estimating the relevance of each predictor. The posterior distributions of the six relevance parameters of the response surface are summarised in box plots in figure \ref{fig:ala1_relevance}, and  the median and $\SI{95}{\percent}$ credible intervals  are tabulated in table \ref{tab:ala1_rel_post}. The interpretation of the relevance features (levels of $\chi$) will be different to that of the other hyperparameters. 

The relevance of $n$ determines the covariance of the response to changes in $n$ \emph{within the same feature}. This can be seen from the equation \ref{eqn:kernel_form} and making use of the fact that all kernel functions, $k(x, x^{\prime})=1$ for $x-x^{\prime}=0$:
\begin{equation*}
\begin{split}
    k^{tot}(\mathbf{x}, \mathbf{x}^{\prime})& = k\left((1, 0, 0, 0, 0, n), (1, 0, 0, 0, 0, n^{\prime})\right) \\
    & = \eta^{2}\cdot 1 \cdot 1\cdot 1 \cdot 1\cdot 1 \cdot k_{M}(n, n^{\prime}; R_{n}) \\
    & = \eta^{2}\cdot k(n, n^{\prime})
\end{split}
\end{equation*}
Here the kernel functions have been re-written with the relevance, $R$, instead of the length-scale $l$. The median relevance of $\log{(n)}$ is equal to $\num{0.029}$ and these imply that for change $n=10$ and $n=1000$ (a change of $1$ on the normalized scale) the covariance will be $\eta^{2}k_{M-52}(0,1; 0.029) \simeq 0.99\eta^{2}$. In other words, the response will be independent of the $n$ as already noted. 

The relevance of the different features determines the amount of information sharing between different features. Between a high relevance feature and all other features, there is little information sharing; between low relevance features there is a large amount of information sharing. To see this, consider the covariance between points at $n$ and $n^{\prime}$ on two different features, $\chi_1$ and $\chi_2$: 
\begin{equation*}
\begin{split}
    k^{tot}(\mathbf{x}, \mathbf{x}^{\prime})& = k\left((1, 0, 0, 0, 0, n), (0, 1, 0, 0, 0, n')\right) \\
    & = \eta^{2}\times k_{M}\left(1, 0; R_{\chi_1}\right) \times k_{M}\left(0, 1; R_{\chi_2}\right) \cdot 1 \cdot 1\cdot 1 \cdot k_{M}(n, n^{\prime}; R_{n}) \\
    &=  \eta^{2}\cdot k_{1}\cdot k_{2}\cdot k(n, n^{\prime})
\end{split}
\end{equation*}

If either $R_{\chi_1}$ or $R_{\chi_2}$ is large then $k_1 \cdot k_2 \simeq 0$ and there will no correlation between $n$ on feature $\chi_1$ and $n^{\prime}$ on feature $\chi_2$. If both $R_{\chi_1}$ and $R_{\chi_2}$ are small then $k_1 \cdot k_2 \simeq 1$ and covariance between $n$ on feature $\chi_1$ and $n^{\prime}$ on $\chi_2$ will be similar to the covariance between $n$ and $n^{\prime}$ on the same feature.  The features for alanine dipeptide are all relatively low relevance (i.e. less than $1$) which is a reflection of the  consistently flat response to changes in $n$ discussed above. Even between  two largest relevance features $(\phi, \psi)$ and $(x,y,z)$, the covariance between $n$ and $n^{\prime}$ on these two features is only altered by  $k_{1}\cdot k_{2} = 0.91\cdot0.92 \simeq 0.83$. 


\subsection{Optimization}\label{sec:ala_opt}

\begin{figure}[p]
    \centering
    \mycaption{Bayesian optimisation trajectories of alanine dipeptide seeded with $30$ hyperparameter trials ($15$ observations per predictor), panel \ref{fig:ala_opt_traj_30},  and 50 hyperparameter trials ($25$ observations per predictor), panel \ref{fig:ala_opt_traj_50}, for five different random subsets  (`iterations') of the total hyperparameter trial data set. The orange values are the trajectories calculated from random sampling, the blue values are the Bayesian optimisation trajectories. The first row (sub-panels (a) - (e)) are the VAMP-2 response, the second row (sub-panels (f) - (j)) show the accompanying number of cluster centres, and the third row (sub-panels (k) - (o)) are the  accompanying feature.}\label{fig:ala_opt_traj}
    \subtop[$N_{seed}=30$\label{fig:ala_opt_traj_30}]{
        \includegraphics[width=0.7\linewidth]{chapters/msm_optimization/figures/ala1_opt_traj_start_obs_30.png}}
    
    \subtop[$N_{seed}=50$ \label{fig:ala_opt_traj_50}]{
        \includegraphics[width=0.7\linewidth]{chapters/msm_optimization/figures/ala1_opt_traj_start_obs_50.png}}
\end{figure}


% Optimum MSM hyperparameters for AADH pre- and post-Bayesian optimisation (BO). Each column represents a Bayesian optimisation experiment, seeded with $N_{seed}$ randomly sampled hyperparameter trials. Five iterations of optimisation were run with $N_{seed}=100$ (labelled $\# 1, 2$ etc.) and a single iteration optimising the response surface using all the trial data ($N_{seed}=361$). Each row is a variable or outcome with values associated with the optimum value of $\mu$ before and after the BO.


\begin{table}
    \centering
    \mycaption{Optimum MSM hyperparameters for alanine dipeptide pre- and post- Bayesian optimisation (BO).  Each row represents a BO experiment, seeded with $N_{seed}$ randomly sampled hyperparameters trials. Five iterations of optimisation were run with $N_{seed}=30, 50$ (labelled $\# 1, 2$ etc.). The number of optimisation steps is equal to the difference in the pre/post value of $N_{total}$.  The optimum of the response surface estimated with all the trial data ($N_{total}=500$) is included even though it was not optimised using BO. Each column is a variable or outcome with values associated with the optimum value of $\mu$ before and after the BO.  
    }
\begin{tabular}{|ll|rr|rr|rr|ll|rr|}
\hline
   &   & $N_{total}$ & & $\mu$ & & $\sigma$ & & $\chi$ &  & $n$ & \\
 $N_{seed}$  &   \#  &         Pre & Post &   Pre &  Post &      Pre &  Post &  Pre & Post & Pre & Post \\
\hline\hline
0  & 1 &         500 &      & 3.318 &       &    0.002 &       &     $(x, y, z)$ &              & 762 &      \\
\hline
30 & 1 &          30 &   40 & 3.302 & 3.338 &    0.004 & 0.192 &  $(\phi, \psi)$ &     $(x, y, z)$ & 577 &  969 \\
   & 2 &          30 &   40 & 3.318 & 3.233 &    0.005 & 0.086 &  $(\phi, \psi)$ &     $(x, y, z)$ & 540 &  133 \\
   & 3 &          30 &   40 & 3.076 & 2.947 &    0.557 & 0.420 &  $(\phi, \psi)$ &  $(\phi, \psi)$ &  88 &   10 \\
   & 4 &          30 &   40 & 3.065 & 3.315 &    0.553 & 0.132 &     $(x, y, z)$ &     $(x, y, z)$ & 627 & 1000 \\
   & 5 &          30 &   40 & 3.313 & 3.258 &    0.005 & 0.194 &     $(x, y, z)$ &  $(\phi, \psi)$ & 968 &  684 \\
   \hline
50 & 1 &          50 &   60 & 3.330 & 3.337 &    0.012 & 0.032 &     $(x, y, z)$ &     $(x, y, z)$ & 251 &  333 \\
   & 2 &          50 &   60 & 3.306 & 3.338 &    0.022 & 0.040 &  $(\phi, \psi)$ &  $(\phi, \psi)$ & 540 &  540 \\
   & 3 &          50 &   60 & 3.309 & 3.327 &    0.013 & 0.013 &     $(x, y, z)$ &     $(x, y, z)$ & 176 &  670 \\
   & 4 &          50 &   60 & 3.307 & 3.318 &    0.005 & 0.004 &  $(\phi, \psi)$ &     $(x, y, z)$ & 634 & 1000 \\
   & 5 &          50 &   60 & 3.308 & 3.327 &    0.004 & 0.058 &     $(x, y, z)$ &     $(x, y, z)$ & 390 &  314 \\
\hline
\end{tabular}

    \label{tab:ala1_best_params}
\end{table}

The maximum of the response surface \emph{at the trial values} gives the optimum hyperparameters incorporating uncertainty and making full use of all the trial information. For alanine dipeptide, the maximum of the response surface, $\mu=\num{3.318}\pm\num{0.004}$, corresponds to using the $(x, y, z)$ coordinates feature with $n=762$ cluster centers. Given its simplicity, visual inspection of the response surface (figure \ref{fig:ala1_response}) was deemed sufficient to confirm that no more sampling was necessary to locate the maximum. 

Bayesian optimisation was used to determine whether this maximum could be achieved using a smaller number of trials and to test the code written to perform the optimisation. $p=10$ steps of Bayesian optimisation was performed on five, randomly sampled, subsets of the full hyperparameter trial data set with sizes $N_{seed}=30,50$.  The optimisation trajectories (i.e. the value of the incumbent and the associated predictors)  with $N_{seed}=30$ trials (or 15 observations-per-predictor) is shown in figure \ref{fig:ala_opt_traj_30} and with $N_{seed}=50$ (or 25 observations-per-predictor) is shown in figure \ref{fig:ala_opt_traj_50}). The $10$ steps of Bayesian optimisation are shown in blue (horizontal axis values $N_{seed} \rightarrow N_{seed}+10$) and for comparison the figure also shows, in orange, the trajectory calculated using randomly sampled trials  (horizontal axis values $N_{seed}-10 \rightarrow N_{seed}$). The input warping  and kernel function used in the response surfaces for all of the Bayesian optimisation experiments were the same as those used on the full trial data set. In principle these modelling choices should be determined independently for each data set but given the simplicity of the response surface it was deemed unnecessary. 

With $N_{seed}=30$ the incumbent trajectories of iteration 2, 3, \& 5 decreased , while for all iterations seeded with $50$ seed trials \ref{fig:ala_opt_traj} the incumbent trajectories remained constant or increased. In both cases the optimal values of $\chi$ were consistently oscillating between $(\phi, \psi)$ and $(x, y, z)$ while the optimal value of $n$ did not converge to a single value across the iterations. A value of  $N_{seed} = 50$ or $25$ observations per predictor were therefore deemed tentatively appropriate. 

There are a number of observations of the optimisation trajectories which reflect on the alanine dipeptide response surface described in section \ref{sec:ala_rsm} and the usefulness of Bayesian optimisation for this system. First, the incumbent trajectories clearly show that Bayesian optimisation does not increase the value of the incumbent by a significant amount. This is a reflection of the simple nature of the response surface and the irrelevance of the number of cluster centres. Second, the response surface (in the search space domain tested) is bimodal with peaks at $\chi=(\phi, \psi)$ torsions and $(x, y, z)$ coordinates. This is reflected in the clear lack of substantive difference between the final values of $\mu$ listed in table \ref{tab:ala1_best_params}.  Third, the almost complete irrelevance of $n$ as a hyperparameter is clearly shown in panels (f) to (j) in which $n \simeq 1000, 500 \& 100$ are arrived at with no clear difference in the value of the incumbent. Fourth, it is possible that with more optimisation steps it could be possible to arrive at the maximum of the response surface with less seed trial observations. While this is a possibility, the fact that Bayesian optimisation is an inherently serial algorithm (parallel variants which sample the acquisition function rather than optimise do exist []), while random sampling is embarrassingly parallel, it was considered more wall-time efficient (if not CPU-time efficient) to err on the side of more random seed trial data and less optimisation steps. 



\section{Conclusions}\label{sec:msm_conc}

This chapter introduced the use of response surfaces and Bayesian optimisation for understanding and optimizing the hyperparameters of an MSMs. A GP model proved a satisfactory statistical model for estimating the response surface of alanine dipeptide with two predictors $\chi$ and $n$. Model selection, using standard GP metrics, selected the logarithmic input warping over the hyperparameter $n$ to make the stationary assumption of the GP more plausible.  While GPs are usually used with continuous predictors the use of dummy coding  was demonstrated to be effective in incorporating the protein feature, $\chi$, as a categorical predictor. The relevance, a GP kernel hyperparameter corresponding to each predictor,  was shown to reflect the importance of each  MSM hyperparameter. For the non-categorical hyperparameter, $n$, the low relevance was a reflection of the near flat response of the MSM to changes in $n$. For the categorical predictor, the protein feature $\chi$, the low relevances of each feature was a reflection of how similar the response surfaces were \emph{conditional} on the value of $\chi$, in other words, how much information sharing there was between the different features. 

The response surface for AADH, with four predictors ($\chi$, $\tau$, $m$ and $n$) was also fit with a GP. The conditional structure of the predictor space meant that the RMSD feature was not able to be incorporated into the response surface. This is a problem for GPs in this setting. In future work more flexible models, such as Random Forests or Tree Pazen Estimators could be used to model this conditional structure more effectively. Model selection was used to select the most appropriate GP kernel and input warping. The final selected model fit the model with varying success. Overall the model fit the data well, but for the best performing feature, $(\phi, \psi, \chi)$ dihedrals, the fit was less satisfactory. 

The relevance of the hyperparameters revealed once again that $n$ was not important in determining the response, while the TICA hyperparameters were the most relevant. While the $(\phi, \psi, \chi)$ dihedrals and interatomic distances features were the best performing, each of the features did not have high relevance. This indicated that the response surfaces conditional on the features were similar.  The relative relevance of the hyperparameters was shown to be useful in visualising the high dimensional surface.

Bayesian optimisation was used to in an attempt to optimise the AADH response surface. After fitting the response surface with $361$ randomly sampled hyperparameters, further Bayesian optimisation did not increase the value of the maximum of the response surface or change significantly the optimal hyperparameters. Bayesian optimisation was also used on subsets of the data and the resulting optimum hyperparameters, determined from a total of $150$ trials, were similar to those determined from the full hyperparameter trial data set. This suggests the Bayesian optimisation could be used seeded with $25$ random observations per MSM hyperparameter. Further work is needed to determine the most efficient combination of random sampling and Bayesian optimisation. 

Once the response surface was optimised and the best performing MSM hyperparameters determined,  inspection of resulting MSM eigenvalue spectrum and the response surface suggested a number of sensitivity tests to be performed.  These tests revealed a fairly robust single dominant relaxation process with associated time-scale of $\SI{2.12}{\micro\second} (\SIrange{1.0}{5.2}{\micro\second})$. A second dominant relaxation process was not ruled out. The most relevant parameters, the TICA lag time, was also associated with changing both the qualitative and quantitative nature of the MSM. This could be an example of the Rashomon effect, something described in the MSM literature. 


% The response of an MSM to a randomly selected set of hyper-paraImeters was measured using standard the MSM metric, VAMP-2. These data were used with Gaussian process regression (GPR) to model the response surface of the MSM. Input warping and the type of kernel used in the  GPR were chosen using cross-validation of the standardised log loss and squared error metrics.  Bayesian optimisation was used to find the optimum of the response surface using the expected improvement as a hyperparameter selection policy. Once the optimum had been found and the corresponding MSM was estimated, the response surface and eigenvalue spectrum was used to explore the robustness of the optimum hyperparameters with a series of sensitivity tests. 


% \begin{table}
%     \centering
%     \mycaption{Summary description of the molecular dynamics data of alanine dipeptide and AADH.}
%     \begin{tabular}{|l|c|c|}
%         \hline
%          & Ala\textsubscript{1} & AADH \\
%          \hline\hline
%          No. MD trajectories & $750$ & $100$ \\
%          Total sampling time & \SI{750}{\nano\second} & \SI{10}{\micro\second} \\
%          Trajectory resolution & \SI{1}{\pico\second} & \SI{100}{\pico\second} \\
%          Temperature & \SI{300}{\kelvin} & \SI{310}{\kelvin} \\
%          Integrator & Langevin & Langevin \\
%          Ensemble & NVT & NVT \\
%          Forcefield & AMBER ff-99SB-ILDN & CHARMM-36 \\
%          Solvation & Explicit, TIP3P & Explicit, TIP3P \\
%          \hline
%     \end{tabular}
%     \label{tab:md_specs}
% \end{table}

% However, not all trials were successful (for example, if the number of TICA dimensions required, $m$, exceeded the number of dimensions of the feature, $\chi$) and these trials were ignored in the following analysis. This resulted in a final trial data set of size $N=500$ for alanine dipeptide and $N=461$ for AADH. The hyperparameter trial data sets will be denoted $\mathcal{D}_{N}(s)$ where $N$ labels the number of trials and $s$ is the name of the system. E.g. the full alanine dipeptide the hyperparameter trial data set is labelled as $\mathcal{D}_{500}(\mathrm{Ala}_{1})$. 

% meaning the response lies in the range \footnote{Recall that first eigenvalue, $\lambda_{1}=1$ and the remaining eigenvalues  $\lambda_{2,3,...} < 1$ thus $\operatorname{VAMP-2}(k=4)$ will be at least $1^2 + 0^2 + 0^2 + 0^2=1$ and at most $1^2 + 1^2 + 1^2 + 1^2=4$} $[1, k]$. For alanine dipeptide values $\tau(\mathrm{MSM})=\SI{9}{\pico\second}$ and $k=5$ were used in line with \cite{bowmanQuantitativeComparisonAlternative2013}.  For AADH values of $\tau=\SI{2}{\nano\second}$ and $k=4$ were used based on the eigenvalue spectrum of a reference MSM described in chapter \ref{chap:aadh}. 

% \begin{table}
%     \mycaption{The hyperparameter search space for AADH. TICA was applied to every feature except RMSD. Torsional angles, $\theta$, were given $(\sin(\theta),\cos(\theta)$ representations. The Cartesian coordinates were first aligned to a single, randomly chosen, trajectory frame so that feature (5) did not include spurious rotational or translational motion. The number of dimensions, `Dim.', refers to the number of individual feature variables created by $\chi$.}
%     \centering
%     \begin{tabularx}{0.9\textwidth}{ |>{\raggedright\arraybackslash}l|l|>{\raggedright\arraybackslash}X|c| >{\raggedright\arraybackslash}X | } 
%     \hline
%     \textbf{Hyper-parameter} & \textbf{Type} & \textbf{Range} & \textbf{Dim.} &\textbf{Details} \\
%      \hline\hline
%     Feature, $\chi$ & Categorical & (1) $(\phi, \psi, \chi)$ & $\num{116}$  & \\
%     & & (2) $|\mathbf{r}_{1}-\mathbf{r}_{2}|$  & $\num{ 2346}$& Heavy atoms inter-atomic distances \\
%     & & (3) $C_{\alpha}-C_{\alpha}$ & $\num{15}$ & alpha-Carbon contacts\\ 
%     & & (4) $X-X$  & $\num{15}$ & Heavy atoms contacts\\ 
%     & & (5) RMSD & $\num{1}$ &  Heavy atoms only\\ 
%     \hline
%     TICA lag time, $\tau$ & Integer &\SIlist[list-final-separator = { ... }]{1;1.1;100}{ns} &  & \\
%     \hline
%     TICA components, $m$& Integer &\numlist[list-final-separator = { ... }]{1;2;20} & & \\
%     \hline
%     Cluster centres, $n$ & Integer & \numlist[list-final-separator = { ... }]{10;11;1000}& &  Clustered using k-means clustering  \\
    
%      \hline
%     \end{tabularx}
%     \label{tab:aadh_searchspace}
% \end{table}


% \begin{table}
%     \centering
%     \mycaption{MSM model, scoring and cross validation (CV) details used in creating the hyperparameter trial data sets for alanine dipeptide and AADH. The number of trials reported is the number after removing models that failed to fit for any reason.}
%     \begin{tabular}{|c|c|c|}
%     \hline
%     & Ala\textsubscript{1} & AADH \\
%     \hline\hline
%     $\tau(\mathrm{MSM})$ & \SI{9}{\pico\second} & \SI{2}{\nano\second} \\         
%     $\operatorname{VAMP-2}(k)$ & $k=5$ & $k=4$ \\
%     No. trials & 500 & 461 \\
%     CV method & 50:50 Shuffle Split & 50:50 Shuffle Split \\
%     CV folds & 20 & 20 \\
%      \hline       
%     \end{tabular}
%     \label{tab:trial_specs}
% \end{table}




  
